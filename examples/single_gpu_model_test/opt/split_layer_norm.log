<run_flexgen>: args.model: facebook/opt-125m
model size: 0.230 GB, cache size: 0.075 GB, hidden size (prefill): 0.003 GB
init weight...
warmup - generate
debug_mode  None
overlap  True
num_gpu_batches  1
i,j,k 0 0 0
layer name:  InputEmbed
i,j,k 0 1 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 3 0
layer name:  MLP
i,j,k 0 4 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]],

        [[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]],

        [[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]],

        [[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]],

        [[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]],

        [[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]],

        [[ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         [ 0.1893, -0.3152,  0.0225,  ...,  0.4775,  0.0009,  0.3083],
         ...,
         [-0.5913,  0.0856,  0.0620,  ...,  0.1704,  0.0709,  0.2294],
         [-0.4705,  0.1664, -0.2727,  ..., -0.1426,  0.4031, -0.1019],
         [-0.2537,  0.4536, -0.0349,  ..., -0.2603, -0.1979,  0.0687]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         [ 0.1216,  0.0053,  0.0762,  ...,  0.2059,  0.0640,  0.1869],
         ...,
         [-0.1479,  0.0280,  0.0249,  ...,  0.0515,  0.0180,  0.0924],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0571, -0.0315,  ..., -0.0862, -0.0652,  0.0067]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 6 0
layer name:  MLP
i,j,k 0 7 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]],

        [[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]],

        [[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]],

        [[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]],

        [[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]],

        [[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]],

        [[ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         [ 0.2510,  0.0334,  0.1531,  ...,  0.7617, -0.5107,  0.4778],
         ...,
         [-0.6743,  0.0577, -0.0894,  ...,  0.0216,  0.1034,  0.2241],
         [-0.4839,  0.0312, -0.3757,  ..., -0.0020,  0.2151, -0.0763],
         [-0.3777,  0.2734, -0.0317,  ..., -0.0818, -0.2764,  0.1699]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         [ 0.1289,  0.0789,  0.1075,  ...,  0.2396, -0.0371,  0.1917],
         ...,
         [-0.1443,  0.0223, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0993,  0.0168, -0.0727,  ...,  0.0081,  0.0487, -0.0018],
         [-0.0915,  0.0270, -0.0274,  ..., -0.0371, -0.0677,  0.0134]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 9 0
layer name:  MLP
i,j,k 0 10 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]],

        [[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]],

        [[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]],

        [[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]],

        [[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]],

        [[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]],

        [[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]],

        [[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]],

        [[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]],

        [[ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         [ 0.4502,  0.0738,  0.2874,  ...,  0.4678, -0.3997,  0.4583],
         ...,
         [-0.9023, -0.1281, -0.0386,  ...,  0.1669,  0.0774,  0.2261],
         [-0.4797,  0.0838, -0.2507,  ...,  0.1096,  0.1709, -0.1299],
         [-0.5171,  0.1652,  0.0507,  ..., -0.1403, -0.4507,  0.1212]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]],

        [[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]],

        [[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]],

        [[ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         [ 0.1869,  0.0911,  0.1410,  ...,  0.1899, -0.0310,  0.1967],
         ...,
         [-0.2219, -0.0217,  0.0005,  ...,  0.0510,  0.0271,  0.0724],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0202],
         [-0.1233,  0.0125, -0.0131,  ..., -0.0500, -0.1038,  0.0041]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 12 0
layer name:  MLP
i,j,k 0 13 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]],

        [[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]],

        [[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]],

        [[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]],

        [[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]],

        [[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]],

        [[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]],

        [[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]],

        [[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]],

        [[ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         [ 0.5229, -0.3838,  0.1807,  ...,  0.2644, -0.3496,  0.5146],
         ...,
         [-0.9292, -0.4958,  0.0572,  ...,  0.2073, -0.0312,  0.3142],
         [-0.4993, -0.2722, -0.1884,  ...,  0.1187,  0.0614,  0.0222],
         [-0.5786,  0.0094, -0.0385,  ...,  0.0810, -0.4199,  0.3369]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]],

        [[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]],

        [[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]],

        [[ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         [ 0.1897, -0.0336,  0.1071,  ...,  0.1270, -0.0237,  0.2012],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0626,  0.0022,  0.0986],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0406,  0.0265,  0.0192],
         [-0.1260, -0.0193, -0.0283,  ..., -0.0075, -0.0966,  0.0447]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 15 0
layer name:  MLP
i,j,k 0 16 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]],

        [[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]],

        [[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]],

        [[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]],

        [[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]],

        [[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]],

        [[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]],

        [[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]],

        [[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]],

        [[ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         [ 0.7095, -0.7378,  0.2485,  ...,  0.4214, -0.3054,  0.6250],
         ...,
         [-0.8022, -0.2666,  0.3044,  ...,  0.1896, -0.0193,  0.1422],
         [-0.3728, -0.0486,  0.1213,  ...,  0.0865,  0.0588, -0.0690],
         [-0.4473, -0.0487,  0.0990,  ..., -0.1376, -0.2910,  0.2834]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]],

        [[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]],

        [[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]],

        [[ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         [ 0.2668, -0.1633,  0.1272,  ...,  0.1746, -0.0248,  0.2441],
         ...,
         [-0.2211, -0.0651,  0.0928,  ...,  0.0615,  0.0066,  0.0526],
         [-0.0973,  0.0008,  0.0448,  ...,  0.0352,  0.0288, -0.0073],
         [-0.1174, -0.0298, -0.0020,  ..., -0.0490, -0.0786,  0.0392]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 18 0
layer name:  MLP
i,j,k 0 19 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]],

        [[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]],

        [[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]],

        [[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]],

        [[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]],

        [[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]],

        [[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]]], device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]],

        [[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]],

        [[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]],

        [[ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         [ 0.4048, -0.4897,  0.3623,  ...,  0.1663, -0.2268,  0.5435],
         ...,
         [-0.6802, -0.2676,  0.2598,  ...,  0.1750, -0.0411,  0.1935],
         [-0.4395, -0.0140,  0.1331,  ...,  0.1322, -0.0430, -0.1018],
         [-0.4507, -0.2827, -0.2815,  ..., -0.1005, -0.4678,  0.2849]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]],

        [[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]],

        [[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]],

        [[ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         [ 1.9495e-01, -1.1389e-01,  1.7261e-01,  ...,  1.0840e-01,
          -1.6739e-02,  2.4915e-01],
         ...,
         [-2.0007e-01, -7.1533e-02,  8.6914e-02,  ...,  6.1279e-02,
          -7.7438e-04,  7.5073e-02],
         [-1.2878e-01,  1.1719e-02,  5.2948e-02,  ...,  5.2124e-02,
          -6.1035e-05, -1.8250e-02],
         [-1.3135e-01, -9.0088e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2830e-01,  5.2246e-02]]], device='cuda:0', dtype=torch.float16)
i,j,k 0 21 0
layer name:  MLP
i,j,k 0 22 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]],

        [[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]],

        [[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]],

        [[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]],

        [[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]],

        [[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]],

        [[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]],

        [[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]],

        [[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]],

        [[ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         [ 0.2426, -0.1660,  0.4980,  ..., -0.0523, -0.0055,  0.6401],
         ...,
         [-0.8774, -0.1282,  0.0766,  ..., -0.1082, -0.0231,  0.2874],
         [-0.6328,  0.1611,  0.1137,  ...,  0.1072, -0.0316,  0.0074],
         [-0.6147, -0.2413, -0.2278,  ...,  0.0396, -0.5962,  0.3452]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]],

        [[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]],

        [[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]],

        [[ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         [ 0.1475, -0.0007,  0.2413,  ...,  0.0337,  0.0529,  0.2920],
         ...,
         [-0.2561, -0.0207,  0.0376,  ..., -0.0212,  0.0064,  0.1009],
         [-0.1880,  0.0761,  0.0532,  ...,  0.0479,  0.0058,  0.0175],
         [-0.1855, -0.0804, -0.0798,  ..., -0.0093, -0.1742,  0.0751]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 24 0
layer name:  MLP
i,j,k 0 25 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]],

        [[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]],

        [[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]],

        [[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]],

        [[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]],

        [[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]],

        [[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]],

        [[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]],

        [[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]],

        [[ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         [ 0.2693, -0.4431,  0.6958,  ..., -0.1653, -0.2241,  0.3145],
         ...,
         [-0.8691,  0.0931,  0.2109,  ..., -0.1842, -0.4109,  0.1577],
         [-0.5044,  0.2842,  0.1027,  ..., -0.0149, -0.2769, -0.0992],
         [-0.5684, -0.1537, -0.2983,  ..., -0.0663, -0.6343,  0.0874]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]],

        [[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]],

        [[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]],

        [[ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         [ 0.1720, -0.1448,  0.3481,  ..., -0.0190, -0.0391,  0.1846],
         ...,
         [-0.2710,  0.0570,  0.0831,  ..., -0.0471, -0.1161,  0.0642],
         [-0.1520,  0.1327,  0.0517,  ...,  0.0085, -0.0730, -0.0178],
         [-0.1895, -0.0609, -0.1075,  ..., -0.0400, -0.2042,  0.0077]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 27 0
layer name:  MLP
i,j,k 0 28 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]],

        [[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]],

        [[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]],

        [[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]],

        [[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]],

        [[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]],

        [[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]],

        [[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]],

        [[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]],

        [[ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         [ 0.3223, -0.9204,  0.6021,  ..., -0.0762, -0.1399,  0.4058],
         ...,
         [-0.7617,  0.8350,  0.3203,  ..., -0.1185, -0.2708,  0.2502],
         [-0.4231,  0.8652,  0.1217,  ...,  0.1766, -0.2971, -0.0331],
         [-0.4670, -0.1050, -0.1682,  ...,  0.4404, -0.6021,  0.1583]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]],

        [[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]],

        [[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]],

        [[ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         [ 0.2260, -0.3713,  0.3667,  ...,  0.0136, -0.0156,  0.2590],
         ...,
         [-0.2625,  0.3430,  0.1368,  ..., -0.0280, -0.0820,  0.1082],
         [-0.1372,  0.3647,  0.0659,  ...,  0.0852, -0.0916,  0.0072],
         [-0.1696, -0.0273, -0.0735,  ...,  0.1312, -0.2169,  0.0371]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 30 0
layer name:  MLP
i,j,k 0 31 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]],

        [[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]],

        [[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]],

        [[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]],

        [[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]],

        [[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]],

        [[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]],

        [[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]],

        [[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]],

        [[ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         [ 0.4065, -0.5132,  0.4629,  ...,  0.0600, -0.3298,  0.1859],
         ...,
         [-1.2002,  0.7056,  0.5396,  ...,  0.1587, -0.0654,  0.0802],
         [-0.9951,  0.9961,  0.3452,  ...,  0.3574,  0.0172, -0.1476],
         [-0.5811, -0.0568, -0.0936,  ...,  0.6807, -0.4812,  0.4531]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]],

        [[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]],

        [[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]],

        [[ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         [ 0.3013, -0.3296,  0.3372,  ...,  0.0883, -0.1477,  0.1627],
         ...,
         [-0.4854,  0.5132,  0.2593,  ...,  0.0879, -0.0100,  0.0552],
         [-0.4094,  0.7251,  0.1826,  ...,  0.1833,  0.0300, -0.0376],
         [-0.2385, -0.0014, -0.0495,  ...,  0.2649, -0.2112,  0.1665]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 33 0
layer name:  MLP
i,j,k 0 34 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]],

        [[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]],

        [[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]],

        [[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 32, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]],

        [[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]],

        [[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]],

        [[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 32 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 32, 768])
hidden tensor([[[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]],

        [[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]],

        [[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]],

        [[ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         [ 0.4070, -0.3967,  0.1803,  ..., -0.2710, -0.4529,  0.1782],
         ...,
         [-1.1914,  2.1094,  0.5586,  ...,  0.3762,  0.0184,  0.1155],
         [-1.0400,  2.2676,  0.3789,  ...,  0.5859, -0.1006,  0.0080],
         [-0.4617,  0.3340,  0.1161,  ...,  0.8467, -0.4814,  0.4617]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]],

        [[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]],

        [[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]],

        [[ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         [ 0.3547, -0.1447,  0.2030,  ..., -0.1223, -0.2944,  0.1880],
         ...,
         [-0.5474,  1.1104,  0.3171,  ...,  0.2135,  0.0300,  0.0840],
         [-0.5186,  1.3076,  0.2496,  ...,  0.3486, -0.0336,  0.0403],
         [-0.2227,  0.2129,  0.0615,  ...,  0.4128, -0.2627,  0.2146]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 36 0
layer name:  MLP
i,j,k 0 37 0
layer name:  OutputEmbed
benchmark - generate
debug_mode  None
overlap  True
num_gpu_batches  1
i,j,k 0 0 0
layer name:  InputEmbed
i,j,k 0 1 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]],

        [[ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         [ 0.2830, -1.1719, -0.0729,  ...,  0.8115,  0.1148, -0.0462],
         ...,
         [-0.4026,  0.3447, -0.0172,  ...,  0.1772,  0.1724,  0.2659],
         [-0.5420,  0.2292, -0.2106,  ..., -0.0223,  0.3484, -0.2135],
         [-0.2556,  0.4478, -0.0226,  ..., -0.2588, -0.0991, -0.0288]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]],

        [[ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],
         ...,
         [-0.0912,  0.0721, -0.0005,  ...,  0.0536,  0.0411,  0.1113],
         [-0.1124,  0.0406, -0.0464,  ...,  0.0022,  0.0786, -0.0495],
         [-0.0830,  0.0606, -0.0327,  ..., -0.0880, -0.0554, -0.0228]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 3 0
layer name:  MLP
i,j,k 0 4 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]],

        [[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]],

        [[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]],

        [[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]],

        [[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]],

        [[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]],

        [[ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         [ 0.2961, -0.2615,  0.0271,  ...,  0.4651,  0.0318,  0.2368],
         ...,
         [-0.5913,  0.0858,  0.0620,  ...,  0.1704,  0.0709,  0.2296],
         [-0.4705,  0.1667, -0.2727,  ..., -0.1426,  0.4033, -0.1019],
         [-0.2537,  0.4539, -0.0349,  ..., -0.2603, -0.1979,  0.0688]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]],

        [[ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         [ 0.1537,  0.0153,  0.0776,  ...,  0.2056,  0.0711,  0.1653],
         ...,
         [-0.1479,  0.0280,  0.0248,  ...,  0.0515,  0.0180,  0.0925],
         [-0.1104,  0.0428, -0.0627,  ..., -0.0310,  0.0870, -0.0088],
         [-0.0816,  0.0572, -0.0315,  ..., -0.0862, -0.0652,  0.0067]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 6 0
layer name:  MLP
i,j,k 0 7 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]],

        [[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]],

        [[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]],

        [[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]],

        [[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]],

        [[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]],

        [[ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         [ 0.3298,  0.0716,  0.1260,  ...,  0.7119, -0.5337,  0.3198],
         ...,
         [-0.6743,  0.0578, -0.0895,  ...,  0.0217,  0.1033,  0.2244],
         [-0.4836,  0.0313, -0.3757,  ..., -0.0020,  0.2148, -0.0762],
         [-0.3777,  0.2737, -0.0317,  ..., -0.0819, -0.2764,  0.1700]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]],

        [[ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         [ 0.1532,  0.0895,  0.1036,  ...,  0.2394, -0.0489,  0.1597],
         ...,
         [-0.1443,  0.0224, -0.0102,  ...,  0.0126,  0.0268,  0.0662],
         [-0.0992,  0.0168, -0.0727,  ...,  0.0081,  0.0486, -0.0018],
         [-0.0915,  0.0271, -0.0274,  ..., -0.0371, -0.0677,  0.0134]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 9 0
layer name:  MLP
i,j,k 0 10 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]],

        [[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]],

        [[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]],

        [[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]],

        [[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]],

        [[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]],

        [[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]],

        [[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]],

        [[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]],

        [[ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         [ 0.4915, -0.0272,  0.2891,  ...,  0.4509, -0.4934,  0.2969],
         ...,
         [-0.9023, -0.1283, -0.0386,  ...,  0.1670,  0.0773,  0.2266],
         [-0.4797,  0.0837, -0.2507,  ...,  0.1096,  0.1705, -0.1298],
         [-0.5171,  0.1655,  0.0507,  ..., -0.1401, -0.4507,  0.1212]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]],

        [[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]],

        [[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]],

        [[ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         [ 0.2087,  0.0619,  0.1472,  ...,  0.1953, -0.0661,  0.1594],
         ...,
         [-0.2219, -0.0218,  0.0005,  ...,  0.0510,  0.0271,  0.0726],
         [-0.1131,  0.0353, -0.0503,  ...,  0.0374,  0.0500, -0.0201],
         [-0.1233,  0.0126, -0.0131,  ..., -0.0499, -0.1038,  0.0041]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 12 0
layer name:  MLP
i,j,k 0 13 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]],

        [[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]],

        [[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]],

        [[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]],

        [[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]],

        [[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]],

        [[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]],

        [[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]],

        [[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]],

        [[ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         [ 0.6831, -0.4355,  0.2218,  ...,  0.3201, -0.3491,  0.1560],
         ...,
         [-0.9292, -0.4958,  0.0573,  ...,  0.2078, -0.0312,  0.3152],
         [-0.4995, -0.2722, -0.1882,  ...,  0.1191,  0.0612,  0.0223],
         [-0.5796,  0.0097, -0.0386,  ...,  0.0815, -0.4199,  0.3369]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]],

        [[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]],

        [[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]],

        [[ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         [ 0.2408, -0.0566,  0.1201,  ...,  0.1454, -0.0318,  0.1077],
         ...,
         [-0.2275, -0.1181,  0.0252,  ...,  0.0627,  0.0022,  0.0989],
         [-0.1154, -0.0579, -0.0356,  ...,  0.0407,  0.0264,  0.0192],
         [-0.1261, -0.0193, -0.0283,  ..., -0.0074, -0.0966,  0.0447]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 15 0
layer name:  MLP
i,j,k 0 16 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]],

        [[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]],

        [[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]],

        [[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]],

        [[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]],

        [[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]],

        [[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]],

        [[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]],

        [[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]],

        [[ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         [ 0.8320, -0.6431, -0.0773,  ...,  0.4893, -0.3113,  0.3635],
         ...,
         [-0.8022, -0.2664,  0.3047,  ...,  0.1901, -0.0192,  0.1433],
         [-0.3733, -0.0487,  0.1213,  ...,  0.0868,  0.0591, -0.0690],
         [-0.4478, -0.0478,  0.0986,  ..., -0.1372, -0.2910,  0.2837]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]],

        [[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]],

        [[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]],

        [[ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         [ 0.3242, -0.1547,  0.0331,  ...,  0.2041, -0.0364,  0.1752],
         ...,
         [-0.2211, -0.0650,  0.0928,  ...,  0.0616,  0.0066,  0.0529],
         [-0.0974,  0.0008,  0.0448,  ...,  0.0352,  0.0289, -0.0073],
         [-0.1175, -0.0296, -0.0020,  ..., -0.0490, -0.0786,  0.0393]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 18 0
layer name:  MLP
i,j,k 0 19 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]],

        [[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]],

        [[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]],

        [[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]],

        [[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]],

        [[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]],

        [[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]]], device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]],

        [[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]],

        [[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]],

        [[ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         [ 0.4395, -0.3245,  0.0848,  ...,  0.1309, -0.2482,  0.3579],
         ...,
         [-0.6802, -0.2668,  0.2600,  ...,  0.1754, -0.0409,  0.1945],
         [-0.4397, -0.0140,  0.1329,  ...,  0.1323, -0.0431, -0.1020],
         [-0.4507, -0.2817, -0.2815,  ..., -0.1005, -0.4673,  0.2852]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]],

        [[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]],

        [[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]],

        [[ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         [ 2.2205e-01, -6.9458e-02,  8.4106e-02,  ...,  9.9976e-02,
          -3.4210e-02,  1.9666e-01],
         ...,
         [-1.9995e-01, -7.1289e-02,  8.6975e-02,  ...,  6.1401e-02,
          -7.3242e-04,  7.5378e-02],
         [-1.2878e-01,  1.1703e-02,  5.2917e-02,  ...,  5.2155e-02,
          -7.6294e-05, -1.8311e-02],
         [-1.3135e-01, -8.9844e-02, -8.6670e-02,  ..., -4.4586e-02,
          -1.2817e-01,  5.2307e-02]]], device='cuda:0', dtype=torch.float16)
i,j,k 0 21 0
layer name:  MLP
i,j,k 0 22 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]],

        [[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]],

        [[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]],

        [[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]],

        [[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]],

        [[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]],

        [[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]],

        [[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]],

        [[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]],

        [[ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         [ 0.4080, -0.0099,  0.3086,  ..., -0.0900, -0.1345,  0.4424],
         ...,
         [-0.8779, -0.1273,  0.0770,  ..., -0.1075, -0.0230,  0.2881],
         [-0.6328,  0.1606,  0.1136,  ...,  0.1072, -0.0315,  0.0078],
         [-0.6157, -0.2399, -0.2281,  ...,  0.0394, -0.5952,  0.3452]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]],

        [[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]],

        [[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]],

        [[ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         [ 0.2362,  0.0614,  0.1914,  ...,  0.0158,  0.0007,  0.2471],
         ...,
         [-0.2561, -0.0205,  0.0377,  ..., -0.0211,  0.0064,  0.1011],
         [-0.1880,  0.0759,  0.0532,  ...,  0.0480,  0.0058,  0.0177],
         [-0.1857, -0.0800, -0.0799,  ..., -0.0094, -0.1738,  0.0751]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 24 0
layer name:  MLP
i,j,k 0 25 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]],

        [[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]],

        [[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]],

        [[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]],

        [[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]],

        [[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]],

        [[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]],

        [[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]],

        [[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]],

        [[ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         [ 0.4851, -0.3062,  0.4895,  ..., -0.2152, -0.3416,  0.1927],
         ...,
         [-0.8696,  0.0943,  0.2112,  ..., -0.1840, -0.4109,  0.1586],
         [-0.5039,  0.2847,  0.1025,  ..., -0.0145, -0.2773, -0.0986],
         [-0.5684, -0.1525, -0.2981,  ..., -0.0666, -0.6328,  0.0877]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]],

        [[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]],

        [[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]],

        [[ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         [ 0.3042, -0.1011,  0.2993,  ..., -0.0532, -0.1102,  0.1492],
         ...,
         [-0.2710,  0.0574,  0.0831,  ..., -0.0471, -0.1161,  0.0645],
         [-0.1519,  0.1328,  0.0517,  ...,  0.0087, -0.0731, -0.0175],
         [-0.1895, -0.0605, -0.1075,  ..., -0.0401, -0.2039,  0.0078]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 27 0
layer name:  MLP
i,j,k 0 28 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]],

        [[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]],

        [[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]],

        [[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]],

        [[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]],

        [[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]],

        [[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]],

        [[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]],

        [[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]],

        [[ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         [ 0.4197, -1.1729,  0.3408,  ..., -0.0953, -0.0786,  0.1842],
         ...,
         [-0.7612,  0.8354,  0.3201,  ..., -0.1180, -0.2705,  0.2507],
         [-0.4224,  0.8657,  0.1217,  ...,  0.1769, -0.2971, -0.0329],
         [-0.4670, -0.1046, -0.1674,  ...,  0.4404, -0.6016,  0.1587]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]],

        [[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]],

        [[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]],

        [[ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         [ 0.3352, -0.6343,  0.2781,  ..., -0.0061,  0.0089,  0.1733],
         ...,
         [-0.2625,  0.3433,  0.1367,  ..., -0.0278, -0.0820,  0.1084],
         [-0.1370,  0.3650,  0.0659,  ...,  0.0853, -0.0916,  0.0073],
         [-0.1696, -0.0272, -0.0732,  ...,  0.1312, -0.2167,  0.0373]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 30 0
layer name:  MLP
i,j,k 0 31 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]],

        [[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]],

        [[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]],

        [[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]],

        [[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]],

        [[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]],

        [[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]],

        [[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]],

        [[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]],

        [[ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         [ 0.5181, -0.8501,  0.3052,  ...,  0.0783, -0.2255,  0.1858],
         ...,
         [-1.2002,  0.7065,  0.5396,  ...,  0.1600, -0.0651,  0.0806],
         [-0.9946,  0.9951,  0.3455,  ...,  0.3577,  0.0180, -0.1472],
         [-0.5801, -0.0580, -0.0931,  ...,  0.6792, -0.4800,  0.4531]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]],

        [[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]],

        [[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]],

        [[ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         [ 0.4404, -0.7710,  0.2864,  ...,  0.1130, -0.1118,  0.1906],
         ...,
         [-0.4854,  0.5142,  0.2593,  ...,  0.0885, -0.0099,  0.0554],
         [-0.4092,  0.7246,  0.1827,  ...,  0.1835,  0.0304, -0.0375],
         [-0.2382, -0.0021, -0.0493,  ...,  0.2644, -0.2107,  0.1665]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 33 0
layer name:  MLP
i,j,k 0 34 0
layer name:  layer_norm
self attention prefill layernorm--------
layernorm----------------------------------------------------------
out tensor([[[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]],

        [[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]],

        [[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]],

        [[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 512, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]],

        [[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]],

        [[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]],

        [[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]]],
       device='cuda:0', dtype=torch.float16)
self attention prefill--------
mha----------------------------------------------------------
b,s,h 4 512 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 512, 768])
hidden tensor([[[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]],

        [[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]],

        [[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]],

        [[ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         [ 0.5410, -1.0771, -0.0032,  ..., -0.1538, -0.3457,  0.1910],
         ...,
         [-1.1904,  2.1113,  0.5591,  ...,  0.3774,  0.0181,  0.1161],
         [-1.0400,  2.2676,  0.3799,  ...,  0.5850, -0.1001,  0.0084],
         [-0.4607,  0.3311,  0.1174,  ...,  0.8452, -0.4807,  0.4609]]],
       device='cuda:0', dtype=torch.float16)
pre_inputs.data tensor([[[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]],

        [[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]],

        [[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]],

        [[ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         [ 0.5322, -0.7617,  0.0726,  ..., -0.0543, -0.2615,  0.2289],
         ...,
         [-0.5469,  1.1113,  0.3174,  ...,  0.2141,  0.0298,  0.0844],
         [-0.5186,  1.3076,  0.2502,  ...,  0.3481, -0.0333,  0.0405],
         [-0.2222,  0.2115,  0.0621,  ...,  0.4121, -0.2622,  0.2144]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 0 36 0
layer name:  MLP
i,j,k 0 37 0
layer name:  OutputEmbed
i,j,k 1 0 0
layer name:  InputEmbed
i,j,k 1 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0673, -0.2440, -0.0515,  ...,  0.4863,  0.2605,  0.0940]],

        [[-0.0673, -0.2440, -0.0515,  ...,  0.4863,  0.2605,  0.0940]],

        [[-0.0673, -0.2440, -0.0515,  ...,  0.4863,  0.2605,  0.0940]],

        [[-0.0673, -0.2440, -0.0515,  ...,  0.4863,  0.2605,  0.0940]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0016, -0.0587, -0.0007,  ...,  0.1262,  0.0631,  0.0529]],

        [[-0.0016, -0.0587, -0.0007,  ...,  0.1262,  0.0631,  0.0529]],

        [[-0.0016, -0.0587, -0.0007,  ...,  0.1262,  0.0631,  0.0529]],

        [[-0.0016, -0.0587, -0.0007,  ...,  0.1262,  0.0631,  0.0529]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 3 0
layer name:  MLP
i,j,k 1 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0342, -0.3533, -0.1876,  ...,  0.3733,  0.2585,  0.1501]],

        [[ 0.0342, -0.3533, -0.1876,  ...,  0.3733,  0.2585,  0.1501]],

        [[ 0.0342, -0.3533, -0.1876,  ...,  0.3733,  0.2585,  0.1501]],

        [[ 0.0342, -0.3533, -0.1876,  ...,  0.3733,  0.2585,  0.1501]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0225, -0.0511, -0.0353,  ...,  0.1116,  0.0650,  0.0739]],

        [[ 0.0225, -0.0511, -0.0353,  ...,  0.1116,  0.0650,  0.0739]],

        [[ 0.0225, -0.0511, -0.0353,  ...,  0.1116,  0.0650,  0.0739]],

        [[ 0.0225, -0.0511, -0.0353,  ...,  0.1116,  0.0650,  0.0739]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 6 0
layer name:  MLP
i,j,k 1 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0867, -0.3538, -0.4385,  ...,  0.6870,  0.0784,  0.3440]],

        [[-0.0867, -0.3538, -0.4385,  ...,  0.6870,  0.0784,  0.3440]],

        [[-0.0867, -0.3538, -0.4385,  ...,  0.6870,  0.0784,  0.3440]],

        [[-0.0867, -0.3538, -0.4385,  ...,  0.6870,  0.0784,  0.3440]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0048, -0.0630, -0.0811,  ...,  0.1600,  0.0286,  0.1000]],

        [[-0.0048, -0.0630, -0.0811,  ...,  0.1600,  0.0286,  0.1000]],

        [[-0.0048, -0.0630, -0.0811,  ...,  0.1600,  0.0286,  0.1000]],

        [[-0.0048, -0.0630, -0.0811,  ...,  0.1600,  0.0286,  0.1000]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 9 0
layer name:  MLP
i,j,k 1 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0866, -0.4373, -0.4148,  ...,  0.4866, -0.2372,  0.1956]],

        [[-0.0866, -0.4373, -0.4148,  ...,  0.4866, -0.2372,  0.1956]],

        [[-0.0866, -0.4373, -0.4148,  ...,  0.4866, -0.2372,  0.1956]],

        [[-0.0866, -0.4373, -0.4148,  ...,  0.4866, -0.2372,  0.1956]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0063, -0.0959, -0.0834,  ...,  0.1376, -0.0399,  0.0709]],

        [[-0.0063, -0.0959, -0.0834,  ...,  0.1376, -0.0399,  0.0709]],

        [[-0.0063, -0.0959, -0.0834,  ...,  0.1376, -0.0399,  0.0709]],

        [[-0.0063, -0.0959, -0.0834,  ...,  0.1376, -0.0399,  0.0709]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 12 0
layer name:  MLP
i,j,k 1 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1632, -0.7554, -0.2620,  ...,  0.4602, -0.6104,  0.2673]],

        [[-0.1632, -0.7554, -0.2620,  ...,  0.4602, -0.6104,  0.2673]],

        [[-0.1632, -0.7554, -0.2620,  ...,  0.4602, -0.6104,  0.2673]],

        [[-0.1632, -0.7554, -0.2620,  ...,  0.4602, -0.6104,  0.2673]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0211, -0.1572, -0.0418,  ...,  0.1207, -0.1209,  0.0841]],

        [[-0.0211, -0.1572, -0.0418,  ...,  0.1207, -0.1209,  0.0841]],

        [[-0.0211, -0.1572, -0.0418,  ...,  0.1207, -0.1209,  0.0841]],

        [[-0.0211, -0.1572, -0.0418,  ...,  0.1207, -0.1209,  0.0841]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 15 0
layer name:  MLP
i,j,k 1 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2125, -0.6641, -0.0736,  ...,  0.4192, -0.8203,  0.1016]],

        [[-0.2125, -0.6641, -0.0736,  ...,  0.4192, -0.8203,  0.1016]],

        [[-0.2125, -0.6641, -0.0736,  ...,  0.4192, -0.8203,  0.1016]],

        [[-0.2125, -0.6641, -0.0736,  ...,  0.4192, -0.8203,  0.1016]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0404, -0.1633, -0.0009,  ...,  0.1224, -0.1869,  0.0457]],

        [[-0.0404, -0.1633, -0.0009,  ...,  0.1224, -0.1869,  0.0457]],

        [[-0.0404, -0.1633, -0.0009,  ...,  0.1224, -0.1869,  0.0457]],

        [[-0.0404, -0.1633, -0.0009,  ...,  0.1224, -0.1869,  0.0457]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 18 0
layer name:  MLP
i,j,k 1 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2394, -0.6431, -0.0570,  ...,  0.3879, -0.6309,  0.0279]],

        [[-0.2394, -0.6431, -0.0570,  ...,  0.3879, -0.6309,  0.0279]],

        [[-0.2394, -0.6431, -0.0570,  ...,  0.3879, -0.6309,  0.0279]],

        [[-0.2394, -0.6431, -0.0570,  ...,  0.3879, -0.6309,  0.0279]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0491, -0.1661,  0.0025,  ...,  0.1196, -0.1482,  0.0283]],

        [[-0.0491, -0.1661,  0.0025,  ...,  0.1196, -0.1482,  0.0283]],

        [[-0.0491, -0.1661,  0.0025,  ...,  0.1196, -0.1482,  0.0283]],

        [[-0.0491, -0.1661,  0.0025,  ...,  0.1196, -0.1482,  0.0283]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 21 0
layer name:  MLP
i,j,k 1 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2739, -0.7881, -0.0262,  ...,  0.4373, -0.6436,  0.1599]],

        [[-0.2739, -0.7881, -0.0262,  ...,  0.4373, -0.6436,  0.1599]],

        [[-0.2739, -0.7881, -0.0262,  ...,  0.4373, -0.6436,  0.1599]],

        [[-0.2739, -0.7881, -0.0262,  ...,  0.4373, -0.6436,  0.1599]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0566, -0.2043,  0.0135,  ...,  0.1411, -0.1549,  0.0651]],

        [[-0.0566, -0.2043,  0.0135,  ...,  0.1411, -0.1549,  0.0651]],

        [[-0.0566, -0.2043,  0.0135,  ...,  0.1411, -0.1549,  0.0651]],

        [[-0.0566, -0.2043,  0.0135,  ...,  0.1411, -0.1549,  0.0651]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 24 0
layer name:  MLP
i,j,k 1 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3057, -0.8950, -0.0891,  ...,  0.4214, -0.7026,  0.1161]],

        [[-0.3057, -0.8950, -0.0891,  ...,  0.4214, -0.7026,  0.1161]],

        [[-0.3057, -0.8950, -0.0891,  ...,  0.4214, -0.7026,  0.1161]],

        [[-0.3057, -0.8950, -0.0891,  ...,  0.4214, -0.7026,  0.1161]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0754, -0.2891, -0.0070,  ...,  0.1469, -0.1976,  0.0571]],

        [[-0.0754, -0.2891, -0.0070,  ...,  0.1469, -0.1976,  0.0571]],

        [[-0.0754, -0.2891, -0.0070,  ...,  0.1469, -0.1976,  0.0571]],

        [[-0.0754, -0.2891, -0.0070,  ...,  0.1469, -0.1976,  0.0571]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 27 0
layer name:  MLP
i,j,k 1 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3425, -0.7524, -0.0875,  ...,  0.2847, -0.5532, -0.0169]],

        [[-0.3425, -0.7524, -0.0875,  ...,  0.2847, -0.5532, -0.0169]],

        [[-0.3425, -0.7524, -0.0875,  ...,  0.2847, -0.5532, -0.0169]],

        [[-0.3425, -0.7524, -0.0875,  ...,  0.2847, -0.5532, -0.0169]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0936, -0.2109, -0.0083,  ...,  0.1223, -0.1699,  0.0176]],

        [[-0.0936, -0.2109, -0.0083,  ...,  0.1223, -0.1699,  0.0176]],

        [[-0.0936, -0.2109, -0.0083,  ...,  0.1223, -0.1699,  0.0176]],

        [[-0.0936, -0.2109, -0.0083,  ...,  0.1223, -0.1699,  0.0176]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 30 0
layer name:  MLP
i,j,k 1 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2242, -0.4824,  0.1362,  ...,  0.3198, -0.4482,  0.1622]],

        [[-0.2242, -0.4824,  0.1362,  ...,  0.3198, -0.4482,  0.1622]],

        [[-0.2242, -0.4824,  0.1362,  ...,  0.3198, -0.4482,  0.1622]],

        [[-0.2242, -0.4824,  0.1362,  ...,  0.3198, -0.4482,  0.1622]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0546, -0.2068,  0.0809,  ...,  0.1499, -0.1523,  0.0881]],

        [[-0.0546, -0.2068,  0.0809,  ...,  0.1499, -0.1523,  0.0881]],

        [[-0.0546, -0.2068,  0.0809,  ...,  0.1499, -0.1523,  0.0881]],

        [[-0.0546, -0.2068,  0.0809,  ...,  0.1499, -0.1523,  0.0881]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 33 0
layer name:  MLP
i,j,k 1 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1263, -0.0767,  0.0334,  ...,  0.7944, -0.7202, -0.0084]],

        [[-0.1263, -0.0767,  0.0334,  ...,  0.7944, -0.7202, -0.0084]],

        [[-0.1263, -0.0767,  0.0334,  ...,  0.7944, -0.7202, -0.0084]],

        [[-0.1263, -0.0767,  0.0334,  ...,  0.7944, -0.7202, -0.0084]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 1 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0217,  0.0451,  0.0517,  ...,  0.3936, -0.3267,  0.0321]],

        [[-0.0217,  0.0451,  0.0517,  ...,  0.3936, -0.3267,  0.0321]],

        [[-0.0217,  0.0451,  0.0517,  ...,  0.3936, -0.3267,  0.0321]],

        [[-0.0217,  0.0451,  0.0517,  ...,  0.3936, -0.3267,  0.0321]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 1 36 0
layer name:  MLP
i,j,k 1 37 0
layer name:  OutputEmbed
i,j,k 2 0 0
layer name:  InputEmbed
i,j,k 2 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1512,  0.1692,  0.1020,  ...,  0.1146, -0.2003,  0.3523]],

        [[-0.1512,  0.1692,  0.1020,  ...,  0.1146, -0.2003,  0.3523]],

        [[-0.1512,  0.1692,  0.1020,  ...,  0.1146, -0.2003,  0.3523]],

        [[-0.1512,  0.1692,  0.1020,  ...,  0.1146, -0.2003,  0.3523]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0707, -0.0127, -0.0094,  ..., -0.0038, -0.0911,  0.0991]],

        [[-0.0707, -0.0127, -0.0094,  ..., -0.0038, -0.0911,  0.0991]],

        [[-0.0707, -0.0127, -0.0094,  ..., -0.0038, -0.0911,  0.0991]],

        [[-0.0707, -0.0127, -0.0094,  ..., -0.0038, -0.0911,  0.0991]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 3 0
layer name:  MLP
i,j,k 2 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1766,  0.1302,  0.0407,  ...,  0.0816, -0.3394,  0.4087]],

        [[-0.1766,  0.1302,  0.0407,  ...,  0.0816, -0.3394,  0.4087]],

        [[-0.1766,  0.1302,  0.0407,  ...,  0.0816, -0.3394,  0.4087]],

        [[-0.1766,  0.1302,  0.0407,  ...,  0.0816, -0.3394,  0.4087]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0729, -0.0081, -0.0233,  ..., -0.0161, -0.1000,  0.0859]],

        [[-0.0729, -0.0081, -0.0233,  ..., -0.0161, -0.1000,  0.0859]],

        [[-0.0729, -0.0081, -0.0233,  ..., -0.0161, -0.1000,  0.0859]],

        [[-0.0729, -0.0081, -0.0233,  ..., -0.0161, -0.1000,  0.0859]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 6 0
layer name:  MLP
i,j,k 2 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1163,  0.0912, -0.0649,  ...,  0.1143, -0.6455,  0.5791]],

        [[-0.1163,  0.0912, -0.0649,  ...,  0.1143, -0.6455,  0.5791]],

        [[-0.1163,  0.0912, -0.0649,  ...,  0.1143, -0.6455,  0.5791]],

        [[-0.1163,  0.0912, -0.0649,  ...,  0.1143, -0.6455,  0.5791]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0522, -0.0171, -0.0422,  ..., -0.0153, -0.1260,  0.0683]],

        [[-0.0522, -0.0171, -0.0422,  ..., -0.0153, -0.1260,  0.0683]],

        [[-0.0522, -0.0171, -0.0422,  ..., -0.0153, -0.1260,  0.0683]],

        [[-0.0522, -0.0171, -0.0422,  ..., -0.0153, -0.1260,  0.0683]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 9 0
layer name:  MLP
i,j,k 2 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0853, -0.0556, -0.0709,  ..., -0.0691, -0.6133,  0.4863]],

        [[-0.0853, -0.0556, -0.0709,  ..., -0.0691, -0.6133,  0.4863]],

        [[-0.0853, -0.0556, -0.0709,  ..., -0.0691, -0.6133,  0.4863]],

        [[-0.0853, -0.0556, -0.0709,  ..., -0.0691, -0.6133,  0.4863]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0483, -0.0408, -0.0443,  ..., -0.0452, -0.1364,  0.0616]],

        [[-0.0483, -0.0408, -0.0443,  ..., -0.0452, -0.1364,  0.0616]],

        [[-0.0483, -0.0408, -0.0443,  ..., -0.0452, -0.1364,  0.0616]],

        [[-0.0483, -0.0408, -0.0443,  ..., -0.0452, -0.1364,  0.0616]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 12 0
layer name:  MLP
i,j,k 2 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0798, -0.1665, -0.0355,  ..., -0.0596, -0.7817,  0.4976]],

        [[-0.0798, -0.1665, -0.0355,  ..., -0.0596, -0.7817,  0.4976]],

        [[-0.0798, -0.1665, -0.0355,  ..., -0.0596, -0.7817,  0.4976]],

        [[-0.0798, -0.1665, -0.0355,  ..., -0.0596, -0.7817,  0.4976]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0456, -0.0587, -0.0375,  ..., -0.0418, -0.1564,  0.0547]],

        [[-0.0456, -0.0587, -0.0375,  ..., -0.0418, -0.1564,  0.0547]],

        [[-0.0456, -0.0587, -0.0375,  ..., -0.0418, -0.1564,  0.0547]],

        [[-0.0456, -0.0587, -0.0375,  ..., -0.0418, -0.1564,  0.0547]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 15 0
layer name:  MLP
i,j,k 2 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0253, -0.1086,  0.1357,  ..., -0.2378, -0.9653,  0.1177]],

        [[ 0.0253, -0.1086,  0.1357,  ..., -0.2378, -0.9653,  0.1177]],

        [[ 0.0253, -0.1086,  0.1357,  ..., -0.2378, -0.9653,  0.1177]],

        [[ 0.0253, -0.1086,  0.1357,  ..., -0.2378, -0.9653,  0.1177]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0281, -0.0509, -0.0083,  ..., -0.0739, -0.2006, -0.0094]],

        [[-0.0281, -0.0509, -0.0083,  ..., -0.0739, -0.2006, -0.0094]],

        [[-0.0281, -0.0509, -0.0083,  ..., -0.0739, -0.2006, -0.0094]],

        [[-0.0281, -0.0509, -0.0083,  ..., -0.0739, -0.2006, -0.0094]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 18 0
layer name:  MLP
i,j,k 2 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0543, -0.1987, -0.0670,  ..., -0.4451, -0.7178,  0.0693]],

        [[-0.0543, -0.1987, -0.0670,  ..., -0.4451, -0.7178,  0.0693]],

        [[-0.0543, -0.1987, -0.0670,  ..., -0.4451, -0.7178,  0.0693]],

        [[-0.0543, -0.1987, -0.0670,  ..., -0.4451, -0.7178,  0.0693]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0426, -0.0721, -0.0450,  ..., -0.1186, -0.1715, -0.0151]],

        [[-0.0426, -0.0721, -0.0450,  ..., -0.1186, -0.1715, -0.0151]],

        [[-0.0426, -0.0721, -0.0450,  ..., -0.1186, -0.1715, -0.0151]],

        [[-0.0426, -0.0721, -0.0450,  ..., -0.1186, -0.1715, -0.0151]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 21 0
layer name:  MLP
i,j,k 2 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1371, -0.1500, -0.0974,  ..., -1.0518, -0.8105, -0.1124]],

        [[-0.1371, -0.1500, -0.0974,  ..., -1.0518, -0.8105, -0.1124]],

        [[-0.1371, -0.1500, -0.0974,  ..., -1.0518, -0.8105, -0.1124]],

        [[-0.1371, -0.1500, -0.0974,  ..., -1.0518, -0.8105, -0.1124]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0601, -0.0602, -0.0513,  ..., -0.2710, -0.2092, -0.0557]],

        [[-0.0601, -0.0602, -0.0513,  ..., -0.2710, -0.2092, -0.0557]],

        [[-0.0601, -0.0602, -0.0513,  ..., -0.2710, -0.2092, -0.0557]],

        [[-0.0601, -0.0602, -0.0513,  ..., -0.2710, -0.2092, -0.0557]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 24 0
layer name:  MLP
i,j,k 2 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0328, -0.4697, -0.1759,  ..., -1.2295, -0.7524, -0.3557]],

        [[ 0.0328, -0.4697, -0.1759,  ..., -1.2295, -0.7524, -0.3557]],

        [[ 0.0328, -0.4697, -0.1759,  ..., -1.2295, -0.7524, -0.3557]],

        [[ 0.0328, -0.4697, -0.1759,  ..., -1.2295, -0.7524, -0.3557]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0170, -0.1626, -0.0757,  ..., -0.3506, -0.2269, -0.1260]],

        [[-0.0170, -0.1626, -0.0757,  ..., -0.3506, -0.2269, -0.1260]],

        [[-0.0170, -0.1626, -0.0757,  ..., -0.3506, -0.2269, -0.1260]],

        [[-0.0170, -0.1626, -0.0757,  ..., -0.3506, -0.2269, -0.1260]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 27 0
layer name:  MLP
i,j,k 2 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2086, -0.3574, -0.0748,  ..., -1.0322, -0.4990, -0.6216]],

        [[ 0.2086, -0.3574, -0.0748,  ..., -1.0322, -0.4990, -0.6216]],

        [[ 0.2086, -0.3574, -0.0748,  ..., -1.0322, -0.4990, -0.6216]],

        [[ 0.2086, -0.3574, -0.0748,  ..., -1.0322, -0.4990, -0.6216]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0579, -0.1299, -0.0538,  ..., -0.4097, -0.2097, -0.2539]],

        [[ 0.0579, -0.1299, -0.0538,  ..., -0.4097, -0.2097, -0.2539]],

        [[ 0.0579, -0.1299, -0.0538,  ..., -0.4097, -0.2097, -0.2539]],

        [[ 0.0579, -0.1299, -0.0538,  ..., -0.4097, -0.2097, -0.2539]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 30 0
layer name:  MLP
i,j,k 2 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0150, -0.9648,  0.2888,  ..., -0.9810, -0.3025, -0.5972]],

        [[ 0.0150, -0.9648,  0.2888,  ..., -0.9810, -0.3025, -0.5972]],

        [[ 0.0150, -0.9648,  0.2888,  ..., -0.9810, -0.3025, -0.5972]],

        [[ 0.0150, -0.9648,  0.2888,  ..., -0.9810, -0.3025, -0.5972]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0049, -0.6816,  0.1312,  ..., -0.5205, -0.1770, -0.3076]],

        [[-0.0049, -0.6816,  0.1312,  ..., -0.5205, -0.1770, -0.3076]],

        [[-0.0049, -0.6816,  0.1312,  ..., -0.5205, -0.1770, -0.3076]],

        [[-0.0049, -0.6816,  0.1312,  ..., -0.5205, -0.1770, -0.3076]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 33 0
layer name:  MLP
i,j,k 2 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0810, -1.8594,  0.3098,  ..., -1.0312, -0.3645, -0.6235]],

        [[ 0.0810, -1.8594,  0.3098,  ..., -1.0312, -0.3645, -0.6235]],

        [[ 0.0810, -1.8594,  0.3098,  ..., -1.0312, -0.3645, -0.6235]],

        [[ 0.0810, -1.8594,  0.3098,  ..., -1.0312, -0.3645, -0.6235]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 2 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0509, -1.1836,  0.2157,  ..., -0.6870, -0.2800, -0.3960]],

        [[ 0.0509, -1.1836,  0.2157,  ..., -0.6870, -0.2800, -0.3960]],

        [[ 0.0509, -1.1836,  0.2157,  ..., -0.6870, -0.2800, -0.3960]],

        [[ 0.0509, -1.1836,  0.2157,  ..., -0.6870, -0.2800, -0.3960]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 2 36 0
layer name:  MLP
i,j,k 2 37 0
layer name:  OutputEmbed
i,j,k 3 0 0
layer name:  InputEmbed
i,j,k 3 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1628,  0.2260,  0.2949,  ..., -0.3198, -0.0120,  0.4480]],

        [[-0.1628,  0.2260,  0.2949,  ..., -0.3198, -0.0120,  0.4480]],

        [[-0.1628,  0.2260,  0.2949,  ..., -0.3198, -0.0120,  0.4480]],

        [[-0.1628,  0.2260,  0.2949,  ..., -0.3198, -0.0120,  0.4480]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0774, -0.0086,  0.0284,  ..., -0.1169, -0.0512,  0.1114]],

        [[-0.0774, -0.0086,  0.0284,  ..., -0.1169, -0.0512,  0.1114]],

        [[-0.0774, -0.0086,  0.0284,  ..., -0.1169, -0.0512,  0.1114]],

        [[-0.0774, -0.0086,  0.0284,  ..., -0.1169, -0.0512,  0.1114]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 3 0
layer name:  MLP
i,j,k 3 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1548,  0.1686,  0.2139,  ..., -0.2507, -0.1687,  0.5405]],

        [[-0.1548,  0.1686,  0.2139,  ..., -0.2507, -0.1687,  0.5405]],

        [[-0.1548,  0.1686,  0.2139,  ..., -0.2507, -0.1687,  0.5405]],

        [[-0.1548,  0.1686,  0.2139,  ..., -0.2507, -0.1687,  0.5405]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0729, -0.0117,  0.0052,  ..., -0.0953, -0.0732,  0.1008]],

        [[-0.0729, -0.0117,  0.0052,  ..., -0.0953, -0.0732,  0.1008]],

        [[-0.0729, -0.0117,  0.0052,  ..., -0.0953, -0.0732,  0.1008]],

        [[-0.0729, -0.0117,  0.0052,  ..., -0.0953, -0.0732,  0.1008]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 6 0
layer name:  MLP
i,j,k 3 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0048,  0.1796,  0.1931,  ..., -0.1321, -0.4521,  0.6084]],

        [[ 0.0048,  0.1796,  0.1931,  ..., -0.1321, -0.4521,  0.6084]],

        [[ 0.0048,  0.1796,  0.1931,  ..., -0.1321, -0.4521,  0.6084]],

        [[ 0.0048,  0.1796,  0.1931,  ..., -0.1321, -0.4521,  0.6084]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0397, -0.0104, -0.0080,  ..., -0.0604, -0.1049,  0.0643]],

        [[-0.0397, -0.0104, -0.0080,  ..., -0.0604, -0.1049,  0.0643]],

        [[-0.0397, -0.0104, -0.0080,  ..., -0.0604, -0.1049,  0.0643]],

        [[-0.0397, -0.0104, -0.0080,  ..., -0.0604, -0.1049,  0.0643]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 9 0
layer name:  MLP
i,j,k 3 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0421,  0.0943,  0.2283,  ..., -0.0038, -0.3384,  0.4556]],

        [[ 0.0421,  0.0943,  0.2283,  ..., -0.0038, -0.3384,  0.4556]],

        [[ 0.0421,  0.0943,  0.2283,  ..., -0.0038, -0.3384,  0.4556]],

        [[ 0.0421,  0.0943,  0.2283,  ..., -0.0038, -0.3384,  0.4556]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0331, -0.0208, -0.0013,  ..., -0.0412, -0.0959,  0.0452]],

        [[-0.0331, -0.0208, -0.0013,  ..., -0.0412, -0.0959,  0.0452]],

        [[-0.0331, -0.0208, -0.0013,  ..., -0.0412, -0.0959,  0.0452]],

        [[-0.0331, -0.0208, -0.0013,  ..., -0.0412, -0.0959,  0.0452]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 12 0
layer name:  MLP
i,j,k 3 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-3.8391e-02,  1.4520e-04,  1.1499e-01,  ...,  3.3539e-02,
          -3.8330e-01,  3.6792e-01]],

        [[-3.8391e-02,  1.4520e-04,  1.1499e-01,  ...,  3.3539e-02,
          -3.8330e-01,  3.6792e-01]],

        [[-3.8391e-02,  1.4520e-04,  1.1499e-01,  ...,  3.3539e-02,
          -3.8330e-01,  3.6792e-01]],

        [[-3.8391e-02,  1.4520e-04,  1.1499e-01,  ...,  3.3539e-02,
          -3.8330e-01,  3.6792e-01]]], device='cuda:0', dtype=torch.float16)
i,j,k 3 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0467, -0.0393, -0.0217,  ..., -0.0349, -0.0999,  0.0236]],

        [[-0.0467, -0.0393, -0.0217,  ..., -0.0349, -0.0999,  0.0236]],

        [[-0.0467, -0.0393, -0.0217,  ..., -0.0349, -0.0999,  0.0236]],

        [[-0.0467, -0.0393, -0.0217,  ..., -0.0349, -0.0999,  0.0236]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 15 0
layer name:  MLP
i,j,k 3 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0451,  0.1541,  0.0933,  ..., -0.0136, -0.5283,  0.0409]],

        [[ 0.0451,  0.1541,  0.0933,  ..., -0.0136, -0.5283,  0.0409]],

        [[ 0.0451,  0.1541,  0.0933,  ..., -0.0136, -0.5283,  0.0409]],

        [[ 0.0451,  0.1541,  0.0933,  ..., -0.0136, -0.5283,  0.0409]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0319, -0.0071, -0.0231,  ..., -0.0422, -0.1328, -0.0313]],

        [[-0.0319, -0.0071, -0.0231,  ..., -0.0422, -0.1328, -0.0313]],

        [[-0.0319, -0.0071, -0.0231,  ..., -0.0422, -0.1328, -0.0313]],

        [[-0.0319, -0.0071, -0.0231,  ..., -0.0422, -0.1328, -0.0313]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 18 0
layer name:  MLP
i,j,k 3 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0612,  0.1317, -0.0489,  ..., -0.0097, -0.1396,  0.0270]],

        [[ 0.0612,  0.1317, -0.0489,  ..., -0.0097, -0.1396,  0.0270]],

        [[ 0.0612,  0.1317, -0.0489,  ..., -0.0097, -0.1396,  0.0270]],

        [[ 0.0612,  0.1317, -0.0489,  ..., -0.0097, -0.1396,  0.0270]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0250, -0.0069, -0.0485,  ..., -0.0406, -0.0662, -0.0307]],

        [[-0.0250, -0.0069, -0.0485,  ..., -0.0406, -0.0662, -0.0307]],

        [[-0.0250, -0.0069, -0.0485,  ..., -0.0406, -0.0662, -0.0307]],

        [[-0.0250, -0.0069, -0.0485,  ..., -0.0406, -0.0662, -0.0307]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 21 0
layer name:  MLP
i,j,k 3 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0384, -0.2246,  0.0879,  ..., -0.0315, -0.1666,  0.0398]],

        [[-0.0384, -0.2246,  0.0879,  ..., -0.0315, -0.1666,  0.0398]],

        [[-0.0384, -0.2246,  0.0879,  ..., -0.0315, -0.1666,  0.0398]],

        [[-0.0384, -0.2246,  0.0879,  ..., -0.0315, -0.1666,  0.0398]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0446, -0.0844, -0.0166,  ..., -0.0453, -0.0731, -0.0284]],

        [[-0.0446, -0.0844, -0.0166,  ..., -0.0453, -0.0731, -0.0284]],

        [[-0.0446, -0.0844, -0.0166,  ..., -0.0453, -0.0731, -0.0284]],

        [[-0.0446, -0.0844, -0.0166,  ..., -0.0453, -0.0731, -0.0284]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 24 0
layer name:  MLP
i,j,k 3 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0472, -0.6348,  0.0275,  ...,  0.1066, -0.2122, -0.2625]],

        [[-0.0472, -0.6348,  0.0275,  ...,  0.1066, -0.2122, -0.2625]],

        [[-0.0472, -0.6348,  0.0275,  ...,  0.1066, -0.2122, -0.2625]],

        [[-0.0472, -0.6348,  0.0275,  ...,  0.1066, -0.2122, -0.2625]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0471, -0.2163, -0.0291,  ..., -0.0125, -0.0908, -0.1072]],

        [[-0.0471, -0.2163, -0.0291,  ..., -0.0125, -0.0908, -0.1072]],

        [[-0.0471, -0.2163, -0.0291,  ..., -0.0125, -0.0908, -0.1072]],

        [[-0.0471, -0.2163, -0.0291,  ..., -0.0125, -0.0908, -0.1072]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 27 0
layer name:  MLP
i,j,k 3 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-7.7026e-02, -8.0566e-01, -2.4719e-02,  ...,  3.6450e-01,
           3.1686e-04, -1.1627e-01]],

        [[-7.7026e-02, -8.0566e-01, -2.4719e-02,  ...,  3.6450e-01,
           3.1686e-04, -1.1627e-01]],

        [[-7.7026e-02, -8.0566e-01, -2.4719e-02,  ...,  3.6450e-01,
           3.1686e-04, -1.1627e-01]],

        [[-7.7026e-02, -8.0566e-01, -2.4719e-02,  ...,  3.6450e-01,
           3.1686e-04, -1.1627e-01]]], device='cuda:0', dtype=torch.float16)
i,j,k 3 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0588, -0.2949, -0.0447,  ...,  0.0953, -0.0334, -0.0765]],

        [[-0.0588, -0.2949, -0.0447,  ...,  0.0953, -0.0334, -0.0765]],

        [[-0.0588, -0.2949, -0.0447,  ...,  0.0953, -0.0334, -0.0765]],

        [[-0.0588, -0.2949, -0.0447,  ...,  0.0953, -0.0334, -0.0765]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 30 0
layer name:  MLP
i,j,k 3 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3936, -0.9072,  0.1714,  ...,  0.7686,  0.2477, -0.1221]],

        [[-0.3936, -0.9072,  0.1714,  ...,  0.7686,  0.2477, -0.1221]],

        [[-0.3936, -0.9072,  0.1714,  ...,  0.7686,  0.2477, -0.1221]],

        [[-0.3936, -0.9072,  0.1714,  ...,  0.7686,  0.2477, -0.1221]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2314, -0.6763,  0.0582,  ...,  0.3691,  0.0953, -0.0941]],

        [[-0.2314, -0.6763,  0.0582,  ...,  0.3691,  0.0953, -0.0941]],

        [[-0.2314, -0.6763,  0.0582,  ...,  0.3691,  0.0953, -0.0941]],

        [[-0.2314, -0.6763,  0.0582,  ...,  0.3691,  0.0953, -0.0941]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 33 0
layer name:  MLP
i,j,k 3 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2034, -2.4824,  0.3726,  ...,  0.7915,  0.2314, -0.1649]],

        [[-0.2034, -2.4824,  0.3726,  ...,  0.7915,  0.2314, -0.1649]],

        [[-0.2034, -2.4824,  0.3726,  ...,  0.7915,  0.2314, -0.1649]],

        [[-0.2034, -2.4824,  0.3726,  ...,  0.7915,  0.2314, -0.1649]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 3 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1633, -1.6777,  0.2485,  ...,  0.5225,  0.1357, -0.1304]],

        [[-0.1633, -1.6777,  0.2485,  ...,  0.5225,  0.1357, -0.1304]],

        [[-0.1633, -1.6777,  0.2485,  ...,  0.5225,  0.1357, -0.1304]],

        [[-0.1633, -1.6777,  0.2485,  ...,  0.5225,  0.1357, -0.1304]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 3 36 0
layer name:  MLP
i,j,k 3 37 0
layer name:  OutputEmbed
i,j,k 4 0 0
layer name:  InputEmbed
i,j,k 4 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3762, -0.2239,  0.0475,  ...,  0.1005,  0.1597, -0.0304]],

        [[-0.3762, -0.2239,  0.0475,  ...,  0.1005,  0.1597, -0.0304]],

        [[-0.3762, -0.2239,  0.0475,  ...,  0.1005,  0.1597, -0.0304]],

        [[-0.3762, -0.2239,  0.0475,  ...,  0.1005,  0.1597, -0.0304]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0886, -0.0767, -0.0035,  ...,  0.0103,  0.0142, -0.0108]],

        [[-0.0886, -0.0767, -0.0035,  ...,  0.0103,  0.0142, -0.0108]],

        [[-0.0886, -0.0767, -0.0035,  ...,  0.0103,  0.0142, -0.0108]],

        [[-0.0886, -0.0767, -0.0035,  ...,  0.0103,  0.0142, -0.0108]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 3 0
layer name:  MLP
i,j,k 4 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3550, -0.3499,  0.1329,  ...,  0.0163,  0.0152, -0.0581]],

        [[-0.3550, -0.3499,  0.1329,  ...,  0.0163,  0.0152, -0.0581]],

        [[-0.3550, -0.3499,  0.1329,  ...,  0.0163,  0.0152, -0.0581]],

        [[-0.3550, -0.3499,  0.1329,  ...,  0.0163,  0.0152, -0.0581]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0895, -0.0665,  0.0193,  ..., -0.0094, -0.0127, -0.0146]],

        [[-0.0895, -0.0665,  0.0193,  ..., -0.0094, -0.0127, -0.0146]],

        [[-0.0895, -0.0665,  0.0193,  ..., -0.0094, -0.0127, -0.0146]],

        [[-0.0895, -0.0665,  0.0193,  ..., -0.0094, -0.0127, -0.0146]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 6 0
layer name:  MLP
i,j,k 4 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2847, -0.3550,  0.1781,  ..., -0.3210, -0.2834, -0.1946]],

        [[-0.2847, -0.3550,  0.1781,  ..., -0.3210, -0.2834, -0.1946]],

        [[-0.2847, -0.3550,  0.1781,  ..., -0.3210, -0.2834, -0.1946]],

        [[-0.2847, -0.3550,  0.1781,  ..., -0.3210, -0.2834, -0.1946]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0631, -0.0745,  0.0222,  ..., -0.0662, -0.0572, -0.0415]],

        [[-0.0631, -0.0745,  0.0222,  ..., -0.0662, -0.0572, -0.0415]],

        [[-0.0631, -0.0745,  0.0222,  ..., -0.0662, -0.0572, -0.0415]],

        [[-0.0631, -0.0745,  0.0222,  ..., -0.0662, -0.0572, -0.0415]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 9 0
layer name:  MLP
i,j,k 4 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3103, -0.3333,  0.3662,  ..., -0.2202, -0.2432, -0.3489]],

        [[-0.3103, -0.3333,  0.3662,  ..., -0.2202, -0.2432, -0.3489]],

        [[-0.3103, -0.3333,  0.3662,  ..., -0.2202, -0.2432, -0.3489]],

        [[-0.3103, -0.3333,  0.3662,  ..., -0.2202, -0.2432, -0.3489]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0729, -0.0773,  0.0584,  ..., -0.0544, -0.0560, -0.0791]],

        [[-0.0729, -0.0773,  0.0584,  ..., -0.0544, -0.0560, -0.0791]],

        [[-0.0729, -0.0773,  0.0584,  ..., -0.0544, -0.0560, -0.0791]],

        [[-0.0729, -0.0773,  0.0584,  ..., -0.0544, -0.0560, -0.0791]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 12 0
layer name:  MLP
i,j,k 4 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3740, -0.1720,  0.6216,  ..., -0.3269, -0.0396, -0.4404]],

        [[-0.3740, -0.1720,  0.6216,  ..., -0.3269, -0.0396, -0.4404]],

        [[-0.3740, -0.1720,  0.6216,  ..., -0.3269, -0.0396, -0.4404]],

        [[-0.3740, -0.1720,  0.6216,  ..., -0.3269, -0.0396, -0.4404]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0759, -0.0399,  0.0984,  ..., -0.0670, -0.0168, -0.0914]],

        [[-0.0759, -0.0399,  0.0984,  ..., -0.0670, -0.0168, -0.0914]],

        [[-0.0759, -0.0399,  0.0984,  ..., -0.0670, -0.0168, -0.0914]],

        [[-0.0759, -0.0399,  0.0984,  ..., -0.0670, -0.0168, -0.0914]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 15 0
layer name:  MLP
i,j,k 4 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2607, -0.1346,  0.6792,  ..., -0.3735, -0.2178, -0.6089]],

        [[-0.2607, -0.1346,  0.6792,  ..., -0.3735, -0.2178, -0.6089]],

        [[-0.2607, -0.1346,  0.6792,  ..., -0.3735, -0.2178, -0.6089]],

        [[-0.2607, -0.1346,  0.6792,  ..., -0.3735, -0.2178, -0.6089]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0656, -0.0366,  0.1244,  ..., -0.0834, -0.0519, -0.1377]],

        [[-0.0656, -0.0366,  0.1244,  ..., -0.0834, -0.0519, -0.1377]],

        [[-0.0656, -0.0366,  0.1244,  ..., -0.0834, -0.0519, -0.1377]],

        [[-0.0656, -0.0366,  0.1244,  ..., -0.0834, -0.0519, -0.1377]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 18 0
layer name:  MLP
i,j,k 4 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2815, -0.1780,  0.5200,  ..., -0.6084, -0.2224, -0.6299]],

        [[-0.2815, -0.1780,  0.5200,  ..., -0.6084, -0.2224, -0.6299]],

        [[-0.2815, -0.1780,  0.5200,  ..., -0.6084, -0.2224, -0.6299]],

        [[-0.2815, -0.1780,  0.5200,  ..., -0.6084, -0.2224, -0.6299]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0745, -0.0492,  0.1066,  ..., -0.1426, -0.0574, -0.1582]],

        [[-0.0745, -0.0492,  0.1066,  ..., -0.1426, -0.0574, -0.1582]],

        [[-0.0745, -0.0492,  0.1066,  ..., -0.1426, -0.0574, -0.1582]],

        [[-0.0745, -0.0492,  0.1066,  ..., -0.1426, -0.0574, -0.1582]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 21 0
layer name:  MLP
i,j,k 4 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2183,  0.0798,  0.6196,  ..., -0.8247, -0.2117, -0.7593]],

        [[-0.2183,  0.0798,  0.6196,  ..., -0.8247, -0.2117, -0.7593]],

        [[-0.2183,  0.0798,  0.6196,  ..., -0.8247, -0.2117, -0.7593]],

        [[-0.2183,  0.0798,  0.6196,  ..., -0.8247, -0.2117, -0.7593]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0599,  0.0190,  0.1477,  ..., -0.2100, -0.0571, -0.1951]],

        [[-0.0599,  0.0190,  0.1477,  ..., -0.2100, -0.0571, -0.1951]],

        [[-0.0599,  0.0190,  0.1477,  ..., -0.2100, -0.0571, -0.1951]],

        [[-0.0599,  0.0190,  0.1477,  ..., -0.2100, -0.0571, -0.1951]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 24 0
layer name:  MLP
i,j,k 4 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1792,  0.2002,  0.6226,  ..., -0.9136,  0.0260, -0.7524]],

        [[-0.1792,  0.2002,  0.6226,  ..., -0.9136,  0.0260, -0.7524]],

        [[-0.1792,  0.2002,  0.6226,  ..., -0.9136,  0.0260, -0.7524]],

        [[-0.1792,  0.2002,  0.6226,  ..., -0.9136,  0.0260, -0.7524]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0579,  0.0704,  0.1829,  ..., -0.2732,  0.0030, -0.2345]],

        [[-0.0579,  0.0704,  0.1829,  ..., -0.2732,  0.0030, -0.2345]],

        [[-0.0579,  0.0704,  0.1829,  ..., -0.2732,  0.0030, -0.2345]],

        [[-0.0579,  0.0704,  0.1829,  ..., -0.2732,  0.0030, -0.2345]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 27 0
layer name:  MLP
i,j,k 4 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1028,  0.8794,  0.5762,  ..., -0.7383, -0.0726, -0.4746]],

        [[-0.1028,  0.8794,  0.5762,  ..., -0.7383, -0.0726, -0.4746]],

        [[-0.1028,  0.8794,  0.5762,  ..., -0.7383, -0.0726, -0.4746]],

        [[-0.1028,  0.8794,  0.5762,  ..., -0.7383, -0.0726, -0.4746]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0377,  0.3506,  0.2206,  ..., -0.2849, -0.0289, -0.1818]],

        [[-0.0377,  0.3506,  0.2206,  ..., -0.2849, -0.0289, -0.1818]],

        [[-0.0377,  0.3506,  0.2206,  ..., -0.2849, -0.0289, -0.1818]],

        [[-0.0377,  0.3506,  0.2206,  ..., -0.2849, -0.0289, -0.1818]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 30 0
layer name:  MLP
i,j,k 4 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1136,  0.8101,  0.5728,  ..., -0.5503, -0.2239, -0.5078]],

        [[ 0.1136,  0.8101,  0.5728,  ..., -0.5503, -0.2239, -0.5078]],

        [[ 0.1136,  0.8101,  0.5728,  ..., -0.5503, -0.2239, -0.5078]],

        [[ 0.1136,  0.8101,  0.5728,  ..., -0.5503, -0.2239, -0.5078]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0649,  0.6650,  0.3018,  ..., -0.2849, -0.1183, -0.2476]],

        [[ 0.0649,  0.6650,  0.3018,  ..., -0.2849, -0.1183, -0.2476]],

        [[ 0.0649,  0.6650,  0.3018,  ..., -0.2849, -0.1183, -0.2476]],

        [[ 0.0649,  0.6650,  0.3018,  ..., -0.2849, -0.1183, -0.2476]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 33 0
layer name:  MLP
i,j,k 4 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3152,  1.5371,  0.8560,  ..., -0.6221, -0.2278, -0.6099]],

        [[ 0.3152,  1.5371,  0.8560,  ..., -0.6221, -0.2278, -0.6099]],

        [[ 0.3152,  1.5371,  0.8560,  ..., -0.6221, -0.2278, -0.6099]],

        [[ 0.3152,  1.5371,  0.8560,  ..., -0.6221, -0.2278, -0.6099]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 4 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2045,  1.0156,  0.5635,  ..., -0.3638, -0.1500, -0.3384]],

        [[ 0.2045,  1.0156,  0.5635,  ..., -0.3638, -0.1500, -0.3384]],

        [[ 0.2045,  1.0156,  0.5635,  ..., -0.3638, -0.1500, -0.3384]],

        [[ 0.2045,  1.0156,  0.5635,  ..., -0.3638, -0.1500, -0.3384]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 4 36 0
layer name:  MLP
i,j,k 4 37 0
layer name:  OutputEmbed
i,j,k 5 0 0
layer name:  InputEmbed
i,j,k 5 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1017,  0.0551, -0.0167,  ...,  0.2450,  0.0374, -0.3601]],

        [[-0.1017,  0.0551, -0.0167,  ...,  0.2450,  0.0374, -0.3601]],

        [[-0.1017,  0.0551, -0.0167,  ...,  0.2450,  0.0374, -0.3601]],

        [[-0.1017,  0.0551, -0.0167,  ...,  0.2450,  0.0374, -0.3601]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0101,  0.0055,  0.0057,  ...,  0.0667,  0.0125, -0.0847]],

        [[-0.0101,  0.0055,  0.0057,  ...,  0.0667,  0.0125, -0.0847]],

        [[-0.0101,  0.0055,  0.0057,  ...,  0.0667,  0.0125, -0.0847]],

        [[-0.0101,  0.0055,  0.0057,  ...,  0.0667,  0.0125, -0.0847]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 3 0
layer name:  MLP
i,j,k 5 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0800, -0.1361, -0.1525,  ...,  0.3020,  0.0293, -0.2976]],

        [[-0.0800, -0.1361, -0.1525,  ...,  0.3020,  0.0293, -0.2976]],

        [[-0.0800, -0.1361, -0.1525,  ...,  0.3020,  0.0293, -0.2976]],

        [[-0.0800, -0.1361, -0.1525,  ...,  0.3020,  0.0293, -0.2976]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0069, -0.0086, -0.0241,  ...,  0.0825,  0.0135, -0.0566]],

        [[-0.0069, -0.0086, -0.0241,  ...,  0.0825,  0.0135, -0.0566]],

        [[-0.0069, -0.0086, -0.0241,  ...,  0.0825,  0.0135, -0.0566]],

        [[-0.0069, -0.0086, -0.0241,  ...,  0.0825,  0.0135, -0.0566]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 6 0
layer name:  MLP
i,j,k 5 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0035,  0.0022, -0.2859,  ...,  0.2426, -0.0106, -0.3491]],

        [[-0.0035,  0.0022, -0.2859,  ...,  0.2426, -0.0106, -0.3491]],

        [[-0.0035,  0.0022, -0.2859,  ...,  0.2426, -0.0106, -0.3491]],

        [[-0.0035,  0.0022, -0.2859,  ...,  0.2426, -0.0106, -0.3491]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0114,  0.0137, -0.0401,  ...,  0.0555,  0.0092, -0.0490]],

        [[ 0.0114,  0.0137, -0.0401,  ...,  0.0555,  0.0092, -0.0490]],

        [[ 0.0114,  0.0137, -0.0401,  ...,  0.0555,  0.0092, -0.0490]],

        [[ 0.0114,  0.0137, -0.0401,  ...,  0.0555,  0.0092, -0.0490]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 9 0
layer name:  MLP
i,j,k 5 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0679,  0.0164, -0.2023,  ...,  0.2067,  0.0134, -0.4133]],

        [[-0.0679,  0.0164, -0.2023,  ...,  0.2067,  0.0134, -0.4133]],

        [[-0.0679,  0.0164, -0.2023,  ...,  0.2067,  0.0134, -0.4133]],

        [[-0.0679,  0.0164, -0.2023,  ...,  0.2067,  0.0134, -0.4133]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0019,  0.0187, -0.0272,  ...,  0.0552,  0.0149, -0.0732]],

        [[-0.0019,  0.0187, -0.0272,  ...,  0.0552,  0.0149, -0.0732]],

        [[-0.0019,  0.0187, -0.0272,  ...,  0.0552,  0.0149, -0.0732]],

        [[-0.0019,  0.0187, -0.0272,  ...,  0.0552,  0.0149, -0.0732]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 12 0
layer name:  MLP
i,j,k 5 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1196, -0.1571, -0.1470,  ...,  0.1838,  0.2070, -0.3979]],

        [[-0.1196, -0.1571, -0.1470,  ...,  0.1838,  0.2070, -0.3979]],

        [[-0.1196, -0.1571, -0.1470,  ...,  0.1838,  0.2070, -0.3979]],

        [[-0.1196, -0.1571, -0.1470,  ...,  0.1838,  0.2070, -0.3979]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0095, -0.0157, -0.0134,  ...,  0.0490,  0.0536, -0.0662]],

        [[-0.0095, -0.0157, -0.0134,  ...,  0.0490,  0.0536, -0.0662]],

        [[-0.0095, -0.0157, -0.0134,  ...,  0.0490,  0.0536, -0.0662]],

        [[-0.0095, -0.0157, -0.0134,  ...,  0.0490,  0.0536, -0.0662]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 15 0
layer name:  MLP
i,j,k 5 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0829, -0.2498, -0.0820,  ...,  0.1932,  0.1683, -0.4065]],

        [[-0.0829, -0.2498, -0.0820,  ...,  0.1932,  0.1683, -0.4065]],

        [[-0.0829, -0.2498, -0.0820,  ...,  0.1932,  0.1683, -0.4065]],

        [[-0.0829, -0.2498, -0.0820,  ...,  0.1932,  0.1683, -0.4065]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0047, -0.0399, -0.0025,  ...,  0.0542,  0.0499, -0.0748]],

        [[-0.0047, -0.0399, -0.0025,  ...,  0.0542,  0.0499, -0.0748]],

        [[-0.0047, -0.0399, -0.0025,  ...,  0.0542,  0.0499, -0.0748]],

        [[-0.0047, -0.0399, -0.0025,  ...,  0.0542,  0.0499, -0.0748]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 18 0
layer name:  MLP
i,j,k 5 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1449, -0.1844, -0.0779,  ..., -0.0828,  0.1808, -0.3086]],

        [[-0.1449, -0.1844, -0.0779,  ..., -0.0828,  0.1808, -0.3086]],

        [[-0.1449, -0.1844, -0.0779,  ..., -0.0828,  0.1808, -0.3086]],

        [[-0.1449, -0.1844, -0.0779,  ..., -0.0828,  0.1808, -0.3086]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0200, -0.0289, -0.0031,  ..., -0.0041,  0.0571, -0.0605]],

        [[-0.0200, -0.0289, -0.0031,  ..., -0.0041,  0.0571, -0.0605]],

        [[-0.0200, -0.0289, -0.0031,  ..., -0.0041,  0.0571, -0.0605]],

        [[-0.0200, -0.0289, -0.0031,  ..., -0.0041,  0.0571, -0.0605]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 21 0
layer name:  MLP
i,j,k 5 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1262,  0.1455,  0.1399,  ..., -0.2786,  0.2369, -0.4351]],

        [[-0.1262,  0.1455,  0.1399,  ..., -0.2786,  0.2369, -0.4351]],

        [[-0.1262,  0.1455,  0.1399,  ..., -0.2786,  0.2369, -0.4351]],

        [[-0.1262,  0.1455,  0.1399,  ..., -0.2786,  0.2369, -0.4351]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0147,  0.0627,  0.0551,  ..., -0.0566,  0.0773, -0.0976]],

        [[-0.0147,  0.0627,  0.0551,  ..., -0.0566,  0.0773, -0.0976]],

        [[-0.0147,  0.0627,  0.0551,  ..., -0.0566,  0.0773, -0.0976]],

        [[-0.0147,  0.0627,  0.0551,  ..., -0.0566,  0.0773, -0.0976]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 24 0
layer name:  MLP
i,j,k 5 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0820,  0.2524,  0.0132,  ..., -0.3835,  0.2028, -0.4155]],

        [[-0.0820,  0.2524,  0.0132,  ..., -0.3835,  0.2028, -0.4155]],

        [[-0.0820,  0.2524,  0.0132,  ..., -0.3835,  0.2028, -0.4155]],

        [[-0.0820,  0.2524,  0.0132,  ..., -0.3835,  0.2028, -0.4155]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0048,  0.1175,  0.0236,  ..., -0.1012,  0.0829, -0.1146]],

        [[-0.0048,  0.1175,  0.0236,  ..., -0.1012,  0.0829, -0.1146]],

        [[-0.0048,  0.1175,  0.0236,  ..., -0.1012,  0.0829, -0.1146]],

        [[-0.0048,  0.1175,  0.0236,  ..., -0.1012,  0.0829, -0.1146]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 27 0
layer name:  MLP
i,j,k 5 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0013,  0.6855,  0.0935,  ..., -0.2656, -0.0042, -0.4951]],

        [[-0.0013,  0.6855,  0.0935,  ..., -0.2656, -0.0042, -0.4951]],

        [[-0.0013,  0.6855,  0.0935,  ..., -0.2656, -0.0042, -0.4951]],

        [[-0.0013,  0.6855,  0.0935,  ..., -0.2656, -0.0042, -0.4951]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0266,  0.3149,  0.0596,  ..., -0.0840,  0.0225, -0.1720]],

        [[ 0.0266,  0.3149,  0.0596,  ..., -0.0840,  0.0225, -0.1720]],

        [[ 0.0266,  0.3149,  0.0596,  ..., -0.0840,  0.0225, -0.1720]],

        [[ 0.0266,  0.3149,  0.0596,  ..., -0.0840,  0.0225, -0.1720]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 30 0
layer name:  MLP
i,j,k 5 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0178,  0.8955,  0.0174,  ...,  0.1769, -0.0117, -0.3979]],

        [[-0.0178,  0.8955,  0.0174,  ...,  0.1769, -0.0117, -0.3979]],

        [[-0.0178,  0.8955,  0.0174,  ...,  0.1769, -0.0117, -0.3979]],

        [[-0.0178,  0.8955,  0.0174,  ...,  0.1769, -0.0117, -0.3979]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0268,  0.7549,  0.0388,  ...,  0.1182,  0.0207, -0.1643]],

        [[ 0.0268,  0.7549,  0.0388,  ...,  0.1182,  0.0207, -0.1643]],

        [[ 0.0268,  0.7549,  0.0388,  ...,  0.1182,  0.0207, -0.1643]],

        [[ 0.0268,  0.7549,  0.0388,  ...,  0.1182,  0.0207, -0.1643]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 33 0
layer name:  MLP
i,j,k 5 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1576,  2.1016,  0.0368,  ..., -0.0286,  0.2722, -0.5479]],

        [[ 0.1576,  2.1016,  0.0368,  ..., -0.0286,  0.2722, -0.5479]],

        [[ 0.1576,  2.1016,  0.0368,  ..., -0.0286,  0.2722, -0.5479]],

        [[ 0.1576,  2.1016,  0.0368,  ..., -0.0286,  0.2722, -0.5479]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 5 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1382,  1.3750,  0.0679,  ...,  0.0278,  0.2090, -0.2676]],

        [[ 0.1382,  1.3750,  0.0679,  ...,  0.0278,  0.2090, -0.2676]],

        [[ 0.1382,  1.3750,  0.0679,  ...,  0.0278,  0.2090, -0.2676]],

        [[ 0.1382,  1.3750,  0.0679,  ...,  0.0278,  0.2090, -0.2676]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 5 36 0
layer name:  MLP
i,j,k 5 37 0
layer name:  OutputEmbed
i,j,k 6 0 0
layer name:  InputEmbed
i,j,k 6 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0948,  0.7285,  0.0129,  ..., -0.1547,  0.3035, -0.3892]],

        [[-0.0948,  0.7285,  0.0129,  ..., -0.1547,  0.3035, -0.3892]],

        [[-0.0948,  0.7285,  0.0129,  ..., -0.1547,  0.3035, -0.3892]],

        [[-0.0948,  0.7285,  0.0129,  ..., -0.1547,  0.3035, -0.3892]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0366,  0.1158, -0.0162,  ..., -0.0502,  0.0381, -0.1184]],

        [[-0.0366,  0.1158, -0.0162,  ..., -0.0502,  0.0381, -0.1184]],

        [[-0.0366,  0.1158, -0.0162,  ..., -0.0502,  0.0381, -0.1184]],

        [[-0.0366,  0.1158, -0.0162,  ..., -0.0502,  0.0381, -0.1184]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 3 0
layer name:  MLP
i,j,k 6 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1884,  0.7734, -0.0787,  ...,  0.0608,  0.3850, -0.3313]],

        [[-0.1884,  0.7734, -0.0787,  ...,  0.0608,  0.3850, -0.3313]],

        [[-0.1884,  0.7734, -0.0787,  ...,  0.0608,  0.3850, -0.3313]],

        [[-0.1884,  0.7734, -0.0787,  ...,  0.0608,  0.3850, -0.3313]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0579,  0.1157, -0.0336,  ..., -0.0040,  0.0501, -0.0900]],

        [[-0.0579,  0.1157, -0.0336,  ..., -0.0040,  0.0501, -0.0900]],

        [[-0.0579,  0.1157, -0.0336,  ..., -0.0040,  0.0501, -0.0900]],

        [[-0.0579,  0.1157, -0.0336,  ..., -0.0040,  0.0501, -0.0900]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 6 0
layer name:  MLP
i,j,k 6 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2764,  0.7266, -0.1401,  ...,  0.1173,  0.3484, -0.4592]],

        [[-0.2764,  0.7266, -0.1401,  ...,  0.1173,  0.3484, -0.4592]],

        [[-0.2764,  0.7266, -0.1401,  ...,  0.1173,  0.3484, -0.4592]],

        [[-0.2764,  0.7266, -0.1401,  ...,  0.1173,  0.3484, -0.4592]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0671,  0.1204, -0.0399,  ...,  0.0050,  0.0395, -0.0964]],

        [[-0.0671,  0.1204, -0.0399,  ...,  0.0050,  0.0395, -0.0964]],

        [[-0.0671,  0.1204, -0.0399,  ...,  0.0050,  0.0395, -0.0964]],

        [[-0.0671,  0.1204, -0.0399,  ...,  0.0050,  0.0395, -0.0964]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 9 0
layer name:  MLP
i,j,k 6 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3772,  0.5625, -0.0309,  ...,  0.1379,  0.2637, -0.5562]],

        [[-0.3772,  0.5625, -0.0309,  ...,  0.1379,  0.2637, -0.5562]],

        [[-0.3772,  0.5625, -0.0309,  ...,  0.1379,  0.2637, -0.5562]],

        [[-0.3772,  0.5625, -0.0309,  ...,  0.1379,  0.2637, -0.5562]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0948,  0.1101, -0.0208,  ...,  0.0132,  0.0358, -0.1320]],

        [[-0.0948,  0.1101, -0.0208,  ...,  0.0132,  0.0358, -0.1320]],

        [[-0.0948,  0.1101, -0.0208,  ...,  0.0132,  0.0358, -0.1320]],

        [[-0.0948,  0.1101, -0.0208,  ...,  0.0132,  0.0358, -0.1320]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 12 0
layer name:  MLP
i,j,k 6 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5757,  0.7759,  0.0331,  ...,  0.1732,  0.3225, -0.5093]],

        [[-0.5757,  0.7759,  0.0331,  ...,  0.1732,  0.3225, -0.5093]],

        [[-0.5757,  0.7759,  0.0331,  ...,  0.1732,  0.3225, -0.5093]],

        [[-0.5757,  0.7759,  0.0331,  ...,  0.1732,  0.3225, -0.5093]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1210,  0.1338, -0.0073,  ...,  0.0180,  0.0457, -0.1140]],

        [[-0.1210,  0.1338, -0.0073,  ...,  0.0180,  0.0457, -0.1140]],

        [[-0.1210,  0.1338, -0.0073,  ...,  0.0180,  0.0457, -0.1140]],

        [[-0.1210,  0.1338, -0.0073,  ...,  0.0180,  0.0457, -0.1140]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 15 0
layer name:  MLP
i,j,k 6 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6235,  0.3833,  0.0559,  ...,  0.0886,  0.2571, -0.6011]],

        [[-0.6235,  0.3833,  0.0559,  ...,  0.0886,  0.2571, -0.6011]],

        [[-0.6235,  0.3833,  0.0559,  ...,  0.0886,  0.2571, -0.6011]],

        [[-0.6235,  0.3833,  0.0559,  ...,  0.0886,  0.2571, -0.6011]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1509,  0.0764, -0.0017,  ...,  0.0048,  0.0402, -0.1447]],

        [[-0.1509,  0.0764, -0.0017,  ...,  0.0048,  0.0402, -0.1447]],

        [[-0.1509,  0.0764, -0.0017,  ...,  0.0048,  0.0402, -0.1447]],

        [[-0.1509,  0.0764, -0.0017,  ...,  0.0048,  0.0402, -0.1447]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 18 0
layer name:  MLP
i,j,k 6 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.7441,  0.4050,  0.1068,  ...,  0.0049,  0.2605, -0.6553]],

        [[-0.7441,  0.4050,  0.1068,  ...,  0.0049,  0.2605, -0.6553]],

        [[-0.7441,  0.4050,  0.1068,  ...,  0.0049,  0.2605, -0.6553]],

        [[-0.7441,  0.4050,  0.1068,  ...,  0.0049,  0.2605, -0.6553]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1948,  0.0930,  0.0119,  ..., -0.0118,  0.0474, -0.1750]],

        [[-0.1948,  0.0930,  0.0119,  ..., -0.0118,  0.0474, -0.1750]],

        [[-0.1948,  0.0930,  0.0119,  ..., -0.0118,  0.0474, -0.1750]],

        [[-0.1948,  0.0930,  0.0119,  ..., -0.0118,  0.0474, -0.1750]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 21 0
layer name:  MLP
i,j,k 6 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6313,  0.5181,  0.1760,  ..., -0.2786,  0.1873, -0.6367]],

        [[-0.6313,  0.5181,  0.1760,  ..., -0.2786,  0.1873, -0.6367]],

        [[-0.6313,  0.5181,  0.1760,  ..., -0.2786,  0.1873, -0.6367]],

        [[-0.6313,  0.5181,  0.1760,  ..., -0.2786,  0.1873, -0.6367]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1708,  0.1321,  0.0359,  ..., -0.0815,  0.0360, -0.1726]],

        [[-0.1708,  0.1321,  0.0359,  ..., -0.0815,  0.0360, -0.1726]],

        [[-0.1708,  0.1321,  0.0359,  ..., -0.0815,  0.0360, -0.1726]],

        [[-0.1708,  0.1321,  0.0359,  ..., -0.0815,  0.0360, -0.1726]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 24 0
layer name:  MLP
i,j,k 6 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5742,  0.1521,  0.0895,  ..., -0.4700, -0.0106, -0.7817]],

        [[-0.5742,  0.1521,  0.0895,  ..., -0.4700, -0.0106, -0.7817]],

        [[-0.5742,  0.1521,  0.0895,  ..., -0.4700, -0.0106, -0.7817]],

        [[-0.5742,  0.1521,  0.0895,  ..., -0.4700, -0.0106, -0.7817]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1819,  0.0515,  0.0191,  ..., -0.1477, -0.0106, -0.2463]],

        [[-0.1819,  0.0515,  0.0191,  ..., -0.1477, -0.0106, -0.2463]],

        [[-0.1819,  0.0515,  0.0191,  ..., -0.1477, -0.0106, -0.2463]],

        [[-0.1819,  0.0515,  0.0191,  ..., -0.1477, -0.0106, -0.2463]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 27 0
layer name:  MLP
i,j,k 6 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5366,  0.1897,  0.0592,  ..., -0.7046,  0.0392, -0.7344]],

        [[-0.5366,  0.1897,  0.0592,  ..., -0.7046,  0.0392, -0.7344]],

        [[-0.5366,  0.1897,  0.0592,  ..., -0.7046,  0.0392, -0.7344]],

        [[-0.5366,  0.1897,  0.0592,  ..., -0.7046,  0.0392, -0.7344]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2046,  0.0906,  0.0174,  ..., -0.2715,  0.0117, -0.2791]],

        [[-0.2046,  0.0906,  0.0174,  ..., -0.2715,  0.0117, -0.2791]],

        [[-0.2046,  0.0906,  0.0174,  ..., -0.2715,  0.0117, -0.2791]],

        [[-0.2046,  0.0906,  0.0174,  ..., -0.2715,  0.0117, -0.2791]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 30 0
layer name:  MLP
i,j,k 6 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4514,  0.6265, -0.0521,  ..., -0.3992,  0.1884, -0.6167]],

        [[-0.4514,  0.6265, -0.0521,  ..., -0.3992,  0.1884, -0.6167]],

        [[-0.4514,  0.6265, -0.0521,  ..., -0.3992,  0.1884, -0.6167]],

        [[-0.4514,  0.6265, -0.0521,  ..., -0.3992,  0.1884, -0.6167]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2117,  0.5049, -0.0247,  ..., -0.1980,  0.0928, -0.2883]],

        [[-0.2117,  0.5049, -0.0247,  ..., -0.1980,  0.0928, -0.2883]],

        [[-0.2117,  0.5049, -0.0247,  ..., -0.1980,  0.0928, -0.2883]],

        [[-0.2117,  0.5049, -0.0247,  ..., -0.1980,  0.0928, -0.2883]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 33 0
layer name:  MLP
i,j,k 6 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4595,  1.3428,  0.0422,  ..., -0.4480,  0.2922, -0.6958]],

        [[-0.4595,  1.3428,  0.0422,  ..., -0.4480,  0.2922, -0.6958]],

        [[-0.4595,  1.3428,  0.0422,  ..., -0.4480,  0.2922, -0.6958]],

        [[-0.4595,  1.3428,  0.0422,  ..., -0.4480,  0.2922, -0.6958]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 6 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2563,  0.8789,  0.0435,  ..., -0.2484,  0.1925, -0.3755]],

        [[-0.2563,  0.8789,  0.0435,  ..., -0.2484,  0.1925, -0.3755]],

        [[-0.2563,  0.8789,  0.0435,  ..., -0.2484,  0.1925, -0.3755]],

        [[-0.2563,  0.8789,  0.0435,  ..., -0.2484,  0.1925, -0.3755]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 6 36 0
layer name:  MLP
i,j,k 6 37 0
layer name:  OutputEmbed
i,j,k 7 0 0
layer name:  InputEmbed
i,j,k 7 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0989,  0.4617,  0.1439,  ..., -0.5796,  0.0568, -0.4194]],

        [[-0.0989,  0.4617,  0.1439,  ..., -0.5796,  0.0568, -0.4194]],

        [[-0.0989,  0.4617,  0.1439,  ..., -0.5796,  0.0568, -0.4194]],

        [[-0.0989,  0.4617,  0.1439,  ..., -0.5796,  0.0568, -0.4194]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0192,  0.0887,  0.0357,  ..., -0.1342,  0.0084, -0.1177]],

        [[-0.0192,  0.0887,  0.0357,  ..., -0.1342,  0.0084, -0.1177]],

        [[-0.0192,  0.0887,  0.0357,  ..., -0.1342,  0.0084, -0.1177]],

        [[-0.0192,  0.0887,  0.0357,  ..., -0.1342,  0.0084, -0.1177]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 3 0
layer name:  MLP
i,j,k 7 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0745,  0.5957,  0.0864,  ..., -0.2805, -0.0240, -0.5625]],

        [[-0.0745,  0.5957,  0.0864,  ..., -0.2805, -0.0240, -0.5625]],

        [[-0.0745,  0.5957,  0.0864,  ..., -0.2805, -0.0240, -0.5625]],

        [[-0.0745,  0.5957,  0.0864,  ..., -0.2805, -0.0240, -0.5625]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0145,  0.1149,  0.0257,  ..., -0.0671, -0.0054, -0.1415]],

        [[-0.0145,  0.1149,  0.0257,  ..., -0.0671, -0.0054, -0.1415]],

        [[-0.0145,  0.1149,  0.0257,  ..., -0.0671, -0.0054, -0.1415]],

        [[-0.0145,  0.1149,  0.0257,  ..., -0.0671, -0.0054, -0.1415]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 6 0
layer name:  MLP
i,j,k 7 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1470,  0.5918,  0.0507,  ..., -0.2847, -0.0482, -0.8286]],

        [[ 0.1470,  0.5918,  0.0507,  ..., -0.2847, -0.0482, -0.8286]],

        [[ 0.1470,  0.5918,  0.0507,  ..., -0.2847, -0.0482, -0.8286]],

        [[ 0.1470,  0.5918,  0.0507,  ..., -0.2847, -0.0482, -0.8286]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0345,  0.1272,  0.0163,  ..., -0.0510, -0.0057, -0.1626]],

        [[ 0.0345,  0.1272,  0.0163,  ..., -0.0510, -0.0057, -0.1626]],

        [[ 0.0345,  0.1272,  0.0163,  ..., -0.0510, -0.0057, -0.1626]],

        [[ 0.0345,  0.1272,  0.0163,  ..., -0.0510, -0.0057, -0.1626]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 9 0
layer name:  MLP
i,j,k 7 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2500,  0.3074,  0.0541,  ..., -0.1000, -0.0638, -0.7075]],

        [[ 0.2500,  0.3074,  0.0541,  ..., -0.1000, -0.0638, -0.7075]],

        [[ 0.2500,  0.3074,  0.0541,  ..., -0.1000, -0.0638, -0.7075]],

        [[ 0.2500,  0.3074,  0.0541,  ..., -0.1000, -0.0638, -0.7075]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0659,  0.0861,  0.0183,  ..., -0.0194, -0.0098, -0.1691]],

        [[ 0.0659,  0.0861,  0.0183,  ..., -0.0194, -0.0098, -0.1691]],

        [[ 0.0659,  0.0861,  0.0183,  ..., -0.0194, -0.0098, -0.1691]],

        [[ 0.0659,  0.0861,  0.0183,  ..., -0.0194, -0.0098, -0.1691]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 12 0
layer name:  MLP
i,j,k 7 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2917,  0.2365,  0.1597,  ...,  0.2094,  0.0527, -0.6460]],

        [[ 0.2917,  0.2365,  0.1597,  ...,  0.2094,  0.0527, -0.6460]],

        [[ 0.2917,  0.2365,  0.1597,  ...,  0.2094,  0.0527, -0.6460]],

        [[ 0.2917,  0.2365,  0.1597,  ...,  0.2094,  0.0527, -0.6460]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0694,  0.0605,  0.0419,  ...,  0.0521,  0.0181, -0.1448]],

        [[ 0.0694,  0.0605,  0.0419,  ...,  0.0521,  0.0181, -0.1448]],

        [[ 0.0694,  0.0605,  0.0419,  ...,  0.0521,  0.0181, -0.1448]],

        [[ 0.0694,  0.0605,  0.0419,  ...,  0.0521,  0.0181, -0.1448]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 15 0
layer name:  MLP
i,j,k 7 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2079,  0.1984,  0.4104,  ...,  0.3071, -0.0227, -0.6025]],

        [[ 0.2079,  0.1984,  0.4104,  ...,  0.3071, -0.0227, -0.6025]],

        [[ 0.2079,  0.1984,  0.4104,  ...,  0.3071, -0.0227, -0.6025]],

        [[ 0.2079,  0.1984,  0.4104,  ...,  0.3071, -0.0227, -0.6025]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0592,  0.0627,  0.1056,  ...,  0.0804,  0.0022, -0.1475]],

        [[ 0.0592,  0.0627,  0.1056,  ...,  0.0804,  0.0022, -0.1475]],

        [[ 0.0592,  0.0627,  0.1056,  ...,  0.0804,  0.0022, -0.1475]],

        [[ 0.0592,  0.0627,  0.1056,  ...,  0.0804,  0.0022, -0.1475]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 18 0
layer name:  MLP
i,j,k 7 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1288,  0.4453,  0.4260,  ...,  0.4905,  0.1046, -0.3450]],

        [[-0.1288,  0.4453,  0.4260,  ...,  0.4905,  0.1046, -0.3450]],

        [[-0.1288,  0.4453,  0.4260,  ...,  0.4905,  0.1046, -0.3450]],

        [[-0.1288,  0.4453,  0.4260,  ...,  0.4905,  0.1046, -0.3450]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0294,  0.1444,  0.1240,  ...,  0.1400,  0.0361, -0.0925]],

        [[-0.0294,  0.1444,  0.1240,  ...,  0.1400,  0.0361, -0.0925]],

        [[-0.0294,  0.1444,  0.1240,  ...,  0.1400,  0.0361, -0.0925]],

        [[-0.0294,  0.1444,  0.1240,  ...,  0.1400,  0.0361, -0.0925]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 21 0
layer name:  MLP
i,j,k 7 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2810,  0.5576,  0.3381,  ...,  0.4626,  0.1019, -0.4763]],

        [[-0.2810,  0.5576,  0.3381,  ...,  0.4626,  0.1019, -0.4763]],

        [[-0.2810,  0.5576,  0.3381,  ...,  0.4626,  0.1019, -0.4763]],

        [[-0.2810,  0.5576,  0.3381,  ...,  0.4626,  0.1019, -0.4763]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0709,  0.1859,  0.1109,  ...,  0.1432,  0.0393, -0.1295]],

        [[-0.0709,  0.1859,  0.1109,  ...,  0.1432,  0.0393, -0.1295]],

        [[-0.0709,  0.1859,  0.1109,  ...,  0.1432,  0.0393, -0.1295]],

        [[-0.0709,  0.1859,  0.1109,  ...,  0.1432,  0.0393, -0.1295]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 24 0
layer name:  MLP
i,j,k 7 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3342,  0.4011,  0.2881,  ...,  0.6201,  0.0402, -0.4895]],

        [[-0.3342,  0.4011,  0.2881,  ...,  0.6201,  0.0402, -0.4895]],

        [[-0.3342,  0.4011,  0.2881,  ...,  0.6201,  0.0402, -0.4895]],

        [[-0.3342,  0.4011,  0.2881,  ...,  0.6201,  0.0402, -0.4895]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0991,  0.1721,  0.1095,  ...,  0.2085,  0.0263, -0.1542]],

        [[-0.0991,  0.1721,  0.1095,  ...,  0.2085,  0.0263, -0.1542]],

        [[-0.0991,  0.1721,  0.1095,  ...,  0.2085,  0.0263, -0.1542]],

        [[-0.0991,  0.1721,  0.1095,  ...,  0.2085,  0.0263, -0.1542]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 27 0
layer name:  MLP
i,j,k 7 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4849,  0.8408,  0.2930,  ...,  0.6055, -0.0341, -0.4768]],

        [[-0.4849,  0.8408,  0.2930,  ...,  0.6055, -0.0341, -0.4768]],

        [[-0.4849,  0.8408,  0.2930,  ...,  0.6055, -0.0341, -0.4768]],

        [[-0.4849,  0.8408,  0.2930,  ...,  0.6055, -0.0341, -0.4768]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1680,  0.3567,  0.1293,  ...,  0.2465,  0.0037, -0.1660]],

        [[-0.1680,  0.3567,  0.1293,  ...,  0.2465,  0.0037, -0.1660]],

        [[-0.1680,  0.3567,  0.1293,  ...,  0.2465,  0.0037, -0.1660]],

        [[-0.1680,  0.3567,  0.1293,  ...,  0.2465,  0.0037, -0.1660]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 30 0
layer name:  MLP
i,j,k 7 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6885,  1.1855,  0.2566,  ...,  0.6465, -0.1057, -0.5078]],

        [[-0.6885,  1.1855,  0.2566,  ...,  0.6465, -0.1057, -0.5078]],

        [[-0.6885,  1.1855,  0.2566,  ...,  0.6465, -0.1057, -0.5078]],

        [[-0.6885,  1.1855,  0.2566,  ...,  0.6465, -0.1057, -0.5078]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2917,  0.8892,  0.1455,  ...,  0.3254, -0.0309, -0.2059]],

        [[-0.2917,  0.8892,  0.1455,  ...,  0.3254, -0.0309, -0.2059]],

        [[-0.2917,  0.8892,  0.1455,  ...,  0.3254, -0.0309, -0.2059]],

        [[-0.2917,  0.8892,  0.1455,  ...,  0.3254, -0.0309, -0.2059]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 33 0
layer name:  MLP
i,j,k 7 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.7524,  2.1172,  0.2881,  ...,  0.6963, -0.3628, -0.5410]],

        [[-0.7524,  2.1172,  0.2881,  ...,  0.6963, -0.3628, -0.5410]],

        [[-0.7524,  2.1172,  0.2881,  ...,  0.6963, -0.3628, -0.5410]],

        [[-0.7524,  2.1172,  0.2881,  ...,  0.6963, -0.3628, -0.5410]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 7 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3877,  1.2910,  0.2069,  ...,  0.4272, -0.1982, -0.2532]],

        [[-0.3877,  1.2910,  0.2069,  ...,  0.4272, -0.1982, -0.2532]],

        [[-0.3877,  1.2910,  0.2069,  ...,  0.4272, -0.1982, -0.2532]],

        [[-0.3877,  1.2910,  0.2069,  ...,  0.4272, -0.1982, -0.2532]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 7 36 0
layer name:  MLP
i,j,k 7 37 0
layer name:  OutputEmbed
i,j,k 8 0 0
layer name:  InputEmbed
i,j,k 8 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2900,  0.3999,  0.0714,  ..., -0.2737, -0.2192, -0.1278]],

        [[ 0.2900,  0.3999,  0.0714,  ..., -0.2737, -0.2192, -0.1278]],

        [[ 0.2900,  0.3999,  0.0714,  ..., -0.2737, -0.2192, -0.1278]],

        [[ 0.2900,  0.3999,  0.0714,  ..., -0.2737, -0.2192, -0.1278]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0539,  0.0629,  0.0105,  ..., -0.0632, -0.0558, -0.0305]],

        [[ 0.0539,  0.0629,  0.0105,  ..., -0.0632, -0.0558, -0.0305]],

        [[ 0.0539,  0.0629,  0.0105,  ..., -0.0632, -0.0558, -0.0305]],

        [[ 0.0539,  0.0629,  0.0105,  ..., -0.0632, -0.0558, -0.0305]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 3 0
layer name:  MLP
i,j,k 8 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4973,  0.4038, -0.0850,  ..., -0.1008, -0.3059, -0.1472]],

        [[ 0.4973,  0.4038, -0.0850,  ..., -0.1008, -0.3059, -0.1472]],

        [[ 0.4973,  0.4038, -0.0850,  ..., -0.1008, -0.3059, -0.1472]],

        [[ 0.4973,  0.4038, -0.0850,  ..., -0.1008, -0.3059, -0.1472]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1053,  0.0673, -0.0217,  ..., -0.0273, -0.0626, -0.0291]],

        [[ 0.1053,  0.0673, -0.0217,  ..., -0.0273, -0.0626, -0.0291]],

        [[ 0.1053,  0.0673, -0.0217,  ..., -0.0273, -0.0626, -0.0291]],

        [[ 0.1053,  0.0673, -0.0217,  ..., -0.0273, -0.0626, -0.0291]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 6 0
layer name:  MLP
i,j,k 8 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.7837,  0.4026, -0.1155,  ...,  0.0993, -0.2976, -0.4800]],

        [[ 0.7837,  0.4026, -0.1155,  ...,  0.0993, -0.2976, -0.4800]],

        [[ 0.7837,  0.4026, -0.1155,  ...,  0.0993, -0.2976, -0.4800]],

        [[ 0.7837,  0.4026, -0.1155,  ...,  0.0993, -0.2976, -0.4800]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1368,  0.0699, -0.0217,  ...,  0.0139, -0.0500, -0.0840]],

        [[ 0.1368,  0.0699, -0.0217,  ...,  0.0139, -0.0500, -0.0840]],

        [[ 0.1368,  0.0699, -0.0217,  ...,  0.0139, -0.0500, -0.0840]],

        [[ 0.1368,  0.0699, -0.0217,  ...,  0.0139, -0.0500, -0.0840]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 9 0
layer name:  MLP
i,j,k 8 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.6279, -0.2437, -0.0660,  ...,  0.2625, -0.1144, -0.6680]],

        [[ 0.6279, -0.2437, -0.0660,  ...,  0.2625, -0.1144, -0.6680]],

        [[ 0.6279, -0.2437, -0.0660,  ...,  0.2625, -0.1144, -0.6680]],

        [[ 0.6279, -0.2437, -0.0660,  ...,  0.2625, -0.1144, -0.6680]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1298, -0.0532, -0.0151,  ...,  0.0520, -0.0250, -0.1438]],

        [[ 0.1298, -0.0532, -0.0151,  ...,  0.0520, -0.0250, -0.1438]],

        [[ 0.1298, -0.0532, -0.0151,  ...,  0.0520, -0.0250, -0.1438]],

        [[ 0.1298, -0.0532, -0.0151,  ...,  0.0520, -0.0250, -0.1438]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 12 0
layer name:  MLP
i,j,k 8 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.8237, -0.2761, -0.2115,  ...,  0.6030, -0.0686, -0.7378]],

        [[ 0.8237, -0.2761, -0.2115,  ...,  0.6030, -0.0686, -0.7378]],

        [[ 0.8237, -0.2761, -0.2115,  ...,  0.6030, -0.0686, -0.7378]],

        [[ 0.8237, -0.2761, -0.2115,  ...,  0.6030, -0.0686, -0.7378]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1569, -0.0547, -0.0413,  ...,  0.1150, -0.0142, -0.1531]],

        [[ 0.1569, -0.0547, -0.0413,  ...,  0.1150, -0.0142, -0.1531]],

        [[ 0.1569, -0.0547, -0.0413,  ...,  0.1150, -0.0142, -0.1531]],

        [[ 0.1569, -0.0547, -0.0413,  ...,  0.1150, -0.0142, -0.1531]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 15 0
layer name:  MLP
i,j,k 8 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.6318, -0.3137, -0.0523,  ...,  0.7598, -0.1718, -0.5630]],

        [[ 0.6318, -0.3137, -0.0523,  ...,  0.7598, -0.1718, -0.5630]],

        [[ 0.6318, -0.3137, -0.0523,  ...,  0.7598, -0.1718, -0.5630]],

        [[ 0.6318, -0.3137, -0.0523,  ...,  0.7598, -0.1718, -0.5630]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1472, -0.0741, -0.0121,  ...,  0.1664, -0.0375, -0.1326]],

        [[ 0.1472, -0.0741, -0.0121,  ...,  0.1664, -0.0375, -0.1326]],

        [[ 0.1472, -0.0741, -0.0121,  ...,  0.1664, -0.0375, -0.1326]],

        [[ 0.1472, -0.0741, -0.0121,  ...,  0.1664, -0.0375, -0.1326]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 18 0
layer name:  MLP
i,j,k 8 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4253, -0.2678, -0.1104,  ...,  0.9126, -0.0435, -0.3076]],

        [[ 0.4253, -0.2678, -0.1104,  ...,  0.9126, -0.0435, -0.3076]],

        [[ 0.4253, -0.2678, -0.1104,  ...,  0.9126, -0.0435, -0.3076]],

        [[ 0.4253, -0.2678, -0.1104,  ...,  0.9126, -0.0435, -0.3076]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1098, -0.0690, -0.0276,  ...,  0.2192, -0.0108, -0.0796]],

        [[ 0.1098, -0.0690, -0.0276,  ...,  0.2192, -0.0108, -0.0796]],

        [[ 0.1098, -0.0690, -0.0276,  ...,  0.2192, -0.0108, -0.0796]],

        [[ 0.1098, -0.0690, -0.0276,  ...,  0.2192, -0.0108, -0.0796]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 21 0
layer name:  MLP
i,j,k 8 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3953, -0.4985, -0.0895,  ...,  0.8057, -0.1083, -0.4424]],

        [[ 0.3953, -0.4985, -0.0895,  ...,  0.8057, -0.1083, -0.4424]],

        [[ 0.3953, -0.4985, -0.0895,  ...,  0.8057, -0.1083, -0.4424]],

        [[ 0.3953, -0.4985, -0.0895,  ...,  0.8057, -0.1083, -0.4424]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1133, -0.1351, -0.0225,  ...,  0.2209, -0.0273, -0.1215]],

        [[ 0.1133, -0.1351, -0.0225,  ...,  0.2209, -0.0273, -0.1215]],

        [[ 0.1133, -0.1351, -0.0225,  ...,  0.2209, -0.0273, -0.1215]],

        [[ 0.1133, -0.1351, -0.0225,  ...,  0.2209, -0.0273, -0.1215]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 24 0
layer name:  MLP
i,j,k 8 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1606, -0.3130,  0.1987,  ...,  0.7183, -0.3840, -0.1635]],

        [[ 0.1606, -0.3130,  0.1987,  ...,  0.7183, -0.3840, -0.1635]],

        [[ 0.1606, -0.3130,  0.1987,  ...,  0.7183, -0.3840, -0.1635]],

        [[ 0.1606, -0.3130,  0.1987,  ...,  0.7183, -0.3840, -0.1635]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0556, -0.0961,  0.0643,  ...,  0.2115, -0.1121, -0.0490]],

        [[ 0.0556, -0.0961,  0.0643,  ...,  0.2115, -0.1121, -0.0490]],

        [[ 0.0556, -0.0961,  0.0643,  ...,  0.2115, -0.1121, -0.0490]],

        [[ 0.0556, -0.0961,  0.0643,  ...,  0.2115, -0.1121, -0.0490]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 27 0
layer name:  MLP
i,j,k 8 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1346,  0.5835,  0.4143,  ...,  0.8838, -0.6401, -0.3923]],

        [[-0.1346,  0.5835,  0.4143,  ...,  0.8838, -0.6401, -0.3923]],

        [[-0.1346,  0.5835,  0.4143,  ...,  0.8838, -0.6401, -0.3923]],

        [[-0.1346,  0.5835,  0.4143,  ...,  0.8838, -0.6401, -0.3923]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0379,  0.2341,  0.1567,  ...,  0.3208, -0.2217, -0.1323]],

        [[-0.0379,  0.2341,  0.1567,  ...,  0.3208, -0.2217, -0.1323]],

        [[-0.0379,  0.2341,  0.1567,  ...,  0.3208, -0.2217, -0.1323]],

        [[-0.0379,  0.2341,  0.1567,  ...,  0.3208, -0.2217, -0.1323]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 30 0
layer name:  MLP
i,j,k 8 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2852,  1.1211,  0.6060,  ...,  0.8833, -0.7212, -0.1738]],

        [[-0.2852,  1.1211,  0.6060,  ...,  0.8833, -0.7212, -0.1738]],

        [[-0.2852,  1.1211,  0.6060,  ...,  0.8833, -0.7212, -0.1738]],

        [[-0.2852,  1.1211,  0.6060,  ...,  0.8833, -0.7212, -0.1738]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1085,  0.8032,  0.2932,  ...,  0.4121, -0.3203, -0.0611]],

        [[-0.1085,  0.8032,  0.2932,  ...,  0.4121, -0.3203, -0.0611]],

        [[-0.1085,  0.8032,  0.2932,  ...,  0.4121, -0.3203, -0.0611]],

        [[-0.1085,  0.8032,  0.2932,  ...,  0.4121, -0.3203, -0.0611]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 33 0
layer name:  MLP
i,j,k 8 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2898,  2.6504,  0.7300,  ...,  0.9448, -0.6611, -0.1631]],

        [[-0.2898,  2.6504,  0.7300,  ...,  0.9448, -0.6611, -0.1631]],

        [[-0.2898,  2.6504,  0.7300,  ...,  0.9448, -0.6611, -0.1631]],

        [[-0.2898,  2.6504,  0.7300,  ...,  0.9448, -0.6611, -0.1631]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 8 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1321,  1.5332,  0.4456,  ...,  0.5400, -0.3767, -0.0582]],

        [[-0.1321,  1.5332,  0.4456,  ...,  0.5400, -0.3767, -0.0582]],

        [[-0.1321,  1.5332,  0.4456,  ...,  0.5400, -0.3767, -0.0582]],

        [[-0.1321,  1.5332,  0.4456,  ...,  0.5400, -0.3767, -0.0582]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 8 36 0
layer name:  MLP
i,j,k 8 37 0
layer name:  OutputEmbed
i,j,k 9 0 0
layer name:  InputEmbed
i,j,k 9 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2822,  0.4617,  0.2751,  ..., -0.1037, -0.1562,  0.1727]],

        [[-0.2822,  0.4617,  0.2751,  ..., -0.1037, -0.1562,  0.1727]],

        [[-0.2822,  0.4617,  0.2751,  ..., -0.1037, -0.1562,  0.1727]],

        [[-0.2822,  0.4617,  0.2751,  ..., -0.1037, -0.1562,  0.1727]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0634,  0.0714,  0.0519,  ..., -0.0289, -0.0454,  0.0527]],

        [[-0.0634,  0.0714,  0.0519,  ..., -0.0289, -0.0454,  0.0527]],

        [[-0.0634,  0.0714,  0.0519,  ..., -0.0289, -0.0454,  0.0527]],

        [[-0.0634,  0.0714,  0.0519,  ..., -0.0289, -0.0454,  0.0527]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 3 0
layer name:  MLP
i,j,k 9 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0689,  0.3982,  0.2622,  ...,  0.0582, -0.2720,  0.1220]],

        [[-0.0689,  0.3982,  0.2622,  ...,  0.0582, -0.2720,  0.1220]],

        [[-0.0689,  0.3982,  0.2622,  ...,  0.0582, -0.2720,  0.1220]],

        [[-0.0689,  0.3982,  0.2622,  ...,  0.0582, -0.2720,  0.1220]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0211,  0.0625,  0.0536,  ...,  0.0056, -0.0586,  0.0370]],

        [[-0.0211,  0.0625,  0.0536,  ...,  0.0056, -0.0586,  0.0370]],

        [[-0.0211,  0.0625,  0.0536,  ...,  0.0056, -0.0586,  0.0370]],

        [[-0.0211,  0.0625,  0.0536,  ...,  0.0056, -0.0586,  0.0370]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 6 0
layer name:  MLP
i,j,k 9 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1731,  0.4031,  0.3315,  ...,  0.3860, -0.4771,  0.0248]],

        [[ 0.1731,  0.4031,  0.3315,  ...,  0.3860, -0.4771,  0.0248]],

        [[ 0.1731,  0.4031,  0.3315,  ...,  0.3860, -0.4771,  0.0248]],

        [[ 0.1731,  0.4031,  0.3315,  ...,  0.3860, -0.4771,  0.0248]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0255,  0.0681,  0.0553,  ...,  0.0600, -0.0815,  0.0045]],

        [[ 0.0255,  0.0681,  0.0553,  ...,  0.0600, -0.0815,  0.0045]],

        [[ 0.0255,  0.0681,  0.0553,  ...,  0.0600, -0.0815,  0.0045]],

        [[ 0.0255,  0.0681,  0.0553,  ...,  0.0600, -0.0815,  0.0045]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 9 0
layer name:  MLP
i,j,k 9 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2769,  0.4204,  0.3198,  ...,  0.4678, -0.1792, -0.1273]],

        [[ 0.2769,  0.4204,  0.3198,  ...,  0.4678, -0.1792, -0.1273]],

        [[ 0.2769,  0.4204,  0.3198,  ...,  0.4678, -0.1792, -0.1273]],

        [[ 0.2769,  0.4204,  0.3198,  ...,  0.4678, -0.1792, -0.1273]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0501,  0.0847,  0.0564,  ...,  0.0872, -0.0385, -0.0282]],

        [[ 0.0501,  0.0847,  0.0564,  ...,  0.0872, -0.0385, -0.0282]],

        [[ 0.0501,  0.0847,  0.0564,  ...,  0.0872, -0.0385, -0.0282]],

        [[ 0.0501,  0.0847,  0.0564,  ...,  0.0872, -0.0385, -0.0282]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 12 0
layer name:  MLP
i,j,k 9 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.5020,  0.4814,  0.2145,  ...,  0.5659, -0.0469, -0.2274]],

        [[ 0.5020,  0.4814,  0.2145,  ...,  0.5659, -0.0469, -0.2274]],

        [[ 0.5020,  0.4814,  0.2145,  ...,  0.5659, -0.0469, -0.2274]],

        [[ 0.5020,  0.4814,  0.2145,  ...,  0.5659, -0.0469, -0.2274]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0837,  0.0837,  0.0338,  ...,  0.0956, -0.0129, -0.0468]],

        [[ 0.0837,  0.0837,  0.0338,  ...,  0.0956, -0.0129, -0.0468]],

        [[ 0.0837,  0.0837,  0.0338,  ...,  0.0956, -0.0129, -0.0468]],

        [[ 0.0837,  0.0837,  0.0338,  ...,  0.0956, -0.0129, -0.0468]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 15 0
layer name:  MLP
i,j,k 9 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4714,  0.4160, -0.0473,  ...,  0.6899, -0.1669, -0.5298]],

        [[ 0.4714,  0.4160, -0.0473,  ...,  0.6899, -0.1669, -0.5298]],

        [[ 0.4714,  0.4160, -0.0473,  ...,  0.6899, -0.1669, -0.5298]],

        [[ 0.4714,  0.4160, -0.0473,  ...,  0.6899, -0.1669, -0.5298]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0940,  0.0887, -0.0133,  ...,  0.1310, -0.0359, -0.1146]],

        [[ 0.0940,  0.0887, -0.0133,  ...,  0.1310, -0.0359, -0.1146]],

        [[ 0.0940,  0.0887, -0.0133,  ...,  0.1310, -0.0359, -0.1146]],

        [[ 0.0940,  0.0887, -0.0133,  ...,  0.1310, -0.0359, -0.1146]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 18 0
layer name:  MLP
i,j,k 9 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2178,  0.2798, -0.1548,  ...,  0.7207, -0.2166, -0.3098]],

        [[ 0.2178,  0.2798, -0.1548,  ...,  0.7207, -0.2166, -0.3098]],

        [[ 0.2178,  0.2798, -0.1548,  ...,  0.7207, -0.2166, -0.3098]],

        [[ 0.2178,  0.2798, -0.1548,  ...,  0.7207, -0.2166, -0.3098]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0483,  0.0675, -0.0384,  ...,  0.1552, -0.0513, -0.0766]],

        [[ 0.0483,  0.0675, -0.0384,  ...,  0.1552, -0.0513, -0.0766]],

        [[ 0.0483,  0.0675, -0.0384,  ...,  0.1552, -0.0513, -0.0766]],

        [[ 0.0483,  0.0675, -0.0384,  ...,  0.1552, -0.0513, -0.0766]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 21 0
layer name:  MLP
i,j,k 9 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4370, -0.2812, -0.2490,  ...,  0.6846, -0.2922, -0.5225]],

        [[ 0.4370, -0.2812, -0.2490,  ...,  0.6846, -0.2922, -0.5225]],

        [[ 0.4370, -0.2812, -0.2490,  ...,  0.6846, -0.2922, -0.5225]],

        [[ 0.4370, -0.2812, -0.2490,  ...,  0.6846, -0.2922, -0.5225]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1158, -0.0723, -0.0665,  ...,  0.1753, -0.0754, -0.1394]],

        [[ 0.1158, -0.0723, -0.0665,  ...,  0.1753, -0.0754, -0.1394]],

        [[ 0.1158, -0.0723, -0.0665,  ...,  0.1753, -0.0754, -0.1394]],

        [[ 0.1158, -0.0723, -0.0665,  ...,  0.1753, -0.0754, -0.1394]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 24 0
layer name:  MLP
i,j,k 9 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2463, -0.4485, -0.0991,  ...,  0.5610, -0.0219, -0.2362]],

        [[ 0.2463, -0.4485, -0.0991,  ...,  0.5610, -0.0219, -0.2362]],

        [[ 0.2463, -0.4485, -0.0991,  ...,  0.5610, -0.0219, -0.2362]],

        [[ 0.2463, -0.4485, -0.0991,  ...,  0.5610, -0.0219, -0.2362]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0797, -0.1483, -0.0316,  ...,  0.1636, -0.0068, -0.0762]],

        [[ 0.0797, -0.1483, -0.0316,  ...,  0.1636, -0.0068, -0.0762]],

        [[ 0.0797, -0.1483, -0.0316,  ...,  0.1636, -0.0068, -0.0762]],

        [[ 0.0797, -0.1483, -0.0316,  ...,  0.1636, -0.0068, -0.0762]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 27 0
layer name:  MLP
i,j,k 9 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1406,  0.1257,  0.2208,  ...,  0.3787, -0.3037, -0.1831]],

        [[ 0.1406,  0.1257,  0.2208,  ...,  0.3787, -0.3037, -0.1831]],

        [[ 0.1406,  0.1257,  0.2208,  ...,  0.3787, -0.3037, -0.1831]],

        [[ 0.1406,  0.1257,  0.2208,  ...,  0.3787, -0.3037, -0.1831]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0587,  0.0721,  0.0844,  ...,  0.1405, -0.1089, -0.0649]],

        [[ 0.0587,  0.0721,  0.0844,  ...,  0.1405, -0.1089, -0.0649]],

        [[ 0.0587,  0.0721,  0.0844,  ...,  0.1405, -0.1089, -0.0649]],

        [[ 0.0587,  0.0721,  0.0844,  ...,  0.1405, -0.1089, -0.0649]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 30 0
layer name:  MLP
i,j,k 9 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.5454,  0.7114,  0.0776,  ...,  0.2654, -0.4934, -0.2205]],

        [[ 0.5454,  0.7114,  0.0776,  ...,  0.2654, -0.4934, -0.2205]],

        [[ 0.5454,  0.7114,  0.0776,  ...,  0.2654, -0.4934, -0.2205]],

        [[ 0.5454,  0.7114,  0.0776,  ...,  0.2654, -0.4934, -0.2205]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2708,  0.5562,  0.0458,  ...,  0.1331, -0.2339, -0.0919]],

        [[ 0.2708,  0.5562,  0.0458,  ...,  0.1331, -0.2339, -0.0919]],

        [[ 0.2708,  0.5562,  0.0458,  ...,  0.1331, -0.2339, -0.0919]],

        [[ 0.2708,  0.5562,  0.0458,  ...,  0.1331, -0.2339, -0.0919]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 33 0
layer name:  MLP
i,j,k 9 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.6729,  1.6064,  0.1886,  ...,  0.3440, -0.2937, -0.4573]],

        [[ 0.6729,  1.6064,  0.1886,  ...,  0.3440, -0.2937, -0.4573]],

        [[ 0.6729,  1.6064,  0.1886,  ...,  0.3440, -0.2937, -0.4573]],

        [[ 0.6729,  1.6064,  0.1886,  ...,  0.3440, -0.2937, -0.4573]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 9 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.3804,  0.9443,  0.1262,  ...,  0.2043, -0.1648, -0.2151]],

        [[ 0.3804,  0.9443,  0.1262,  ...,  0.2043, -0.1648, -0.2151]],

        [[ 0.3804,  0.9443,  0.1262,  ...,  0.2043, -0.1648, -0.2151]],

        [[ 0.3804,  0.9443,  0.1262,  ...,  0.2043, -0.1648, -0.2151]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 9 36 0
layer name:  MLP
i,j,k 9 37 0
layer name:  OutputEmbed
i,j,k 10 0 0
layer name:  InputEmbed
i,j,k 10 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1841,  0.0754, -0.3003,  ...,  0.3149,  0.3438, -0.5576]],

        [[-0.1841,  0.0754, -0.3003,  ...,  0.3149,  0.3438, -0.5576]],

        [[-0.1841,  0.0754, -0.3003,  ...,  0.3149,  0.3438, -0.5576]],

        [[-0.1841,  0.0754, -0.3003,  ...,  0.3149,  0.3438, -0.5576]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0243,  0.0128, -0.0566,  ...,  0.0861,  0.0809, -0.1418]],

        [[-0.0243,  0.0128, -0.0566,  ...,  0.0861,  0.0809, -0.1418]],

        [[-0.0243,  0.0128, -0.0566,  ...,  0.0861,  0.0809, -0.1418]],

        [[-0.0243,  0.0128, -0.0566,  ...,  0.0861,  0.0809, -0.1418]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 3 0
layer name:  MLP
i,j,k 10 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1222,  0.0386, -0.3750,  ...,  0.4531,  0.3237, -0.5146]],

        [[-0.1222,  0.0386, -0.3750,  ...,  0.4531,  0.3237, -0.5146]],

        [[-0.1222,  0.0386, -0.3750,  ...,  0.4531,  0.3237, -0.5146]],

        [[-0.1222,  0.0386, -0.3750,  ...,  0.4531,  0.3237, -0.5146]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0133,  0.0250, -0.0736,  ...,  0.1204,  0.0726, -0.1115]],

        [[-0.0133,  0.0250, -0.0736,  ...,  0.1204,  0.0726, -0.1115]],

        [[-0.0133,  0.0250, -0.0736,  ...,  0.1204,  0.0726, -0.1115]],

        [[-0.0133,  0.0250, -0.0736,  ...,  0.1204,  0.0726, -0.1115]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 6 0
layer name:  MLP
i,j,k 10 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0054,  0.1406, -0.5020,  ...,  0.7759,  0.4246, -0.5840]],

        [[-0.0054,  0.1406, -0.5020,  ...,  0.7759,  0.4246, -0.5840]],

        [[-0.0054,  0.1406, -0.5020,  ...,  0.7759,  0.4246, -0.5840]],

        [[-0.0054,  0.1406, -0.5020,  ...,  0.7759,  0.4246, -0.5840]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0143,  0.0425, -0.0753,  ...,  0.1506,  0.0825, -0.0884]],

        [[ 0.0143,  0.0425, -0.0753,  ...,  0.1506,  0.0825, -0.0884]],

        [[ 0.0143,  0.0425, -0.0753,  ...,  0.1506,  0.0825, -0.0884]],

        [[ 0.0143,  0.0425, -0.0753,  ...,  0.1506,  0.0825, -0.0884]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 9 0
layer name:  MLP
i,j,k 10 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0426,  0.2976, -0.4248,  ...,  0.6963,  0.4141, -0.5356]],

        [[-0.0426,  0.2976, -0.4248,  ...,  0.6963,  0.4141, -0.5356]],

        [[-0.0426,  0.2976, -0.4248,  ...,  0.6963,  0.4141, -0.5356]],

        [[-0.0426,  0.2976, -0.4248,  ...,  0.6963,  0.4141, -0.5356]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0068,  0.0794, -0.0643,  ...,  0.1520,  0.0914, -0.0908]],

        [[ 0.0068,  0.0794, -0.0643,  ...,  0.1520,  0.0914, -0.0908]],

        [[ 0.0068,  0.0794, -0.0643,  ...,  0.1520,  0.0914, -0.0908]],

        [[ 0.0068,  0.0794, -0.0643,  ...,  0.1520,  0.0914, -0.0908]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 12 0
layer name:  MLP
i,j,k 10 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0659,  0.2429, -0.5776,  ...,  0.7231,  0.5986, -0.6772]],

        [[-0.0659,  0.2429, -0.5776,  ...,  0.7231,  0.5986, -0.6772]],

        [[-0.0659,  0.2429, -0.5776,  ...,  0.7231,  0.5986, -0.6772]],

        [[-0.0659,  0.2429, -0.5776,  ...,  0.7231,  0.5986, -0.6772]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0037,  0.0626, -0.0880,  ...,  0.1478,  0.1252, -0.1150]],

        [[ 0.0037,  0.0626, -0.0880,  ...,  0.1478,  0.1252, -0.1150]],

        [[ 0.0037,  0.0626, -0.0880,  ...,  0.1478,  0.1252, -0.1150]],

        [[ 0.0037,  0.0626, -0.0880,  ...,  0.1478,  0.1252, -0.1150]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 15 0
layer name:  MLP
i,j,k 10 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0100,  0.2917, -0.5757,  ...,  0.6699,  0.3926, -0.6772]],

        [[-0.0100,  0.2917, -0.5757,  ...,  0.6699,  0.3926, -0.6772]],

        [[-0.0100,  0.2917, -0.5757,  ...,  0.6699,  0.3926, -0.6772]],

        [[-0.0100,  0.2917, -0.5757,  ...,  0.6699,  0.3926, -0.6772]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0142,  0.0845, -0.0984,  ...,  0.1511,  0.0965, -0.1276]],

        [[ 0.0142,  0.0845, -0.0984,  ...,  0.1511,  0.0965, -0.1276]],

        [[ 0.0142,  0.0845, -0.0984,  ...,  0.1511,  0.0965, -0.1276]],

        [[ 0.0142,  0.0845, -0.0984,  ...,  0.1511,  0.0965, -0.1276]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 18 0
layer name:  MLP
i,j,k 10 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2542,  0.2352, -0.5576,  ...,  0.7769,  0.2405, -0.4263]],

        [[-0.2542,  0.2352, -0.5576,  ...,  0.7769,  0.2405, -0.4263]],

        [[-0.2542,  0.2352, -0.5576,  ...,  0.7769,  0.2405, -0.4263]],

        [[-0.2542,  0.2352, -0.5576,  ...,  0.7769,  0.2405, -0.4263]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0438,  0.0793, -0.1107,  ...,  0.1934,  0.0723, -0.0864]],

        [[-0.0438,  0.0793, -0.1107,  ...,  0.1934,  0.0723, -0.0864]],

        [[-0.0438,  0.0793, -0.1107,  ...,  0.1934,  0.0723, -0.0864]],

        [[-0.0438,  0.0793, -0.1107,  ...,  0.1934,  0.0723, -0.0864]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 21 0
layer name:  MLP
i,j,k 10 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0206, -0.2688, -0.5083,  ...,  0.5469, -0.0098, -0.5439]],

        [[ 0.0206, -0.2688, -0.5083,  ...,  0.5469, -0.0098, -0.5439]],

        [[ 0.0206, -0.2688, -0.5083,  ...,  0.5469, -0.0098, -0.5439]],

        [[ 0.0206, -0.2688, -0.5083,  ...,  0.5469, -0.0098, -0.5439]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0263, -0.0493, -0.1165,  ...,  0.1627,  0.0168, -0.1268]],

        [[ 0.0263, -0.0493, -0.1165,  ...,  0.1627,  0.0168, -0.1268]],

        [[ 0.0263, -0.0493, -0.1165,  ...,  0.1627,  0.0168, -0.1268]],

        [[ 0.0263, -0.0493, -0.1165,  ...,  0.1627,  0.0168, -0.1268]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 24 0
layer name:  MLP
i,j,k 10 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0146, -0.5557, -0.3369,  ...,  0.5542, -0.1394, -0.2598]],

        [[-0.0146, -0.5557, -0.3369,  ...,  0.5542, -0.1394, -0.2598]],

        [[-0.0146, -0.5557, -0.3369,  ...,  0.5542, -0.1394, -0.2598]],

        [[-0.0146, -0.5557, -0.3369,  ...,  0.5542, -0.1394, -0.2598]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0179, -0.1674, -0.0865,  ...,  0.1848, -0.0224, -0.0638]],

        [[ 0.0179, -0.1674, -0.0865,  ...,  0.1848, -0.0224, -0.0638]],

        [[ 0.0179, -0.1674, -0.0865,  ...,  0.1848, -0.0224, -0.0638]],

        [[ 0.0179, -0.1674, -0.0865,  ...,  0.1848, -0.0224, -0.0638]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 27 0
layer name:  MLP
i,j,k 10 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0912, -0.1487, -0.1323,  ...,  0.5405, -0.4097, -0.0821]],

        [[-0.0912, -0.1487, -0.1323,  ...,  0.5405, -0.4097, -0.0821]],

        [[-0.0912, -0.1487, -0.1323,  ...,  0.5405, -0.4097, -0.0821]],

        [[-0.0912, -0.1487, -0.1323,  ...,  0.5405, -0.4097, -0.0821]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0086, -0.0078, -0.0305,  ...,  0.2313, -0.1357, -0.0091]],

        [[-0.0086, -0.0078, -0.0305,  ...,  0.2313, -0.1357, -0.0091]],

        [[-0.0086, -0.0078, -0.0305,  ...,  0.2313, -0.1357, -0.0091]],

        [[-0.0086, -0.0078, -0.0305,  ...,  0.2313, -0.1357, -0.0091]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 30 0
layer name:  MLP
i,j,k 10 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1315,  0.2612, -0.2367,  ...,  0.5044, -0.6597,  0.0858]],

        [[ 0.1315,  0.2612, -0.2367,  ...,  0.5044, -0.6597,  0.0858]],

        [[ 0.1315,  0.2612, -0.2367,  ...,  0.5044, -0.6597,  0.0858]],

        [[ 0.1315,  0.2612, -0.2367,  ...,  0.5044, -0.6597,  0.0858]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1026,  0.2798, -0.0933,  ...,  0.2866, -0.3145,  0.0734]],

        [[ 0.1026,  0.2798, -0.0933,  ...,  0.2866, -0.3145,  0.0734]],

        [[ 0.1026,  0.2798, -0.0933,  ...,  0.2866, -0.3145,  0.0734]],

        [[ 0.1026,  0.2798, -0.0933,  ...,  0.2866, -0.3145,  0.0734]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 33 0
layer name:  MLP
i,j,k 10 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1487,  0.8604, -0.1187,  ...,  0.4302, -0.6221,  0.0617]],

        [[ 0.1487,  0.8604, -0.1187,  ...,  0.4302, -0.6221,  0.0617]],

        [[ 0.1487,  0.8604, -0.1187,  ...,  0.4302, -0.6221,  0.0617]],

        [[ 0.1487,  0.8604, -0.1187,  ...,  0.4302, -0.6221,  0.0617]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 10 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1298,  0.6084, -0.0296,  ...,  0.2954, -0.3645,  0.0779]],

        [[ 0.1298,  0.6084, -0.0296,  ...,  0.2954, -0.3645,  0.0779]],

        [[ 0.1298,  0.6084, -0.0296,  ...,  0.2954, -0.3645,  0.0779]],

        [[ 0.1298,  0.6084, -0.0296,  ...,  0.2954, -0.3645,  0.0779]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 10 36 0
layer name:  MLP
i,j,k 10 37 0
layer name:  OutputEmbed
i,j,k 11 0 0
layer name:  InputEmbed
i,j,k 11 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1998,  0.4028, -0.1641,  ..., -0.0582, -0.2039, -0.3347]],

        [[-0.1998,  0.4028, -0.1641,  ..., -0.0582, -0.2039, -0.3347]],

        [[-0.1998,  0.4028, -0.1641,  ..., -0.0582, -0.2039, -0.3347]],

        [[-0.1998,  0.4028, -0.1641,  ..., -0.0582, -0.2039, -0.3347]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0482,  0.0616, -0.0460,  ..., -0.0192, -0.0566, -0.0953]],

        [[-0.0482,  0.0616, -0.0460,  ..., -0.0192, -0.0566, -0.0953]],

        [[-0.0482,  0.0616, -0.0460,  ..., -0.0192, -0.0566, -0.0953]],

        [[-0.0482,  0.0616, -0.0460,  ..., -0.0192, -0.0566, -0.0953]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 3 0
layer name:  MLP
i,j,k 11 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3567,  0.4509, -0.2134,  ...,  0.1266, -0.1320, -0.3062]],

        [[-0.3567,  0.4509, -0.2134,  ...,  0.1266, -0.1320, -0.3062]],

        [[-0.3567,  0.4509, -0.2134,  ...,  0.1266, -0.1320, -0.3062]],

        [[-0.3567,  0.4509, -0.2134,  ...,  0.1266, -0.1320, -0.3062]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0837,  0.0731, -0.0533,  ...,  0.0219, -0.0332, -0.0724]],

        [[-0.0837,  0.0731, -0.0533,  ...,  0.0219, -0.0332, -0.0724]],

        [[-0.0837,  0.0731, -0.0533,  ...,  0.0219, -0.0332, -0.0724]],

        [[-0.0837,  0.0731, -0.0533,  ...,  0.0219, -0.0332, -0.0724]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 6 0
layer name:  MLP
i,j,k 11 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3474,  0.5410, -0.1487,  ...,  0.2869, -0.1260, -0.2024]],

        [[-0.3474,  0.5410, -0.1487,  ...,  0.2869, -0.1260, -0.2024]],

        [[-0.3474,  0.5410, -0.1487,  ...,  0.2869, -0.1260, -0.2024]],

        [[-0.3474,  0.5410, -0.1487,  ...,  0.2869, -0.1260, -0.2024]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0682,  0.0959, -0.0299,  ...,  0.0453, -0.0256, -0.0363]],

        [[-0.0682,  0.0959, -0.0299,  ...,  0.0453, -0.0256, -0.0363]],

        [[-0.0682,  0.0959, -0.0299,  ...,  0.0453, -0.0256, -0.0363]],

        [[-0.0682,  0.0959, -0.0299,  ...,  0.0453, -0.0256, -0.0363]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 9 0
layer name:  MLP
i,j,k 11 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4011,  0.5239, -0.0095,  ...,  0.1431, -0.1326, -0.2964]],

        [[-0.4011,  0.5239, -0.0095,  ...,  0.1431, -0.1326, -0.2964]],

        [[-0.4011,  0.5239, -0.0095,  ...,  0.1431, -0.1326, -0.2964]],

        [[-0.4011,  0.5239, -0.0095,  ...,  0.1431, -0.1326, -0.2964]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0840,  0.1060, -0.0054,  ...,  0.0236, -0.0287, -0.0615]],

        [[-0.0840,  0.1060, -0.0054,  ...,  0.0236, -0.0287, -0.0615]],

        [[-0.0840,  0.1060, -0.0054,  ...,  0.0236, -0.0287, -0.0615]],

        [[-0.0840,  0.1060, -0.0054,  ...,  0.0236, -0.0287, -0.0615]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 12 0
layer name:  MLP
i,j,k 11 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.7236,  0.5776,  0.0602,  ...,  0.4099,  0.1483, -0.2115]],

        [[-0.7236,  0.5776,  0.0602,  ...,  0.4099,  0.1483, -0.2115]],

        [[-0.7236,  0.5776,  0.0602,  ...,  0.4099,  0.1483, -0.2115]],

        [[-0.7236,  0.5776,  0.0602,  ...,  0.4099,  0.1483, -0.2115]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1367,  0.1054,  0.0078,  ...,  0.0712,  0.0235, -0.0439]],

        [[-0.1367,  0.1054,  0.0078,  ...,  0.0712,  0.0235, -0.0439]],

        [[-0.1367,  0.1054,  0.0078,  ...,  0.0712,  0.0235, -0.0439]],

        [[-0.1367,  0.1054,  0.0078,  ...,  0.0712,  0.0235, -0.0439]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 15 0
layer name:  MLP
i,j,k 11 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.9829,  0.4370, -0.2529,  ...,  0.5518,  0.1015, -0.0325]],

        [[-0.9829,  0.4370, -0.2529,  ...,  0.5518,  0.1015, -0.0325]],

        [[-0.9829,  0.4370, -0.2529,  ...,  0.5518,  0.1015, -0.0325]],

        [[-0.9829,  0.4370, -0.2529,  ...,  0.5518,  0.1015, -0.0325]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2190,  0.0983, -0.0550,  ...,  0.1093,  0.0182, -0.0097]],

        [[-0.2190,  0.0983, -0.0550,  ...,  0.1093,  0.0182, -0.0097]],

        [[-0.2190,  0.0983, -0.0550,  ...,  0.1093,  0.0182, -0.0097]],

        [[-0.2190,  0.0983, -0.0550,  ...,  0.1093,  0.0182, -0.0097]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 18 0
layer name:  MLP
i,j,k 11 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-1.1455e+00,  2.6978e-01, -1.0687e-01,  ...,  5.4102e-01,
          -8.3637e-04,  1.0352e-01]],

        [[-1.1455e+00,  2.6978e-01, -1.0687e-01,  ...,  5.4102e-01,
          -8.3637e-04,  1.0352e-01]],

        [[-1.1455e+00,  2.6978e-01, -1.0687e-01,  ...,  5.4102e-01,
          -8.3637e-04,  1.0352e-01]],

        [[-1.1455e+00,  2.6978e-01, -1.0687e-01,  ...,  5.4102e-01,
          -8.3637e-04,  1.0352e-01]]], device='cuda:0', dtype=torch.float16)
i,j,k 11 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3005,  0.0724, -0.0296,  ...,  0.1279, -0.0031,  0.0266]],

        [[-0.3005,  0.0724, -0.0296,  ...,  0.1279, -0.0031,  0.0266]],

        [[-0.3005,  0.0724, -0.0296,  ...,  0.1279, -0.0031,  0.0266]],

        [[-0.3005,  0.0724, -0.0296,  ...,  0.1279, -0.0031,  0.0266]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 21 0
layer name:  MLP
i,j,k 11 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-1.1445,  0.0043, -0.1202,  ...,  0.4478, -0.0183,  0.0535]],

        [[-1.1445,  0.0043, -0.1202,  ...,  0.4478, -0.0183,  0.0535]],

        [[-1.1445,  0.0043, -0.1202,  ...,  0.4478, -0.0183,  0.0535]],

        [[-1.1445,  0.0043, -0.1202,  ...,  0.4478, -0.0183,  0.0535]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3132,  0.0054, -0.0336,  ...,  0.1171, -0.0065,  0.0125]],

        [[-0.3132,  0.0054, -0.0336,  ...,  0.1171, -0.0065,  0.0125]],

        [[-0.3132,  0.0054, -0.0336,  ...,  0.1171, -0.0065,  0.0125]],

        [[-0.3132,  0.0054, -0.0336,  ...,  0.1171, -0.0065,  0.0125]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 24 0
layer name:  MLP
i,j,k 11 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-1.0703, -0.2474, -0.0696,  ...,  0.5469, -0.0303, -0.0500]],

        [[-1.0703, -0.2474, -0.0696,  ...,  0.5469, -0.0303, -0.0500]],

        [[-1.0703, -0.2474, -0.0696,  ...,  0.5469, -0.0303, -0.0500]],

        [[-1.0703, -0.2474, -0.0696,  ...,  0.5469, -0.0303, -0.0500]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3560, -0.0824, -0.0234,  ...,  0.1685, -0.0097, -0.0186]],

        [[-0.3560, -0.0824, -0.0234,  ...,  0.1685, -0.0097, -0.0186]],

        [[-0.3560, -0.0824, -0.0234,  ...,  0.1685, -0.0097, -0.0186]],

        [[-0.3560, -0.0824, -0.0234,  ...,  0.1685, -0.0097, -0.0186]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 27 0
layer name:  MLP
i,j,k 11 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-1.0889, -0.0190,  0.0662,  ...,  0.5674, -0.2084, -0.0214]],

        [[-1.0889, -0.0190,  0.0662,  ...,  0.5674, -0.2084, -0.0214]],

        [[-1.0889, -0.0190,  0.0662,  ...,  0.5674, -0.2084, -0.0214]],

        [[-1.0889, -0.0190,  0.0662,  ...,  0.5674, -0.2084, -0.0214]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.4336,  0.0202,  0.0271,  ...,  0.2261, -0.0812, -0.0077]],

        [[-0.4336,  0.0202,  0.0271,  ...,  0.2261, -0.0812, -0.0077]],

        [[-0.4336,  0.0202,  0.0271,  ...,  0.2261, -0.0812, -0.0077]],

        [[-0.4336,  0.0202,  0.0271,  ...,  0.2261, -0.0812, -0.0077]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 30 0
layer name:  MLP
i,j,k 11 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.7915,  0.1704, -0.0069,  ...,  0.6772, -0.1165, -0.0268]],

        [[-0.7915,  0.1704, -0.0069,  ...,  0.6772, -0.1165, -0.0268]],

        [[-0.7915,  0.1704, -0.0069,  ...,  0.6772, -0.1165, -0.0268]],

        [[-0.7915,  0.1704, -0.0069,  ...,  0.6772, -0.1165, -0.0268]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3667,  0.1816,  0.0048,  ...,  0.3376, -0.0522, -0.0032]],

        [[-0.3667,  0.1816,  0.0048,  ...,  0.3376, -0.0522, -0.0032]],

        [[-0.3667,  0.1816,  0.0048,  ...,  0.3376, -0.0522, -0.0032]],

        [[-0.3667,  0.1816,  0.0048,  ...,  0.3376, -0.0522, -0.0032]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 33 0
layer name:  MLP
i,j,k 11 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3069,  0.6509,  0.0513,  ...,  0.4885, -0.2961, -0.1978]],

        [[-0.3069,  0.6509,  0.0513,  ...,  0.4885, -0.2961, -0.1978]],

        [[-0.3069,  0.6509,  0.0513,  ...,  0.4885, -0.2961, -0.1978]],

        [[-0.3069,  0.6509,  0.0513,  ...,  0.4885, -0.2961, -0.1978]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 11 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1516,  0.4404,  0.0503,  ...,  0.2925, -0.1715, -0.0847]],

        [[-0.1516,  0.4404,  0.0503,  ...,  0.2925, -0.1715, -0.0847]],

        [[-0.1516,  0.4404,  0.0503,  ...,  0.2925, -0.1715, -0.0847]],

        [[-0.1516,  0.4404,  0.0503,  ...,  0.2925, -0.1715, -0.0847]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 11 36 0
layer name:  MLP
i,j,k 11 37 0
layer name:  OutputEmbed
i,j,k 12 0 0
layer name:  InputEmbed
i,j,k 12 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4409,  0.1764,  0.3076,  ...,  0.4915,  0.2537, -0.3604]],

        [[ 0.4409,  0.1764,  0.3076,  ...,  0.4915,  0.2537, -0.3604]],

        [[ 0.4409,  0.1764,  0.3076,  ...,  0.4915,  0.2537, -0.3604]],

        [[ 0.4409,  0.1764,  0.3076,  ...,  0.4915,  0.2537, -0.3604]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1054,  0.0423,  0.0861,  ...,  0.1246,  0.0667, -0.0634]],

        [[ 0.1054,  0.0423,  0.0861,  ...,  0.1246,  0.0667, -0.0634]],

        [[ 0.1054,  0.0423,  0.0861,  ...,  0.1246,  0.0667, -0.0634]],

        [[ 0.1054,  0.0423,  0.0861,  ...,  0.1246,  0.0667, -0.0634]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 3 0
layer name:  MLP
i,j,k 12 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2966,  0.2268,  0.3091,  ...,  0.3481,  0.2400, -0.4255]],

        [[ 0.2966,  0.2268,  0.3091,  ...,  0.3481,  0.2400, -0.4255]],

        [[ 0.2966,  0.2268,  0.3091,  ...,  0.3481,  0.2400, -0.4255]],

        [[ 0.2966,  0.2268,  0.3091,  ...,  0.3481,  0.2400, -0.4255]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1035,  0.0743,  0.1116,  ...,  0.1192,  0.0731, -0.0974]],

        [[ 0.1035,  0.0743,  0.1116,  ...,  0.1192,  0.0731, -0.0974]],

        [[ 0.1035,  0.0743,  0.1116,  ...,  0.1192,  0.0731, -0.0974]],

        [[ 0.1035,  0.0743,  0.1116,  ...,  0.1192,  0.0731, -0.0974]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 6 0
layer name:  MLP
i,j,k 12 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4390,  0.3347,  0.4485,  ...,  0.3384,  0.2275, -0.4563]],

        [[ 0.4390,  0.3347,  0.4485,  ...,  0.3384,  0.2275, -0.4563]],

        [[ 0.4390,  0.3347,  0.4485,  ...,  0.3384,  0.2275, -0.4563]],

        [[ 0.4390,  0.3347,  0.4485,  ...,  0.3384,  0.2275, -0.4563]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1239,  0.1016,  0.1273,  ...,  0.0968,  0.0679, -0.0732]],

        [[ 0.1239,  0.1016,  0.1273,  ...,  0.0968,  0.0679, -0.0732]],

        [[ 0.1239,  0.1016,  0.1273,  ...,  0.0968,  0.0679, -0.0732]],

        [[ 0.1239,  0.1016,  0.1273,  ...,  0.0968,  0.0679, -0.0732]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 9 0
layer name:  MLP
i,j,k 12 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3772,  0.1814,  0.5244,  ...,  0.3162,  0.0616, -0.3789]],

        [[ 0.3772,  0.1814,  0.5244,  ...,  0.3162,  0.0616, -0.3789]],

        [[ 0.3772,  0.1814,  0.5244,  ...,  0.3162,  0.0616, -0.3789]],

        [[ 0.3772,  0.1814,  0.5244,  ...,  0.3162,  0.0616, -0.3789]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1238,  0.0779,  0.1569,  ...,  0.1068,  0.0400, -0.0724]],

        [[ 0.1238,  0.0779,  0.1569,  ...,  0.1068,  0.0400, -0.0724]],

        [[ 0.1238,  0.0779,  0.1569,  ...,  0.1068,  0.0400, -0.0724]],

        [[ 0.1238,  0.0779,  0.1569,  ...,  0.1068,  0.0400, -0.0724]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 12 0
layer name:  MLP
i,j,k 12 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4421,  0.3938,  0.4172,  ...,  0.4316,  0.0765, -0.3535]],

        [[ 0.4421,  0.3938,  0.4172,  ...,  0.4316,  0.0765, -0.3535]],

        [[ 0.4421,  0.3938,  0.4172,  ...,  0.4316,  0.0765, -0.3535]],

        [[ 0.4421,  0.3938,  0.4172,  ...,  0.4316,  0.0765, -0.3535]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1272,  0.1204,  0.1230,  ...,  0.1256,  0.0441, -0.0595]],

        [[ 0.1272,  0.1204,  0.1230,  ...,  0.1256,  0.0441, -0.0595]],

        [[ 0.1272,  0.1204,  0.1230,  ...,  0.1256,  0.0441, -0.0595]],

        [[ 0.1272,  0.1204,  0.1230,  ...,  0.1256,  0.0441, -0.0595]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 15 0
layer name:  MLP
i,j,k 12 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2974,  0.4888,  0.2006,  ...,  0.3420,  0.0482, -0.4099]],

        [[ 0.2974,  0.4888,  0.2006,  ...,  0.3420,  0.0482, -0.4099]],

        [[ 0.2974,  0.4888,  0.2006,  ...,  0.3420,  0.0482, -0.4099]],

        [[ 0.2974,  0.4888,  0.2006,  ...,  0.3420,  0.0482, -0.4099]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1071,  0.1681,  0.0784,  ...,  0.1140,  0.0405, -0.0835]],

        [[ 0.1071,  0.1681,  0.0784,  ...,  0.1140,  0.0405, -0.0835]],

        [[ 0.1071,  0.1681,  0.0784,  ...,  0.1140,  0.0405, -0.0835]],

        [[ 0.1071,  0.1681,  0.0784,  ...,  0.1140,  0.0405, -0.0835]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 18 0
layer name:  MLP
i,j,k 12 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1393,  0.2317, -0.0017,  ...,  0.4590,  0.0015, -0.2661]],

        [[ 0.1393,  0.2317, -0.0017,  ...,  0.4590,  0.0015, -0.2661]],

        [[ 0.1393,  0.2317, -0.0017,  ...,  0.4590,  0.0015, -0.2661]],

        [[ 0.1393,  0.2317, -0.0017,  ...,  0.4590,  0.0015, -0.2661]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0739,  0.1096,  0.0271,  ...,  0.1670,  0.0287, -0.0580]],

        [[ 0.0739,  0.1096,  0.0271,  ...,  0.1670,  0.0287, -0.0580]],

        [[ 0.0739,  0.1096,  0.0271,  ...,  0.1670,  0.0287, -0.0580]],

        [[ 0.0739,  0.1096,  0.0271,  ...,  0.1670,  0.0287, -0.0580]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 21 0
layer name:  MLP
i,j,k 12 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0098, -0.0800, -0.1329,  ...,  0.3167, -0.1738, -0.0765]],

        [[ 0.0098, -0.0800, -0.1329,  ...,  0.3167, -0.1738, -0.0765]],

        [[ 0.0098, -0.0800, -0.1329,  ...,  0.3167, -0.1738, -0.0765]],

        [[ 0.0098, -0.0800, -0.1329,  ...,  0.3167, -0.1738, -0.0765]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0352,  0.0099, -0.0132,  ...,  0.1324, -0.0253,  0.0043]],

        [[ 0.0352,  0.0099, -0.0132,  ...,  0.1324, -0.0253,  0.0043]],

        [[ 0.0352,  0.0099, -0.0132,  ...,  0.1324, -0.0253,  0.0043]],

        [[ 0.0352,  0.0099, -0.0132,  ...,  0.1324, -0.0253,  0.0043]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 24 0
layer name:  MLP
i,j,k 12 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0052, -0.3716, -0.1294,  ...,  0.4846, -0.3550,  0.2352]],

        [[-0.0052, -0.3716, -0.1294,  ...,  0.4846, -0.3550,  0.2352]],

        [[-0.0052, -0.3716, -0.1294,  ...,  0.4846, -0.3550,  0.2352]],

        [[-0.0052, -0.3716, -0.1294,  ...,  0.4846, -0.3550,  0.2352]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0318, -0.1069, -0.0150,  ...,  0.1924, -0.0920,  0.1133]],

        [[ 0.0318, -0.1069, -0.0150,  ...,  0.1924, -0.0920,  0.1133]],

        [[ 0.0318, -0.1069, -0.0150,  ...,  0.1924, -0.0920,  0.1133]],

        [[ 0.0318, -0.1069, -0.0150,  ...,  0.1924, -0.0920,  0.1133]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 27 0
layer name:  MLP
i,j,k 12 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1907, -0.5195,  0.0011,  ...,  0.2673, -0.4666,  0.2274]],

        [[-0.1907, -0.5195,  0.0011,  ...,  0.2673, -0.4666,  0.2274]],

        [[-0.1907, -0.5195,  0.0011,  ...,  0.2673, -0.4666,  0.2274]],

        [[-0.1907, -0.5195,  0.0011,  ...,  0.2673, -0.4666,  0.2274]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0372, -0.1371,  0.0319,  ...,  0.1334, -0.1460,  0.1184]],

        [[-0.0372, -0.1371,  0.0319,  ...,  0.1334, -0.1460,  0.1184]],

        [[-0.0372, -0.1371,  0.0319,  ...,  0.1334, -0.1460,  0.1184]],

        [[-0.0372, -0.1371,  0.0319,  ...,  0.1334, -0.1460,  0.1184]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 30 0
layer name:  MLP
i,j,k 12 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1454, -0.2101, -0.0625,  ...,  0.3091, -0.5156,  0.3545]],

        [[-0.1454, -0.2101, -0.0625,  ...,  0.3091, -0.5156,  0.3545]],

        [[-0.1454, -0.2101, -0.0625,  ...,  0.3091, -0.5156,  0.3545]],

        [[-0.1454, -0.2101, -0.0625,  ...,  0.3091, -0.5156,  0.3545]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0203, -0.0504,  0.0087,  ...,  0.1671, -0.1907,  0.1826]],

        [[-0.0203, -0.0504,  0.0087,  ...,  0.1671, -0.1907,  0.1826]],

        [[-0.0203, -0.0504,  0.0087,  ...,  0.1671, -0.1907,  0.1826]],

        [[-0.0203, -0.0504,  0.0087,  ...,  0.1671, -0.1907,  0.1826]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 33 0
layer name:  MLP
i,j,k 12 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1578, -0.5000,  0.0869,  ...,  0.2751, -0.5210,  0.5718]],

        [[-0.1578, -0.5000,  0.0869,  ...,  0.2751, -0.5210,  0.5718]],

        [[-0.1578, -0.5000,  0.0869,  ...,  0.2751, -0.5210,  0.5718]],

        [[-0.1578, -0.5000,  0.0869,  ...,  0.2751, -0.5210,  0.5718]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 12 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0342, -0.1549,  0.0883,  ...,  0.1780, -0.2438,  0.3086]],

        [[-0.0342, -0.1549,  0.0883,  ...,  0.1780, -0.2438,  0.3086]],

        [[-0.0342, -0.1549,  0.0883,  ...,  0.1780, -0.2438,  0.3086]],

        [[-0.0342, -0.1549,  0.0883,  ...,  0.1780, -0.2438,  0.3086]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 12 36 0
layer name:  MLP
i,j,k 12 37 0
layer name:  OutputEmbed
i,j,k 13 0 0
layer name:  InputEmbed
i,j,k 13 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0227,  0.4702, -0.2507,  ..., -0.0248, -0.2230,  0.0633]],

        [[ 0.0227,  0.4702, -0.2507,  ..., -0.0248, -0.2230,  0.0633]],

        [[ 0.0227,  0.4702, -0.2507,  ..., -0.0248, -0.2230,  0.0633]],

        [[ 0.0227,  0.4702, -0.2507,  ..., -0.0248, -0.2230,  0.0633]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0164,  0.0654, -0.0818,  ..., -0.0258, -0.0765,  0.0099]],

        [[-0.0164,  0.0654, -0.0818,  ..., -0.0258, -0.0765,  0.0099]],

        [[-0.0164,  0.0654, -0.0818,  ..., -0.0258, -0.0765,  0.0099]],

        [[-0.0164,  0.0654, -0.0818,  ..., -0.0258, -0.0765,  0.0099]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 3 0
layer name:  MLP
i,j,k 13 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1370,  0.5405, -0.2217,  ...,  0.0263, -0.0473,  0.1104]],

        [[ 0.1370,  0.5405, -0.2217,  ...,  0.0263, -0.0473,  0.1104]],

        [[ 0.1370,  0.5405, -0.2217,  ...,  0.0263, -0.0473,  0.1104]],

        [[ 0.1370,  0.5405, -0.2217,  ...,  0.0263, -0.0473,  0.1104]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0098,  0.0738, -0.0703,  ..., -0.0156, -0.0327,  0.0208]],

        [[ 0.0098,  0.0738, -0.0703,  ..., -0.0156, -0.0327,  0.0208]],

        [[ 0.0098,  0.0738, -0.0703,  ..., -0.0156, -0.0327,  0.0208]],

        [[ 0.0098,  0.0738, -0.0703,  ..., -0.0156, -0.0327,  0.0208]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 6 0
layer name:  MLP
i,j,k 13 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4553,  0.4810, -0.1101,  ...,  0.0066,  0.0822,  0.1871]],

        [[ 0.4553,  0.4810, -0.1101,  ...,  0.0066,  0.0822,  0.1871]],

        [[ 0.4553,  0.4810, -0.1101,  ...,  0.0066,  0.0822,  0.1871]],

        [[ 0.4553,  0.4810, -0.1101,  ...,  0.0066,  0.0822,  0.1871]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0642,  0.0699, -0.0377,  ..., -0.0180, -0.0072,  0.0213]],

        [[ 0.0642,  0.0699, -0.0377,  ..., -0.0180, -0.0072,  0.0213]],

        [[ 0.0642,  0.0699, -0.0377,  ..., -0.0180, -0.0072,  0.0213]],

        [[ 0.0642,  0.0699, -0.0377,  ..., -0.0180, -0.0072,  0.0213]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 9 0
layer name:  MLP
i,j,k 13 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.5518,  0.5693, -0.0034,  ...,  0.0199,  0.3374,  0.1882]],

        [[ 0.5518,  0.5693, -0.0034,  ...,  0.0199,  0.3374,  0.1882]],

        [[ 0.5518,  0.5693, -0.0034,  ...,  0.0199,  0.3374,  0.1882]],

        [[ 0.5518,  0.5693, -0.0034,  ...,  0.0199,  0.3374,  0.1882]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0921,  0.1027, -0.0187,  ..., -0.0149,  0.0438,  0.0233]],

        [[ 0.0921,  0.1027, -0.0187,  ..., -0.0149,  0.0438,  0.0233]],

        [[ 0.0921,  0.1027, -0.0187,  ..., -0.0149,  0.0438,  0.0233]],

        [[ 0.0921,  0.1027, -0.0187,  ..., -0.0149,  0.0438,  0.0233]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 12 0
layer name:  MLP
i,j,k 13 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.7202,  0.6646, -0.2106,  ...,  0.1804,  0.6162,  0.4065]],

        [[ 0.7202,  0.6646, -0.2106,  ...,  0.1804,  0.6162,  0.4065]],

        [[ 0.7202,  0.6646, -0.2106,  ...,  0.1804,  0.6162,  0.4065]],

        [[ 0.7202,  0.6646, -0.2106,  ...,  0.1804,  0.6162,  0.4065]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1115,  0.1060, -0.0547,  ...,  0.0150,  0.0934,  0.0632]],

        [[ 0.1115,  0.1060, -0.0547,  ...,  0.0150,  0.0934,  0.0632]],

        [[ 0.1115,  0.1060, -0.0547,  ...,  0.0150,  0.0934,  0.0632]],

        [[ 0.1115,  0.1060, -0.0547,  ...,  0.0150,  0.0934,  0.0632]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 15 0
layer name:  MLP
i,j,k 13 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.8882,  0.5815, -0.4468,  ..., -0.0277,  0.6548,  0.2744]],

        [[ 0.8882,  0.5815, -0.4468,  ..., -0.0277,  0.6548,  0.2744]],

        [[ 0.8882,  0.5815, -0.4468,  ..., -0.0277,  0.6548,  0.2744]],

        [[ 0.8882,  0.5815, -0.4468,  ..., -0.0277,  0.6548,  0.2744]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1854,  0.1236, -0.1125,  ..., -0.0228,  0.1238,  0.0468]],

        [[ 0.1854,  0.1236, -0.1125,  ..., -0.0228,  0.1238,  0.0468]],

        [[ 0.1854,  0.1236, -0.1125,  ..., -0.0228,  0.1238,  0.0468]],

        [[ 0.1854,  0.1236, -0.1125,  ..., -0.0228,  0.1238,  0.0468]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 18 0
layer name:  MLP
i,j,k 13 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.6885,  0.3013, -0.5850,  ...,  0.1725,  0.6440,  0.6440]],

        [[ 0.6885,  0.3013, -0.5850,  ...,  0.1725,  0.6440,  0.6440]],

        [[ 0.6885,  0.3013, -0.5850,  ...,  0.1725,  0.6440,  0.6440]],

        [[ 0.6885,  0.3013, -0.5850,  ...,  0.1725,  0.6440,  0.6440]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1648,  0.0681, -0.1625,  ...,  0.0252,  0.1418,  0.1587]],

        [[ 0.1648,  0.0681, -0.1625,  ...,  0.0252,  0.1418,  0.1587]],

        [[ 0.1648,  0.0681, -0.1625,  ...,  0.0252,  0.1418,  0.1587]],

        [[ 0.1648,  0.0681, -0.1625,  ...,  0.0252,  0.1418,  0.1587]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 21 0
layer name:  MLP
i,j,k 13 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.7598,  0.1735, -0.8516,  ...,  0.3179,  0.0632,  0.6997]],

        [[ 0.7598,  0.1735, -0.8516,  ...,  0.3179,  0.0632,  0.6997]],

        [[ 0.7598,  0.1735, -0.8516,  ...,  0.3179,  0.0632,  0.6997]],

        [[ 0.7598,  0.1735, -0.8516,  ...,  0.3179,  0.0632,  0.6997]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2305,  0.0489, -0.2861,  ...,  0.0834,  0.0043,  0.2075]],

        [[ 0.2305,  0.0489, -0.2861,  ...,  0.0834,  0.0043,  0.2075]],

        [[ 0.2305,  0.0489, -0.2861,  ...,  0.0834,  0.0043,  0.2075]],

        [[ 0.2305,  0.0489, -0.2861,  ...,  0.0834,  0.0043,  0.2075]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 24 0
layer name:  MLP
i,j,k 13 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.5132, -0.3926, -0.9131,  ...,  0.4209, -0.3115,  0.5596]],

        [[ 0.5132, -0.3926, -0.9131,  ...,  0.4209, -0.3115,  0.5596]],

        [[ 0.5132, -0.3926, -0.9131,  ...,  0.4209, -0.3115,  0.5596]],

        [[ 0.5132, -0.3926, -0.9131,  ...,  0.4209, -0.3115,  0.5596]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1693, -0.1591, -0.3350,  ...,  0.1216, -0.1214,  0.1791]],

        [[ 0.1693, -0.1591, -0.3350,  ...,  0.1216, -0.1214,  0.1791]],

        [[ 0.1693, -0.1591, -0.3350,  ...,  0.1216, -0.1214,  0.1791]],

        [[ 0.1693, -0.1591, -0.3350,  ...,  0.1216, -0.1214,  0.1791]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 27 0
layer name:  MLP
i,j,k 13 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4563,  0.2932, -0.8867,  ...,  0.3757, -0.0170,  0.0507]],

        [[ 0.4563,  0.2932, -0.8867,  ...,  0.3757, -0.0170,  0.0507]],

        [[ 0.4563,  0.2932, -0.8867,  ...,  0.3757, -0.0170,  0.0507]],

        [[ 0.4563,  0.2932, -0.8867,  ...,  0.3757, -0.0170,  0.0507]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1887,  0.1384, -0.3977,  ...,  0.1464, -0.0177,  0.0091]],

        [[ 0.1887,  0.1384, -0.3977,  ...,  0.1464, -0.0177,  0.0091]],

        [[ 0.1887,  0.1384, -0.3977,  ...,  0.1464, -0.0177,  0.0091]],

        [[ 0.1887,  0.1384, -0.3977,  ...,  0.1464, -0.0177,  0.0091]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 30 0
layer name:  MLP
i,j,k 13 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0763,  0.6577, -0.6729,  ...,  0.5249, -0.1063,  0.0262]],

        [[ 0.0763,  0.6577, -0.6729,  ...,  0.5249, -0.1063,  0.0262]],

        [[ 0.0763,  0.6577, -0.6729,  ...,  0.5249, -0.1063,  0.0262]],

        [[ 0.0763,  0.6577, -0.6729,  ...,  0.5249, -0.1063,  0.0262]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0396,  0.5435, -0.3579,  ...,  0.2629, -0.0634,  0.0089]],

        [[ 0.0396,  0.5435, -0.3579,  ...,  0.2629, -0.0634,  0.0089]],

        [[ 0.0396,  0.5435, -0.3579,  ...,  0.2629, -0.0634,  0.0089]],

        [[ 0.0396,  0.5435, -0.3579,  ...,  0.2629, -0.0634,  0.0089]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 33 0
layer name:  MLP
i,j,k 13 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1750,  1.4736, -0.4209,  ...,  0.4487, -0.0230,  0.0026]],

        [[ 0.1750,  1.4736, -0.4209,  ...,  0.4487, -0.0230,  0.0026]],

        [[ 0.1750,  1.4736, -0.4209,  ...,  0.4487, -0.0230,  0.0026]],

        [[ 0.1750,  1.4736, -0.4209,  ...,  0.4487, -0.0230,  0.0026]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 13 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1094,  0.9429, -0.2559,  ...,  0.2722, -0.0209,  0.0082]],

        [[ 0.1094,  0.9429, -0.2559,  ...,  0.2722, -0.0209,  0.0082]],

        [[ 0.1094,  0.9429, -0.2559,  ...,  0.2722, -0.0209,  0.0082]],

        [[ 0.1094,  0.9429, -0.2559,  ...,  0.2722, -0.0209,  0.0082]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 13 36 0
layer name:  MLP
i,j,k 13 37 0
layer name:  OutputEmbed
i,j,k 14 0 0
layer name:  InputEmbed
i,j,k 14 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0987,  0.7046,  0.0179,  ..., -0.1145,  0.2668, -0.3689]],

        [[-0.0987,  0.7046,  0.0179,  ..., -0.1145,  0.2668, -0.3689]],

        [[-0.0987,  0.7046,  0.0179,  ..., -0.1145,  0.2668, -0.3689]],

        [[-0.0987,  0.7046,  0.0179,  ..., -0.1145,  0.2668, -0.3689]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0362,  0.1082, -0.0145,  ..., -0.0402,  0.0300, -0.1095]],

        [[-0.0362,  0.1082, -0.0145,  ..., -0.0402,  0.0300, -0.1095]],

        [[-0.0362,  0.1082, -0.0145,  ..., -0.0402,  0.0300, -0.1095]],

        [[-0.0362,  0.1082, -0.0145,  ..., -0.0402,  0.0300, -0.1095]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 3 0
layer name:  MLP
i,j,k 14 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0997,  0.8618, -0.0665,  ...,  0.2607,  0.3096, -0.1875]],

        [[-0.0997,  0.8618, -0.0665,  ...,  0.2607,  0.3096, -0.1875]],

        [[-0.0997,  0.8618, -0.0665,  ...,  0.2607,  0.3096, -0.1875]],

        [[-0.0997,  0.8618, -0.0665,  ...,  0.2607,  0.3096, -0.1875]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0380,  0.1277, -0.0305,  ...,  0.0402,  0.0354, -0.0520]],

        [[-0.0380,  0.1277, -0.0305,  ...,  0.0402,  0.0354, -0.0520]],

        [[-0.0380,  0.1277, -0.0305,  ...,  0.0402,  0.0354, -0.0520]],

        [[-0.0380,  0.1277, -0.0305,  ...,  0.0402,  0.0354, -0.0520]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 6 0
layer name:  MLP
i,j,k 14 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0126,  1.0596,  0.0955,  ...,  0.5894,  0.1736, -0.2463]],

        [[ 0.0126,  1.0596,  0.0955,  ...,  0.5894,  0.1736, -0.2463]],

        [[ 0.0126,  1.0596,  0.0955,  ...,  0.5894,  0.1736, -0.2463]],

        [[ 0.0126,  1.0596,  0.0955,  ...,  0.5894,  0.1736, -0.2463]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0134,  0.1768,  0.0031,  ...,  0.0850,  0.0106, -0.0551]],

        [[-0.0134,  0.1768,  0.0031,  ...,  0.0850,  0.0106, -0.0551]],

        [[-0.0134,  0.1768,  0.0031,  ...,  0.0850,  0.0106, -0.0551]],

        [[-0.0134,  0.1768,  0.0031,  ...,  0.0850,  0.0106, -0.0551]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 9 0
layer name:  MLP
i,j,k 14 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0087,  1.1338,  0.0511,  ...,  0.6758,  0.2013, -0.2749]],

        [[-0.0087,  1.1338,  0.0511,  ...,  0.6758,  0.2013, -0.2749]],

        [[-0.0087,  1.1338,  0.0511,  ...,  0.6758,  0.2013, -0.2749]],

        [[-0.0087,  1.1338,  0.0511,  ...,  0.6758,  0.2013, -0.2749]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0174,  0.2183, -0.0053,  ...,  0.1155,  0.0209, -0.0679]],

        [[-0.0174,  0.2183, -0.0053,  ...,  0.1155,  0.0209, -0.0679]],

        [[-0.0174,  0.2183, -0.0053,  ...,  0.1155,  0.0209, -0.0679]],

        [[-0.0174,  0.2183, -0.0053,  ...,  0.1155,  0.0209, -0.0679]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 12 0
layer name:  MLP
i,j,k 14 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0519,  1.3076, -0.2898,  ...,  0.7803,  0.1855, -0.3435]],

        [[ 0.0519,  1.3076, -0.2898,  ...,  0.7803,  0.1855, -0.3435]],

        [[ 0.0519,  1.3076, -0.2898,  ...,  0.7803,  0.1855, -0.3435]],

        [[ 0.0519,  1.3076, -0.2898,  ...,  0.7803,  0.1855, -0.3435]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0063,  0.2150, -0.0634,  ...,  0.1186,  0.0173, -0.0763]],

        [[-0.0063,  0.2150, -0.0634,  ...,  0.1186,  0.0173, -0.0763]],

        [[-0.0063,  0.2150, -0.0634,  ...,  0.1186,  0.0173, -0.0763]],

        [[-0.0063,  0.2150, -0.0634,  ...,  0.1186,  0.0173, -0.0763]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 15 0
layer name:  MLP
i,j,k 14 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3435,  1.1426, -0.0867,  ...,  0.3628, -0.1257,  0.1259]],

        [[ 0.3435,  1.1426, -0.0867,  ...,  0.3628, -0.1257,  0.1259]],

        [[ 0.3435,  1.1426, -0.0867,  ...,  0.3628, -0.1257,  0.1259]],

        [[ 0.3435,  1.1426, -0.0867,  ...,  0.3628, -0.1257,  0.1259]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0618,  0.2515, -0.0315,  ...,  0.0616, -0.0388,  0.0151]],

        [[ 0.0618,  0.2515, -0.0315,  ...,  0.0616, -0.0388,  0.0151]],

        [[ 0.0618,  0.2515, -0.0315,  ...,  0.0616, -0.0388,  0.0151]],

        [[ 0.0618,  0.2515, -0.0315,  ...,  0.0616, -0.0388,  0.0151]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 18 0
layer name:  MLP
i,j,k 14 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1012,  0.6519, -0.4094,  ...,  0.4849, -0.0424,  0.1458]],

        [[ 0.1012,  0.6519, -0.4094,  ...,  0.4849, -0.0424,  0.1458]],

        [[ 0.1012,  0.6519, -0.4094,  ...,  0.4849, -0.0424,  0.1458]],

        [[ 0.1012,  0.6519, -0.4094,  ...,  0.4849, -0.0424,  0.1458]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0111,  0.1494, -0.1053,  ...,  0.0938, -0.0225,  0.0241]],

        [[ 0.0111,  0.1494, -0.1053,  ...,  0.0938, -0.0225,  0.0241]],

        [[ 0.0111,  0.1494, -0.1053,  ...,  0.0938, -0.0225,  0.0241]],

        [[ 0.0111,  0.1494, -0.1053,  ...,  0.0938, -0.0225,  0.0241]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 21 0
layer name:  MLP
i,j,k 14 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0627,  0.4624,  0.0020,  ...,  0.9326, -0.4893,  0.4236]],

        [[ 0.0627,  0.4624,  0.0020,  ...,  0.9326, -0.4893,  0.4236]],

        [[ 0.0627,  0.4624,  0.0020,  ...,  0.9326, -0.4893,  0.4236]],

        [[ 0.0627,  0.4624,  0.0020,  ...,  0.9326, -0.4893,  0.4236]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0083,  0.1246, -0.0089,  ...,  0.2374, -0.1371,  0.1039]],

        [[ 0.0083,  0.1246, -0.0089,  ...,  0.2374, -0.1371,  0.1039]],

        [[ 0.0083,  0.1246, -0.0089,  ...,  0.2374, -0.1371,  0.1039]],

        [[ 0.0083,  0.1246, -0.0089,  ...,  0.2374, -0.1371,  0.1039]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 24 0
layer name:  MLP
i,j,k 14 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0082,  0.1198,  0.0745,  ...,  1.0938, -0.6255,  0.4500]],

        [[-0.0082,  0.1198,  0.0745,  ...,  1.0938, -0.6255,  0.4500]],

        [[-0.0082,  0.1198,  0.0745,  ...,  1.0938, -0.6255,  0.4500]],

        [[-0.0082,  0.1198,  0.0745,  ...,  1.0938, -0.6255,  0.4500]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0092,  0.0393,  0.0134,  ...,  0.3020, -0.1918,  0.1249]],

        [[-0.0092,  0.0393,  0.0134,  ...,  0.3020, -0.1918,  0.1249]],

        [[-0.0092,  0.0393,  0.0134,  ...,  0.3020, -0.1918,  0.1249]],

        [[-0.0092,  0.0393,  0.0134,  ...,  0.3020, -0.1918,  0.1249]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 27 0
layer name:  MLP
i,j,k 14 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1160,  0.4731,  0.1353,  ...,  0.9995, -0.8428,  0.7705]],

        [[ 0.1160,  0.4731,  0.1353,  ...,  0.9995, -0.8428,  0.7705]],

        [[ 0.1160,  0.4731,  0.1353,  ...,  0.9995, -0.8428,  0.7705]],

        [[ 0.1160,  0.4731,  0.1353,  ...,  0.9995, -0.8428,  0.7705]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0401,  0.1873,  0.0429,  ...,  0.3582, -0.3159,  0.2725]],

        [[ 0.0401,  0.1873,  0.0429,  ...,  0.3582, -0.3159,  0.2725]],

        [[ 0.0401,  0.1873,  0.0429,  ...,  0.3582, -0.3159,  0.2725]],

        [[ 0.0401,  0.1873,  0.0429,  ...,  0.3582, -0.3159,  0.2725]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 30 0
layer name:  MLP
i,j,k 14 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1039,  0.5635,  0.0738,  ...,  1.0684, -0.8218,  0.8423]],

        [[ 0.1039,  0.5635,  0.0738,  ...,  1.0684, -0.8218,  0.8423]],

        [[ 0.1039,  0.5635,  0.0738,  ...,  1.0684, -0.8218,  0.8423]],

        [[ 0.1039,  0.5635,  0.0738,  ...,  1.0684, -0.8218,  0.8423]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0460,  0.3950,  0.0293,  ...,  0.4539, -0.3618,  0.3433]],

        [[ 0.0460,  0.3950,  0.0293,  ...,  0.4539, -0.3618,  0.3433]],

        [[ 0.0460,  0.3950,  0.0293,  ...,  0.4539, -0.3618,  0.3433]],

        [[ 0.0460,  0.3950,  0.0293,  ...,  0.4539, -0.3618,  0.3433]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 33 0
layer name:  MLP
i,j,k 14 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1664,  0.8125,  0.1958,  ...,  1.0918, -0.5972,  0.7656]],

        [[-0.1664,  0.8125,  0.1958,  ...,  1.0918, -0.5972,  0.7656]],

        [[-0.1664,  0.8125,  0.1958,  ...,  1.0918, -0.5972,  0.7656]],

        [[-0.1664,  0.8125,  0.1958,  ...,  1.0918, -0.5972,  0.7656]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 14 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0837,  0.5112,  0.1216,  ...,  0.6069, -0.3596,  0.4067]],

        [[-0.0837,  0.5112,  0.1216,  ...,  0.6069, -0.3596,  0.4067]],

        [[-0.0837,  0.5112,  0.1216,  ...,  0.6069, -0.3596,  0.4067]],

        [[-0.0837,  0.5112,  0.1216,  ...,  0.6069, -0.3596,  0.4067]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 14 36 0
layer name:  MLP
i,j,k 14 37 0
layer name:  OutputEmbed
i,j,k 15 0 0
layer name:  InputEmbed
i,j,k 15 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3816,  0.3896, -0.2269,  ...,  0.0193, -0.2030,  0.3760]],

        [[ 0.3816,  0.3896, -0.2269,  ...,  0.0193, -0.2030,  0.3760]],

        [[ 0.3816,  0.3896, -0.2269,  ...,  0.0193, -0.2030,  0.3760]],

        [[ 0.3816,  0.3896, -0.2269,  ...,  0.0193, -0.2030,  0.3760]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0435,  0.0310, -0.0967,  ..., -0.0341, -0.0925,  0.0913]],

        [[ 0.0435,  0.0310, -0.0967,  ..., -0.0341, -0.0925,  0.0913]],

        [[ 0.0435,  0.0310, -0.0967,  ..., -0.0341, -0.0925,  0.0913]],

        [[ 0.0435,  0.0310, -0.0967,  ..., -0.0341, -0.0925,  0.0913]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 3 0
layer name:  MLP
i,j,k 15 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1982,  0.4495, -0.2625,  ...,  0.0936, -0.2913,  0.4326]],

        [[ 0.1982,  0.4495, -0.2625,  ...,  0.0936, -0.2913,  0.4326]],

        [[ 0.1982,  0.4495, -0.2625,  ...,  0.0936, -0.2913,  0.4326]],

        [[ 0.1982,  0.4495, -0.2625,  ...,  0.0936, -0.2913,  0.4326]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0025,  0.0361, -0.0955,  ..., -0.0199, -0.0934,  0.0801]],

        [[ 0.0025,  0.0361, -0.0955,  ..., -0.0199, -0.0934,  0.0801]],

        [[ 0.0025,  0.0361, -0.0955,  ..., -0.0199, -0.0934,  0.0801]],

        [[ 0.0025,  0.0361, -0.0955,  ..., -0.0199, -0.0934,  0.0801]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 6 0
layer name:  MLP
i,j,k 15 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2402,  0.5098, -0.2832,  ...,  0.2328, -0.2920,  0.6001]],

        [[ 0.2402,  0.5098, -0.2832,  ...,  0.2328, -0.2920,  0.6001]],

        [[ 0.2402,  0.5098, -0.2832,  ...,  0.2328, -0.2920,  0.6001]],

        [[ 0.2402,  0.5098, -0.2832,  ...,  0.2328, -0.2920,  0.6001]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0017,  0.0478, -0.0842,  ..., -0.0017, -0.0820,  0.0687]],

        [[ 0.0017,  0.0478, -0.0842,  ..., -0.0017, -0.0820,  0.0687]],

        [[ 0.0017,  0.0478, -0.0842,  ..., -0.0017, -0.0820,  0.0687]],

        [[ 0.0017,  0.0478, -0.0842,  ..., -0.0017, -0.0820,  0.0687]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 9 0
layer name:  MLP
i,j,k 15 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1088,  0.3535, -0.2571,  ...,  0.3040, -0.2192,  0.5176]],

        [[ 0.1088,  0.3535, -0.2571,  ...,  0.3040, -0.2192,  0.5176]],

        [[ 0.1088,  0.3535, -0.2571,  ...,  0.3040, -0.2192,  0.5176]],

        [[ 0.1088,  0.3535, -0.2571,  ...,  0.3040, -0.2192,  0.5176]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0190,  0.0299, -0.0819,  ...,  0.0154, -0.0751,  0.0601]],

        [[-0.0190,  0.0299, -0.0819,  ...,  0.0154, -0.0751,  0.0601]],

        [[-0.0190,  0.0299, -0.0819,  ...,  0.0154, -0.0751,  0.0601]],

        [[-0.0190,  0.0299, -0.0819,  ...,  0.0154, -0.0751,  0.0601]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 12 0
layer name:  MLP
i,j,k 15 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0415,  0.3237, -0.1978,  ...,  0.3127, -0.2507,  0.3857]],

        [[ 0.0415,  0.3237, -0.1978,  ...,  0.3127, -0.2507,  0.3857]],

        [[ 0.0415,  0.3237, -0.1978,  ...,  0.3127, -0.2507,  0.3857]],

        [[ 0.0415,  0.3237, -0.1978,  ...,  0.3127, -0.2507,  0.3857]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0319,  0.0148, -0.0681,  ...,  0.0110, -0.0766,  0.0287]],

        [[-0.0319,  0.0148, -0.0681,  ...,  0.0110, -0.0766,  0.0287]],

        [[-0.0319,  0.0148, -0.0681,  ...,  0.0110, -0.0766,  0.0287]],

        [[-0.0319,  0.0148, -0.0681,  ...,  0.0110, -0.0766,  0.0287]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 15 0
layer name:  MLP
i,j,k 15 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0279,  0.1396, -0.1803,  ...,  0.0801, -0.5283,  0.2795]],

        [[-0.0279,  0.1396, -0.1803,  ...,  0.0801, -0.5283,  0.2795]],

        [[-0.0279,  0.1396, -0.1803,  ...,  0.0801, -0.5283,  0.2795]],

        [[-0.0279,  0.1396, -0.1803,  ...,  0.0801, -0.5283,  0.2795]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0431, -0.0082, -0.0685,  ..., -0.0234, -0.1281,  0.0153]],

        [[-0.0431, -0.0082, -0.0685,  ..., -0.0234, -0.1281,  0.0153]],

        [[-0.0431, -0.0082, -0.0685,  ..., -0.0234, -0.1281,  0.0153]],

        [[-0.0431, -0.0082, -0.0685,  ..., -0.0234, -0.1281,  0.0153]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 18 0
layer name:  MLP
i,j,k 15 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1414, -0.0118, -0.5615,  ..., -0.1523, -0.5000,  0.3767]],

        [[-0.1414, -0.0118, -0.5615,  ..., -0.1523, -0.5000,  0.3767]],

        [[-0.1414, -0.0118, -0.5615,  ..., -0.1523, -0.5000,  0.3767]],

        [[-0.1414, -0.0118, -0.5615,  ..., -0.1523, -0.5000,  0.3767]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0667, -0.0368, -0.1509,  ..., -0.0675, -0.1370,  0.0479]],

        [[-0.0667, -0.0368, -0.1509,  ..., -0.0675, -0.1370,  0.0479]],

        [[-0.0667, -0.0368, -0.1509,  ..., -0.0675, -0.1370,  0.0479]],

        [[-0.0667, -0.0368, -0.1509,  ..., -0.0675, -0.1370,  0.0479]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 21 0
layer name:  MLP
i,j,k 15 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2145, -0.1090, -0.5337,  ...,  0.0567, -0.5078,  0.2576]],

        [[-0.2145, -0.1090, -0.5337,  ...,  0.0567, -0.5078,  0.2576]],

        [[-0.2145, -0.1090, -0.5337,  ...,  0.0567, -0.5078,  0.2576]],

        [[-0.2145, -0.1090, -0.5337,  ...,  0.0567, -0.5078,  0.2576]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0861, -0.0563, -0.1658,  ..., -0.0211, -0.1548,  0.0299]],

        [[-0.0861, -0.0563, -0.1658,  ..., -0.0211, -0.1548,  0.0299]],

        [[-0.0861, -0.0563, -0.1658,  ..., -0.0211, -0.1548,  0.0299]],

        [[-0.0861, -0.0563, -0.1658,  ..., -0.0211, -0.1548,  0.0299]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 24 0
layer name:  MLP
i,j,k 15 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2179, -0.5112, -0.5786,  ...,  0.0012, -0.3608,  0.0616]],

        [[-0.2179, -0.5112, -0.5786,  ...,  0.0012, -0.3608,  0.0616]],

        [[-0.2179, -0.5112, -0.5786,  ...,  0.0012, -0.3608,  0.0616]],

        [[-0.2179, -0.5112, -0.5786,  ...,  0.0012, -0.3608,  0.0616]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0939, -0.1891, -0.1995,  ..., -0.0351, -0.1335, -0.0156]],

        [[-0.0939, -0.1891, -0.1995,  ..., -0.0351, -0.1335, -0.0156]],

        [[-0.0939, -0.1891, -0.1995,  ..., -0.0351, -0.1335, -0.0156]],

        [[-0.0939, -0.1891, -0.1995,  ..., -0.0351, -0.1335, -0.0156]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 27 0
layer name:  MLP
i,j,k 15 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3582,  0.0324, -0.3665,  ..., -0.0330, -0.4653,  0.0432]],

        [[-0.3582,  0.0324, -0.3665,  ..., -0.0330, -0.4653,  0.0432]],

        [[-0.3582,  0.0324, -0.3665,  ..., -0.0330, -0.4653,  0.0432]],

        [[-0.3582,  0.0324, -0.3665,  ..., -0.0330, -0.4653,  0.0432]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1571,  0.0070, -0.1665,  ..., -0.0424, -0.1986, -0.0137]],

        [[-0.1571,  0.0070, -0.1665,  ..., -0.0424, -0.1986, -0.0137]],

        [[-0.1571,  0.0070, -0.1665,  ..., -0.0424, -0.1986, -0.0137]],

        [[-0.1571,  0.0070, -0.1665,  ..., -0.0424, -0.1986, -0.0137]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 30 0
layer name:  MLP
i,j,k 15 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4941,  0.5063, -0.0237,  ...,  0.1671, -0.2827,  0.0087]],

        [[-0.4941,  0.5063, -0.0237,  ...,  0.1671, -0.2827,  0.0087]],

        [[-0.4941,  0.5063, -0.0237,  ...,  0.1671, -0.2827,  0.0087]],

        [[-0.4941,  0.5063, -0.0237,  ...,  0.1671, -0.2827,  0.0087]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2372,  0.3608, -0.0333,  ...,  0.0514, -0.1545, -0.0176]],

        [[-0.2372,  0.3608, -0.0333,  ...,  0.0514, -0.1545, -0.0176]],

        [[-0.2372,  0.3608, -0.0333,  ...,  0.0514, -0.1545, -0.0176]],

        [[-0.2372,  0.3608, -0.0333,  ...,  0.0514, -0.1545, -0.0176]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 33 0
layer name:  MLP
i,j,k 15 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5884,  1.6074,  0.2664,  ...,  0.0668, -0.2404,  0.0605]],

        [[-0.5884,  1.6074,  0.2664,  ...,  0.0668, -0.2404,  0.0605]],

        [[-0.5884,  1.6074,  0.2664,  ...,  0.0668, -0.2404,  0.0605]],

        [[-0.5884,  1.6074,  0.2664,  ...,  0.0668, -0.2404,  0.0605]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 15 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3564,  1.0068,  0.1577,  ...,  0.0314, -0.1755,  0.0255]],

        [[-0.3564,  1.0068,  0.1577,  ...,  0.0314, -0.1755,  0.0255]],

        [[-0.3564,  1.0068,  0.1577,  ...,  0.0314, -0.1755,  0.0255]],

        [[-0.3564,  1.0068,  0.1577,  ...,  0.0314, -0.1755,  0.0255]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 15 36 0
layer name:  MLP
i,j,k 15 37 0
layer name:  OutputEmbed
i,j,k 16 0 0
layer name:  InputEmbed
i,j,k 16 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0641,  0.5083,  0.2059,  ..., -0.3584, -0.0457,  0.2769]],

        [[-0.0641,  0.5083,  0.2059,  ..., -0.3584, -0.0457,  0.2769]],

        [[-0.0641,  0.5083,  0.2059,  ..., -0.3584, -0.0457,  0.2769]],

        [[-0.0641,  0.5083,  0.2059,  ..., -0.3584, -0.0457,  0.2769]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0208,  0.0775,  0.0344,  ..., -0.0828, -0.0234,  0.0790]],

        [[-0.0208,  0.0775,  0.0344,  ..., -0.0828, -0.0234,  0.0790]],

        [[-0.0208,  0.0775,  0.0344,  ..., -0.0828, -0.0234,  0.0790]],

        [[-0.0208,  0.0775,  0.0344,  ..., -0.0828, -0.0234,  0.0790]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 3 0
layer name:  MLP
i,j,k 16 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0749,  0.3997,  0.2333,  ..., -0.4119, -0.2139,  0.2427]],

        [[-0.0749,  0.3997,  0.2333,  ..., -0.4119, -0.2139,  0.2427]],

        [[-0.0749,  0.3997,  0.2333,  ..., -0.4119, -0.2139,  0.2427]],

        [[-0.0749,  0.3997,  0.2333,  ..., -0.4119, -0.2139,  0.2427]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0234,  0.0615,  0.0459,  ..., -0.1000, -0.0491,  0.0665]],

        [[-0.0234,  0.0615,  0.0459,  ..., -0.1000, -0.0491,  0.0665]],

        [[-0.0234,  0.0615,  0.0459,  ..., -0.1000, -0.0491,  0.0665]],

        [[-0.0234,  0.0615,  0.0459,  ..., -0.1000, -0.0491,  0.0665]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 6 0
layer name:  MLP
i,j,k 16 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0564,  0.3054,  0.3721,  ..., -0.5308, -0.4175,  0.3481]],

        [[ 0.0564,  0.3054,  0.3721,  ..., -0.5308, -0.4175,  0.3481]],

        [[ 0.0564,  0.3054,  0.3721,  ..., -0.5308, -0.4175,  0.3481]],

        [[ 0.0564,  0.3054,  0.3721,  ..., -0.5308, -0.4175,  0.3481]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0039,  0.0499,  0.0620,  ..., -0.0964, -0.0728,  0.0631]],

        [[ 0.0039,  0.0499,  0.0620,  ..., -0.0964, -0.0728,  0.0631]],

        [[ 0.0039,  0.0499,  0.0620,  ..., -0.0964, -0.0728,  0.0631]],

        [[ 0.0039,  0.0499,  0.0620,  ..., -0.0964, -0.0728,  0.0631]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 9 0
layer name:  MLP
i,j,k 16 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1918,  0.3003,  0.3088,  ..., -0.3401, -0.3252,  0.2717]],

        [[ 0.1918,  0.3003,  0.3088,  ..., -0.3401, -0.3252,  0.2717]],

        [[ 0.1918,  0.3003,  0.3088,  ..., -0.3401, -0.3252,  0.2717]],

        [[ 0.1918,  0.3003,  0.3088,  ..., -0.3401, -0.3252,  0.2717]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0320,  0.0582,  0.0529,  ..., -0.0719, -0.0651,  0.0524]],

        [[ 0.0320,  0.0582,  0.0529,  ..., -0.0719, -0.0651,  0.0524]],

        [[ 0.0320,  0.0582,  0.0529,  ..., -0.0719, -0.0651,  0.0524]],

        [[ 0.0320,  0.0582,  0.0529,  ..., -0.0719, -0.0651,  0.0524]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 12 0
layer name:  MLP
i,j,k 16 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3115,  0.2406,  0.2974,  ..., -0.2871, -0.1621,  0.2141]],

        [[ 0.3115,  0.2406,  0.2974,  ..., -0.2871, -0.1621,  0.2141]],

        [[ 0.3115,  0.2406,  0.2974,  ..., -0.2871, -0.1621,  0.2141]],

        [[ 0.3115,  0.2406,  0.2974,  ..., -0.2871, -0.1621,  0.2141]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0483,  0.0385,  0.0470,  ..., -0.0551, -0.0331,  0.0366]],

        [[ 0.0483,  0.0385,  0.0470,  ..., -0.0551, -0.0331,  0.0366]],

        [[ 0.0483,  0.0385,  0.0470,  ..., -0.0551, -0.0331,  0.0366]],

        [[ 0.0483,  0.0385,  0.0470,  ..., -0.0551, -0.0331,  0.0366]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 15 0
layer name:  MLP
i,j,k 16 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2754,  0.2117,  0.0586,  ..., -0.2175, -0.4021,  0.2761]],

        [[ 0.2754,  0.2117,  0.0586,  ..., -0.2175, -0.4021,  0.2761]],

        [[ 0.2754,  0.2117,  0.0586,  ..., -0.2175, -0.4021,  0.2761]],

        [[ 0.2754,  0.2117,  0.0586,  ..., -0.2175, -0.4021,  0.2761]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0509,  0.0426,  0.0063,  ..., -0.0471, -0.0818,  0.0526]],

        [[ 0.0509,  0.0426,  0.0063,  ..., -0.0471, -0.0818,  0.0526]],

        [[ 0.0509,  0.0426,  0.0063,  ..., -0.0471, -0.0818,  0.0526]],

        [[ 0.0509,  0.0426,  0.0063,  ..., -0.0471, -0.0818,  0.0526]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 18 0
layer name:  MLP
i,j,k 16 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1837,  0.1023, -0.3047,  ..., -0.1475, -0.8057,  0.4114]],

        [[ 0.1837,  0.1023, -0.3047,  ..., -0.1475, -0.8057,  0.4114]],

        [[ 0.1837,  0.1023, -0.3047,  ..., -0.1475, -0.8057,  0.4114]],

        [[ 0.1837,  0.1023, -0.3047,  ..., -0.1475, -0.8057,  0.4114]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0394,  0.0232, -0.0730,  ..., -0.0375, -0.1830,  0.0972]],

        [[ 0.0394,  0.0232, -0.0730,  ..., -0.0375, -0.1830,  0.0972]],

        [[ 0.0394,  0.0232, -0.0730,  ..., -0.0375, -0.1830,  0.0972]],

        [[ 0.0394,  0.0232, -0.0730,  ..., -0.0375, -0.1830,  0.0972]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 21 0
layer name:  MLP
i,j,k 16 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0225, -0.3240, -0.4509,  ...,  0.2114, -0.9097,  0.3682]],

        [[-0.0225, -0.3240, -0.4509,  ...,  0.2114, -0.9097,  0.3682]],

        [[-0.0225, -0.3240, -0.4509,  ...,  0.2114, -0.9097,  0.3682]],

        [[-0.0225, -0.3240, -0.4509,  ...,  0.2114, -0.9097,  0.3682]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0060, -0.0825, -0.1179,  ...,  0.0514, -0.2279,  0.0938]],

        [[-0.0060, -0.0825, -0.1179,  ...,  0.0514, -0.2279,  0.0938]],

        [[-0.0060, -0.0825, -0.1179,  ...,  0.0514, -0.2279,  0.0938]],

        [[-0.0060, -0.0825, -0.1179,  ...,  0.0514, -0.2279,  0.0938]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 24 0
layer name:  MLP
i,j,k 16 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2402, -0.4597, -0.5981,  ..., -0.0026, -0.9087,  0.1288]],

        [[ 0.2402, -0.4597, -0.5981,  ..., -0.0026, -0.9087,  0.1288]],

        [[ 0.2402, -0.4597, -0.5981,  ..., -0.0026, -0.9087,  0.1288]],

        [[ 0.2402, -0.4597, -0.5981,  ..., -0.0026, -0.9087,  0.1288]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0741, -0.1443, -0.1777,  ..., -0.0045, -0.2632,  0.0365]],

        [[ 0.0741, -0.1443, -0.1777,  ..., -0.0045, -0.2632,  0.0365]],

        [[ 0.0741, -0.1443, -0.1777,  ..., -0.0045, -0.2632,  0.0365]],

        [[ 0.0741, -0.1443, -0.1777,  ..., -0.0045, -0.2632,  0.0365]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 27 0
layer name:  MLP
i,j,k 16 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2656,  0.3877, -0.5913,  ..., -0.3452, -0.7637,  0.1508]],

        [[ 0.2656,  0.3877, -0.5913,  ..., -0.3452, -0.7637,  0.1508]],

        [[ 0.2656,  0.3877, -0.5913,  ..., -0.3452, -0.7637,  0.1508]],

        [[ 0.2656,  0.3877, -0.5913,  ..., -0.3452, -0.7637,  0.1508]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1043,  0.1641, -0.2126,  ..., -0.1207, -0.2693,  0.0575]],

        [[ 0.1043,  0.1641, -0.2126,  ..., -0.1207, -0.2693,  0.0575]],

        [[ 0.1043,  0.1641, -0.2126,  ..., -0.1207, -0.2693,  0.0575]],

        [[ 0.1043,  0.1641, -0.2126,  ..., -0.1207, -0.2693,  0.0575]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 30 0
layer name:  MLP
i,j,k 16 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2455,  0.8164, -0.1862,  ..., -0.0402, -0.6455,  0.1192]],

        [[-0.2455,  0.8164, -0.1862,  ..., -0.0402, -0.6455,  0.1192]],

        [[-0.2455,  0.8164, -0.1862,  ..., -0.0402, -0.6455,  0.1192]],

        [[-0.2455,  0.8164, -0.1862,  ..., -0.0402, -0.6455,  0.1192]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1010,  0.6509, -0.0813,  ..., -0.0101, -0.3123,  0.0692]],

        [[-0.1010,  0.6509, -0.0813,  ..., -0.0101, -0.3123,  0.0692]],

        [[-0.1010,  0.6509, -0.0813,  ..., -0.0101, -0.3123,  0.0692]],

        [[-0.1010,  0.6509, -0.0813,  ..., -0.0101, -0.3123,  0.0692]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 33 0
layer name:  MLP
i,j,k 16 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0872,  1.7900,  0.1235,  ...,  0.0280, -0.5503,  0.1002]],

        [[-0.0872,  1.7900,  0.1235,  ...,  0.0280, -0.5503,  0.1002]],

        [[-0.0872,  1.7900,  0.1235,  ...,  0.0280, -0.5503,  0.1002]],

        [[-0.0872,  1.7900,  0.1235,  ...,  0.0280, -0.5503,  0.1002]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 16 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0281,  1.2422,  0.1116,  ...,  0.0466, -0.3672,  0.0890]],

        [[-0.0281,  1.2422,  0.1116,  ...,  0.0466, -0.3672,  0.0890]],

        [[-0.0281,  1.2422,  0.1116,  ...,  0.0466, -0.3672,  0.0890]],

        [[-0.0281,  1.2422,  0.1116,  ...,  0.0466, -0.3672,  0.0890]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 16 36 0
layer name:  MLP
i,j,k 16 37 0
layer name:  OutputEmbed
i,j,k 17 0 0
layer name:  InputEmbed
i,j,k 17 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2759,  0.3479, -0.1240,  ..., -0.2913, -0.1373,  0.2720]],

        [[-0.2759,  0.3479, -0.1240,  ..., -0.2913, -0.1373,  0.2720]],

        [[-0.2759,  0.3479, -0.1240,  ..., -0.2913, -0.1373,  0.2720]],

        [[-0.2759,  0.3479, -0.1240,  ..., -0.2913, -0.1373,  0.2720]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0809,  0.0279, -0.0552,  ..., -0.0883, -0.0606,  0.0609]],

        [[-0.0809,  0.0279, -0.0552,  ..., -0.0883, -0.0606,  0.0609]],

        [[-0.0809,  0.0279, -0.0552,  ..., -0.0883, -0.0606,  0.0609]],

        [[-0.0809,  0.0279, -0.0552,  ..., -0.0883, -0.0606,  0.0609]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 3 0
layer name:  MLP
i,j,k 17 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2079,  0.5391, -0.0638,  ..., -0.2068, -0.1387,  0.2394]],

        [[-0.2079,  0.5391, -0.0638,  ..., -0.2068, -0.1387,  0.2394]],

        [[-0.2079,  0.5391, -0.0638,  ..., -0.2068, -0.1387,  0.2394]],

        [[-0.2079,  0.5391, -0.0638,  ..., -0.2068, -0.1387,  0.2394]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0698,  0.0637, -0.0391,  ..., -0.0721, -0.0537,  0.0454]],

        [[-0.0698,  0.0637, -0.0391,  ..., -0.0721, -0.0537,  0.0454]],

        [[-0.0698,  0.0637, -0.0391,  ..., -0.0721, -0.0537,  0.0454]],

        [[-0.0698,  0.0637, -0.0391,  ..., -0.0721, -0.0537,  0.0454]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 6 0
layer name:  MLP
i,j,k 17 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1326,  0.6245, -0.0851,  ..., -0.3372, -0.2822,  0.3701]],

        [[-0.1326,  0.6245, -0.0851,  ..., -0.3372, -0.2822,  0.3701]],

        [[-0.1326,  0.6245, -0.0851,  ..., -0.3372, -0.2822,  0.3701]],

        [[-0.1326,  0.6245, -0.0851,  ..., -0.3372, -0.2822,  0.3701]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0484,  0.0849, -0.0385,  ..., -0.0806, -0.0691,  0.0459]],

        [[-0.0484,  0.0849, -0.0385,  ..., -0.0806, -0.0691,  0.0459]],

        [[-0.0484,  0.0849, -0.0385,  ..., -0.0806, -0.0691,  0.0459]],

        [[-0.0484,  0.0849, -0.0385,  ..., -0.0806, -0.0691,  0.0459]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 9 0
layer name:  MLP
i,j,k 17 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1500,  0.4934,  0.0320,  ..., -0.2493, -0.3313,  0.1527]],

        [[-0.1500,  0.4934,  0.0320,  ..., -0.2493, -0.3313,  0.1527]],

        [[-0.1500,  0.4934,  0.0320,  ..., -0.2493, -0.3313,  0.1527]],

        [[-0.1500,  0.4934,  0.0320,  ..., -0.2493, -0.3313,  0.1527]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0536,  0.0754, -0.0184,  ..., -0.0719, -0.0834,  0.0081]],

        [[-0.0536,  0.0754, -0.0184,  ..., -0.0719, -0.0834,  0.0081]],

        [[-0.0536,  0.0754, -0.0184,  ..., -0.0719, -0.0834,  0.0081]],

        [[-0.0536,  0.0754, -0.0184,  ..., -0.0719, -0.0834,  0.0081]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 12 0
layer name:  MLP
i,j,k 17 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2070,  0.5742,  0.0488,  ..., -0.3342, -0.4260,  0.0421]],

        [[-0.2070,  0.5742,  0.0488,  ..., -0.3342, -0.4260,  0.0421]],

        [[-0.2070,  0.5742,  0.0488,  ..., -0.3342, -0.4260,  0.0421]],

        [[-0.2070,  0.5742,  0.0488,  ..., -0.3342, -0.4260,  0.0421]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0599,  0.0761, -0.0155,  ..., -0.0808, -0.0961, -0.0150]],

        [[-0.0599,  0.0761, -0.0155,  ..., -0.0808, -0.0961, -0.0150]],

        [[-0.0599,  0.0761, -0.0155,  ..., -0.0808, -0.0961, -0.0150]],

        [[-0.0599,  0.0761, -0.0155,  ..., -0.0808, -0.0961, -0.0150]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 15 0
layer name:  MLP
i,j,k 17 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2268,  0.4639,  0.1108,  ..., -0.5698, -0.3665,  0.0030]],

        [[-0.2268,  0.4639,  0.1108,  ..., -0.5698, -0.3665,  0.0030]],

        [[-0.2268,  0.4639,  0.1108,  ..., -0.5698, -0.3665,  0.0030]],

        [[-0.2268,  0.4639,  0.1108,  ..., -0.5698, -0.3665,  0.0030]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0696,  0.0734, -0.0034,  ..., -0.1294, -0.0911, -0.0228]],

        [[-0.0696,  0.0734, -0.0034,  ..., -0.1294, -0.0911, -0.0228]],

        [[-0.0696,  0.0734, -0.0034,  ..., -0.1294, -0.0911, -0.0228]],

        [[-0.0696,  0.0734, -0.0034,  ..., -0.1294, -0.0911, -0.0228]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 18 0
layer name:  MLP
i,j,k 17 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1166,  0.4307,  0.0172,  ..., -0.4846, -0.6152,  0.0319]],

        [[-0.1166,  0.4307,  0.0172,  ..., -0.4846, -0.6152,  0.0319]],

        [[-0.1166,  0.4307,  0.0172,  ..., -0.4846, -0.6152,  0.0319]],

        [[-0.1166,  0.4307,  0.0172,  ..., -0.4846, -0.6152,  0.0319]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0483,  0.0765, -0.0199,  ..., -0.1223, -0.1487, -0.0143]],

        [[-0.0483,  0.0765, -0.0199,  ..., -0.1223, -0.1487, -0.0143]],

        [[-0.0483,  0.0765, -0.0199,  ..., -0.1223, -0.1487, -0.0143]],

        [[-0.0483,  0.0765, -0.0199,  ..., -0.1223, -0.1487, -0.0143]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 21 0
layer name:  MLP
i,j,k 17 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3254,  0.4548, -0.0095,  ..., -0.3931, -0.7812,  0.0145]],

        [[-0.3254,  0.4548, -0.0095,  ..., -0.3931, -0.7812,  0.0145]],

        [[-0.3254,  0.4548, -0.0095,  ..., -0.3931, -0.7812,  0.0145]],

        [[-0.3254,  0.4548, -0.0095,  ..., -0.3931, -0.7812,  0.0145]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0983,  0.0969, -0.0225,  ..., -0.1155, -0.2009, -0.0178]],

        [[-0.0983,  0.0969, -0.0225,  ..., -0.1155, -0.2009, -0.0178]],

        [[-0.0983,  0.0969, -0.0225,  ..., -0.1155, -0.2009, -0.0178]],

        [[-0.0983,  0.0969, -0.0225,  ..., -0.1155, -0.2009, -0.0178]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 24 0
layer name:  MLP
i,j,k 17 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0912,  0.0605, -0.1548,  ..., -0.6665, -0.6147, -0.1136]],

        [[-0.0912,  0.0605, -0.1548,  ..., -0.6665, -0.6147, -0.1136]],

        [[-0.0912,  0.0605, -0.1548,  ..., -0.6665, -0.6147, -0.1136]],

        [[-0.0912,  0.0605, -0.1548,  ..., -0.6665, -0.6147, -0.1136]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0450,  0.0075, -0.0653,  ..., -0.2100, -0.1951, -0.0548]],

        [[-0.0450,  0.0075, -0.0653,  ..., -0.2100, -0.1951, -0.0548]],

        [[-0.0450,  0.0075, -0.0653,  ..., -0.2100, -0.1951, -0.0548]],

        [[-0.0450,  0.0075, -0.0653,  ..., -0.2100, -0.1951, -0.0548]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 27 0
layer name:  MLP
i,j,k 17 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1356,  0.5840, -0.1263,  ..., -0.8433, -0.7290, -0.2106]],

        [[ 0.1356,  0.5840, -0.1263,  ..., -0.8433, -0.7290, -0.2106]],

        [[ 0.1356,  0.5840, -0.1263,  ..., -0.8433, -0.7290, -0.2106]],

        [[ 0.1356,  0.5840, -0.1263,  ..., -0.8433, -0.7290, -0.2106]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0358,  0.2126, -0.0649,  ..., -0.3232, -0.2808, -0.0931]],

        [[ 0.0358,  0.2126, -0.0649,  ..., -0.3232, -0.2808, -0.0931]],

        [[ 0.0358,  0.2126, -0.0649,  ..., -0.3232, -0.2808, -0.0931]],

        [[ 0.0358,  0.2126, -0.0649,  ..., -0.3232, -0.2808, -0.0931]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 30 0
layer name:  MLP
i,j,k 17 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1108,  0.8594, -0.0181,  ..., -0.6284, -0.5830, -0.0976]],

        [[ 0.1108,  0.8594, -0.0181,  ..., -0.6284, -0.5830, -0.0976]],

        [[ 0.1108,  0.8594, -0.0181,  ..., -0.6284, -0.5830, -0.0976]],

        [[ 0.1108,  0.8594, -0.0181,  ..., -0.6284, -0.5830, -0.0976]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0447,  0.6484, -0.0232,  ..., -0.3230, -0.3042, -0.0585]],

        [[ 0.0447,  0.6484, -0.0232,  ..., -0.3230, -0.3042, -0.0585]],

        [[ 0.0447,  0.6484, -0.0232,  ..., -0.3230, -0.3042, -0.0585]],

        [[ 0.0447,  0.6484, -0.0232,  ..., -0.3230, -0.3042, -0.0585]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 33 0
layer name:  MLP
i,j,k 17 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3223,  1.6475,  0.4358,  ..., -0.8198, -0.4980, -0.1108]],

        [[ 0.3223,  1.6475,  0.4358,  ..., -0.8198, -0.4980, -0.1108]],

        [[ 0.3223,  1.6475,  0.4358,  ..., -0.8198, -0.4980, -0.1108]],

        [[ 0.3223,  1.6475,  0.4358,  ..., -0.8198, -0.4980, -0.1108]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 17 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1815,  1.0088,  0.2620,  ..., -0.4736, -0.3267, -0.0634]],

        [[ 0.1815,  1.0088,  0.2620,  ..., -0.4736, -0.3267, -0.0634]],

        [[ 0.1815,  1.0088,  0.2620,  ..., -0.4736, -0.3267, -0.0634]],

        [[ 0.1815,  1.0088,  0.2620,  ..., -0.4736, -0.3267, -0.0634]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 17 36 0
layer name:  MLP
i,j,k 17 37 0
layer name:  OutputEmbed
i,j,k 18 0 0
layer name:  InputEmbed
i,j,k 18 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0803,  0.0418, -0.0313,  ...,  0.2749,  0.0134, -0.3147]],

        [[-0.0803,  0.0418, -0.0313,  ...,  0.2749,  0.0134, -0.3147]],

        [[-0.0803,  0.0418, -0.0313,  ...,  0.2749,  0.0134, -0.3147]],

        [[-0.0803,  0.0418, -0.0313,  ...,  0.2749,  0.0134, -0.3147]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0046,  0.0034,  0.0031,  ...,  0.0712,  0.0079, -0.0676]],

        [[-0.0046,  0.0034,  0.0031,  ...,  0.0712,  0.0079, -0.0676]],

        [[-0.0046,  0.0034,  0.0031,  ...,  0.0712,  0.0079, -0.0676]],

        [[-0.0046,  0.0034,  0.0031,  ...,  0.0712,  0.0079, -0.0676]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 3 0
layer name:  MLP
i,j,k 18 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0813, -0.0588, -0.1434,  ...,  0.2649,  0.0409, -0.2439]],

        [[-0.0813, -0.0588, -0.1434,  ...,  0.2649,  0.0409, -0.2439]],

        [[-0.0813, -0.0588, -0.1434,  ...,  0.2649,  0.0409, -0.2439]],

        [[-0.0813, -0.0588, -0.1434,  ...,  0.2649,  0.0409, -0.2439]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0065,  0.0054, -0.0209,  ...,  0.0728,  0.0160, -0.0406]],

        [[-0.0065,  0.0054, -0.0209,  ...,  0.0728,  0.0160, -0.0406]],

        [[-0.0065,  0.0054, -0.0209,  ...,  0.0728,  0.0160, -0.0406]],

        [[-0.0065,  0.0054, -0.0209,  ...,  0.0728,  0.0160, -0.0406]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 6 0
layer name:  MLP
i,j,k 18 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0105,  0.1492, -0.3152,  ...,  0.1779,  0.0945, -0.2585]],

        [[-0.0105,  0.1492, -0.3152,  ...,  0.1779,  0.0945, -0.2585]],

        [[-0.0105,  0.1492, -0.3152,  ...,  0.1779,  0.0945, -0.2585]],

        [[-0.0105,  0.1492, -0.3152,  ...,  0.1779,  0.0945, -0.2585]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0107,  0.0424, -0.0454,  ...,  0.0448,  0.0272, -0.0313]],

        [[ 0.0107,  0.0424, -0.0454,  ...,  0.0448,  0.0272, -0.0313]],

        [[ 0.0107,  0.0424, -0.0454,  ...,  0.0448,  0.0272, -0.0313]],

        [[ 0.0107,  0.0424, -0.0454,  ...,  0.0448,  0.0272, -0.0313]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 9 0
layer name:  MLP
i,j,k 18 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1183,  0.2211, -0.2397,  ...,  0.2020,  0.1344, -0.2986]],

        [[-0.1183,  0.2211, -0.2397,  ...,  0.2020,  0.1344, -0.2986]],

        [[-0.1183,  0.2211, -0.2397,  ...,  0.2020,  0.1344, -0.2986]],

        [[-0.1183,  0.2211, -0.2397,  ...,  0.2020,  0.1344, -0.2986]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0113,  0.0640, -0.0333,  ...,  0.0550,  0.0393, -0.0470]],

        [[-0.0113,  0.0640, -0.0333,  ...,  0.0550,  0.0393, -0.0470]],

        [[-0.0113,  0.0640, -0.0333,  ...,  0.0550,  0.0393, -0.0470]],

        [[-0.0113,  0.0640, -0.0333,  ...,  0.0550,  0.0393, -0.0470]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 12 0
layer name:  MLP
i,j,k 18 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1691,  0.1276, -0.3374,  ...,  0.1722,  0.1818, -0.3147]],

        [[-0.1691,  0.1276, -0.3374,  ...,  0.1722,  0.1818, -0.3147]],

        [[-0.1691,  0.1276, -0.3374,  ...,  0.1722,  0.1818, -0.3147]],

        [[-0.1691,  0.1276, -0.3374,  ...,  0.1722,  0.1818, -0.3147]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0178,  0.0399, -0.0479,  ...,  0.0467,  0.0486, -0.0474]],

        [[-0.0178,  0.0399, -0.0479,  ...,  0.0467,  0.0486, -0.0474]],

        [[-0.0178,  0.0399, -0.0479,  ...,  0.0467,  0.0486, -0.0474]],

        [[-0.0178,  0.0399, -0.0479,  ...,  0.0467,  0.0486, -0.0474]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 15 0
layer name:  MLP
i,j,k 18 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1477,  0.0980, -0.3628,  ...,  0.0449,  0.1814, -0.2196]],

        [[-0.1477,  0.0980, -0.3628,  ...,  0.0449,  0.1814, -0.2196]],

        [[-0.1477,  0.0980, -0.3628,  ...,  0.0449,  0.1814, -0.2196]],

        [[-0.1477,  0.0980, -0.3628,  ...,  0.0449,  0.1814, -0.2196]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0182,  0.0394, -0.0591,  ...,  0.0237,  0.0522, -0.0324]],

        [[-0.0182,  0.0394, -0.0591,  ...,  0.0237,  0.0522, -0.0324]],

        [[-0.0182,  0.0394, -0.0591,  ...,  0.0237,  0.0522, -0.0324]],

        [[-0.0182,  0.0394, -0.0591,  ...,  0.0237,  0.0522, -0.0324]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 18 0
layer name:  MLP
i,j,k 18 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1620,  0.1912, -0.5664,  ..., -0.1659,  0.0524, -0.0952]],

        [[-0.1620,  0.1912, -0.5664,  ..., -0.1659,  0.0524, -0.0952]],

        [[-0.1620,  0.1912, -0.5664,  ..., -0.1659,  0.0524, -0.0952]],

        [[-0.1620,  0.1912, -0.5664,  ..., -0.1659,  0.0524, -0.0952]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0231,  0.0660, -0.1138,  ..., -0.0221,  0.0276, -0.0059]],

        [[-0.0231,  0.0660, -0.1138,  ..., -0.0221,  0.0276, -0.0059]],

        [[-0.0231,  0.0660, -0.1138,  ..., -0.0221,  0.0276, -0.0059]],

        [[-0.0231,  0.0660, -0.1138,  ..., -0.0221,  0.0276, -0.0059]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 21 0
layer name:  MLP
i,j,k 18 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0741,  0.4819, -0.5859,  ..., -0.1925, -0.2030,  0.0285]],

        [[-0.0741,  0.4819, -0.5859,  ..., -0.1925, -0.2030,  0.0285]],

        [[-0.0741,  0.4819, -0.5859,  ..., -0.1925, -0.2030,  0.0285]],

        [[-0.0741,  0.4819, -0.5859,  ..., -0.1925, -0.2030,  0.0285]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-2.6703e-05,  1.4966e-01, -1.3098e-01,  ..., -3.2135e-02,
          -3.1952e-02,  2.4567e-02]],

        [[-2.6703e-05,  1.4966e-01, -1.3098e-01,  ..., -3.2135e-02,
          -3.1952e-02,  2.4567e-02]],

        [[-2.6703e-05,  1.4966e-01, -1.3098e-01,  ..., -3.2135e-02,
          -3.1952e-02,  2.4567e-02]],

        [[-2.6703e-05,  1.4966e-01, -1.3098e-01,  ..., -3.2135e-02,
          -3.1952e-02,  2.4567e-02]]], device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 24 0
layer name:  MLP
i,j,k 18 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0110, -0.1555, -0.5244,  ..., -0.3264, -0.5356, -0.0252]],

        [[ 0.0110, -0.1555, -0.5244,  ..., -0.3264, -0.5356, -0.0252]],

        [[ 0.0110, -0.1555, -0.5244,  ..., -0.3264, -0.5356, -0.0252]],

        [[ 0.0110, -0.1555, -0.5244,  ..., -0.3264, -0.5356, -0.0252]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0240, -0.0257, -0.1399,  ..., -0.0795, -0.1390,  0.0093]],

        [[ 0.0240, -0.0257, -0.1399,  ..., -0.0795, -0.1390,  0.0093]],

        [[ 0.0240, -0.0257, -0.1399,  ..., -0.0795, -0.1390,  0.0093]],

        [[ 0.0240, -0.0257, -0.1399,  ..., -0.0795, -0.1390,  0.0093]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 27 0
layer name:  MLP
i,j,k 18 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1766, -0.0180, -0.4172,  ..., -0.4902, -0.6748, -0.1891]],

        [[ 0.1766, -0.0180, -0.4172,  ..., -0.4902, -0.6748, -0.1891]],

        [[ 0.1766, -0.0180, -0.4172,  ..., -0.4902, -0.6748, -0.1891]],

        [[ 0.1766, -0.0180, -0.4172,  ..., -0.4902, -0.6748, -0.1891]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0895,  0.0384, -0.1353,  ..., -0.1592, -0.2250, -0.0479]],

        [[ 0.0895,  0.0384, -0.1353,  ..., -0.1592, -0.2250, -0.0479]],

        [[ 0.0895,  0.0384, -0.1353,  ..., -0.1592, -0.2250, -0.0479]],

        [[ 0.0895,  0.0384, -0.1353,  ..., -0.1592, -0.2250, -0.0479]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 30 0
layer name:  MLP
i,j,k 18 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0054,  0.6890, -0.1355,  ..., -0.1978, -0.6797, -0.2162]],

        [[-0.0054,  0.6890, -0.1355,  ..., -0.1978, -0.6797, -0.2162]],

        [[-0.0054,  0.6890, -0.1355,  ..., -0.1978, -0.6797, -0.2162]],

        [[-0.0054,  0.6890, -0.1355,  ..., -0.1978, -0.6797, -0.2162]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0304,  0.5854, -0.0423,  ..., -0.0748, -0.3213, -0.0756]],

        [[ 0.0304,  0.5854, -0.0423,  ..., -0.0748, -0.3213, -0.0756]],

        [[ 0.0304,  0.5854, -0.0423,  ..., -0.0748, -0.3213, -0.0756]],

        [[ 0.0304,  0.5854, -0.0423,  ..., -0.0748, -0.3213, -0.0756]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 33 0
layer name:  MLP
i,j,k 18 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1772,  1.2725,  0.0144,  ..., -0.5073, -0.4851, -0.2563]],

        [[ 0.1772,  1.2725,  0.0144,  ..., -0.5073, -0.4851, -0.2563]],

        [[ 0.1772,  1.2725,  0.0144,  ..., -0.5073, -0.4851, -0.2563]],

        [[ 0.1772,  1.2725,  0.0144,  ..., -0.5073, -0.4851, -0.2563]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 18 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1403,  0.8267,  0.0489,  ..., -0.2480, -0.2725, -0.0984]],

        [[ 0.1403,  0.8267,  0.0489,  ..., -0.2480, -0.2725, -0.0984]],

        [[ 0.1403,  0.8267,  0.0489,  ..., -0.2480, -0.2725, -0.0984]],

        [[ 0.1403,  0.8267,  0.0489,  ..., -0.2480, -0.2725, -0.0984]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 18 36 0
layer name:  MLP
i,j,k 18 37 0
layer name:  OutputEmbed
i,j,k 19 0 0
layer name:  InputEmbed
i,j,k 19 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0930,  0.7388,  0.0146,  ..., -0.1471,  0.2949, -0.3660]],

        [[-0.0930,  0.7388,  0.0146,  ..., -0.1471,  0.2949, -0.3660]],

        [[-0.0930,  0.7388,  0.0146,  ..., -0.1471,  0.2949, -0.3660]],

        [[-0.0930,  0.7388,  0.0146,  ..., -0.1471,  0.2949, -0.3660]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0347,  0.1144, -0.0150,  ..., -0.0466,  0.0355, -0.1077]],

        [[-0.0347,  0.1144, -0.0150,  ..., -0.0466,  0.0355, -0.1077]],

        [[-0.0347,  0.1144, -0.0150,  ..., -0.0466,  0.0355, -0.1077]],

        [[-0.0347,  0.1144, -0.0150,  ..., -0.0466,  0.0355, -0.1077]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 3 0
layer name:  MLP
i,j,k 19 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2300,  0.7705, -0.0716,  ...,  0.0775,  0.4043, -0.3066]],

        [[-0.2300,  0.7705, -0.0716,  ...,  0.0775,  0.4043, -0.3066]],

        [[-0.2300,  0.7705, -0.0716,  ...,  0.0775,  0.4043, -0.3066]],

        [[-0.2300,  0.7705, -0.0716,  ...,  0.0775,  0.4043, -0.3066]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0663,  0.1165, -0.0311,  ...,  0.0007,  0.0547, -0.0829]],

        [[-0.0663,  0.1165, -0.0311,  ...,  0.0007,  0.0547, -0.0829]],

        [[-0.0663,  0.1165, -0.0311,  ...,  0.0007,  0.0547, -0.0829]],

        [[-0.0663,  0.1165, -0.0311,  ...,  0.0007,  0.0547, -0.0829]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 6 0
layer name:  MLP
i,j,k 19 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2563,  0.7051, -0.1400,  ...,  0.1439,  0.3760, -0.3367]],

        [[-0.2563,  0.7051, -0.1400,  ...,  0.1439,  0.3760, -0.3367]],

        [[-0.2563,  0.7051, -0.1400,  ...,  0.1439,  0.3760, -0.3367]],

        [[-0.2563,  0.7051, -0.1400,  ...,  0.1439,  0.3760, -0.3367]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0630,  0.1198, -0.0391,  ...,  0.0113,  0.0462, -0.0731]],

        [[-0.0630,  0.1198, -0.0391,  ...,  0.0113,  0.0462, -0.0731]],

        [[-0.0630,  0.1198, -0.0391,  ...,  0.0113,  0.0462, -0.0731]],

        [[-0.0630,  0.1198, -0.0391,  ...,  0.0113,  0.0462, -0.0731]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 9 0
layer name:  MLP
i,j,k 19 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4021,  0.5605, -0.0598,  ...,  0.2673,  0.2729, -0.4192]],

        [[-0.4021,  0.5605, -0.0598,  ...,  0.2673,  0.2729, -0.4192]],

        [[-0.4021,  0.5605, -0.0598,  ...,  0.2673,  0.2729, -0.4192]],

        [[-0.4021,  0.5605, -0.0598,  ...,  0.2673,  0.2729, -0.4192]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0983,  0.1104, -0.0251,  ...,  0.0411,  0.0386, -0.1006]],

        [[-0.0983,  0.1104, -0.0251,  ...,  0.0411,  0.0386, -0.1006]],

        [[-0.0983,  0.1104, -0.0251,  ...,  0.0411,  0.0386, -0.1006]],

        [[-0.0983,  0.1104, -0.0251,  ...,  0.0411,  0.0386, -0.1006]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 12 0
layer name:  MLP
i,j,k 19 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5410,  0.7456, -0.1851,  ...,  0.3953,  0.2803, -0.4370]],

        [[-0.5410,  0.7456, -0.1851,  ...,  0.3953,  0.2803, -0.4370]],

        [[-0.5410,  0.7456, -0.1851,  ...,  0.3953,  0.2803, -0.4370]],

        [[-0.5410,  0.7456, -0.1851,  ...,  0.3953,  0.2803, -0.4370]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1115,  0.1257, -0.0460,  ...,  0.0582,  0.0376, -0.0968]],

        [[-0.1115,  0.1257, -0.0460,  ...,  0.0582,  0.0376, -0.0968]],

        [[-0.1115,  0.1257, -0.0460,  ...,  0.0582,  0.0376, -0.0968]],

        [[-0.1115,  0.1257, -0.0460,  ...,  0.0582,  0.0376, -0.0968]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 15 0
layer name:  MLP
i,j,k 19 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6343,  0.5308, -0.2656,  ...,  0.1827,  0.2084, -0.4500]],

        [[-0.6343,  0.5308, -0.2656,  ...,  0.1827,  0.2084, -0.4500]],

        [[-0.6343,  0.5308, -0.2656,  ...,  0.1827,  0.2084, -0.4500]],

        [[-0.6343,  0.5308, -0.2656,  ...,  0.1827,  0.2084, -0.4500]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1493,  0.1081, -0.0657,  ...,  0.0241,  0.0301, -0.1085]],

        [[-0.1493,  0.1081, -0.0657,  ...,  0.0241,  0.0301, -0.1085]],

        [[-0.1493,  0.1081, -0.0657,  ...,  0.0241,  0.0301, -0.1085]],

        [[-0.1493,  0.1081, -0.0657,  ...,  0.0241,  0.0301, -0.1085]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 18 0
layer name:  MLP
i,j,k 19 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6294,  0.5215, -0.2328,  ...,  0.1014, -0.0249, -0.3962]],

        [[-0.6294,  0.5215, -0.2328,  ...,  0.1014, -0.0249, -0.3962]],

        [[-0.6294,  0.5215, -0.2328,  ...,  0.1014, -0.0249, -0.3962]],

        [[-0.6294,  0.5215, -0.2328,  ...,  0.1014, -0.0249, -0.3962]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1609,  0.1194, -0.0641,  ...,  0.0107, -0.0169, -0.1060]],

        [[-0.1609,  0.1194, -0.0641,  ...,  0.0107, -0.0169, -0.1060]],

        [[-0.1609,  0.1194, -0.0641,  ...,  0.0107, -0.0169, -0.1060]],

        [[-0.1609,  0.1194, -0.0641,  ...,  0.0107, -0.0169, -0.1060]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 21 0
layer name:  MLP
i,j,k 19 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5552,  0.5308, -0.3252,  ..., -0.0400, -0.3091, -0.3879]],

        [[-0.5552,  0.5308, -0.3252,  ..., -0.0400, -0.3091, -0.3879]],

        [[-0.5552,  0.5308, -0.3252,  ..., -0.0400, -0.3091, -0.3879]],

        [[-0.5552,  0.5308, -0.3252,  ..., -0.0400, -0.3091, -0.3879]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1451,  0.1313, -0.0884,  ..., -0.0199, -0.0819, -0.1047]],

        [[-0.1451,  0.1313, -0.0884,  ..., -0.0199, -0.0819, -0.1047]],

        [[-0.1451,  0.1313, -0.0884,  ..., -0.0199, -0.0819, -0.1047]],

        [[-0.1451,  0.1313, -0.0884,  ..., -0.0199, -0.0819, -0.1047]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 24 0
layer name:  MLP
i,j,k 19 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4045, -0.0591, -0.2430,  ..., -0.3088, -0.6377, -0.3848]],

        [[-0.4045, -0.0591, -0.2430,  ..., -0.3088, -0.6377, -0.3848]],

        [[-0.4045, -0.0591, -0.2430,  ..., -0.3088, -0.6377, -0.3848]],

        [[-0.4045, -0.0591, -0.2430,  ..., -0.3088, -0.6377, -0.3848]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1223, -0.0181, -0.0765,  ..., -0.0950, -0.1847, -0.1187]],

        [[-0.1223, -0.0181, -0.0765,  ..., -0.0950, -0.1847, -0.1187]],

        [[-0.1223, -0.0181, -0.0765,  ..., -0.0950, -0.1847, -0.1187]],

        [[-0.1223, -0.0181, -0.0765,  ..., -0.0950, -0.1847, -0.1187]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 27 0
layer name:  MLP
i,j,k 19 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4358, -0.1960, -0.0478,  ..., -0.6406, -0.6680, -0.3230]],

        [[-0.4358, -0.1960, -0.0478,  ..., -0.6406, -0.6680, -0.3230]],

        [[-0.4358, -0.1960, -0.0478,  ..., -0.6406, -0.6680, -0.3230]],

        [[-0.4358, -0.1960, -0.0478,  ..., -0.6406, -0.6680, -0.3230]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1536, -0.0476, -0.0217,  ..., -0.2288, -0.2368, -0.1158]],

        [[-0.1536, -0.0476, -0.0217,  ..., -0.2288, -0.2368, -0.1158]],

        [[-0.1536, -0.0476, -0.0217,  ..., -0.2288, -0.2368, -0.1158]],

        [[-0.1536, -0.0476, -0.0217,  ..., -0.2288, -0.2368, -0.1158]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 30 0
layer name:  MLP
i,j,k 19 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.7261,  0.5933, -0.1339,  ..., -0.4971, -0.5576, -0.2319]],

        [[-0.7261,  0.5933, -0.1339,  ..., -0.4971, -0.5576, -0.2319]],

        [[-0.7261,  0.5933, -0.1339,  ..., -0.4971, -0.5576, -0.2319]],

        [[-0.7261,  0.5933, -0.1339,  ..., -0.4971, -0.5576, -0.2319]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3296,  0.4614, -0.0622,  ..., -0.2351, -0.2671, -0.1014]],

        [[-0.3296,  0.4614, -0.0622,  ..., -0.2351, -0.2671, -0.1014]],

        [[-0.3296,  0.4614, -0.0622,  ..., -0.2351, -0.2671, -0.1014]],

        [[-0.3296,  0.4614, -0.0622,  ..., -0.2351, -0.2671, -0.1014]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 33 0
layer name:  MLP
i,j,k 19 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6089,  0.9248,  0.0202,  ..., -0.6792, -0.3655, -0.1558]],

        [[-0.6089,  0.9248,  0.0202,  ..., -0.6792, -0.3655, -0.1558]],

        [[-0.6089,  0.9248,  0.0202,  ..., -0.6792, -0.3655, -0.1558]],

        [[-0.6089,  0.9248,  0.0202,  ..., -0.6792, -0.3655, -0.1558]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 19 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3315,  0.6060,  0.0293,  ..., -0.3706, -0.2224, -0.0676]],

        [[-0.3315,  0.6060,  0.0293,  ..., -0.3706, -0.2224, -0.0676]],

        [[-0.3315,  0.6060,  0.0293,  ..., -0.3706, -0.2224, -0.0676]],

        [[-0.3315,  0.6060,  0.0293,  ..., -0.3706, -0.2224, -0.0676]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 19 36 0
layer name:  MLP
i,j,k 19 37 0
layer name:  OutputEmbed
i,j,k 20 0 0
layer name:  InputEmbed
i,j,k 20 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0983,  0.4719,  0.1455,  ..., -0.5737,  0.0559, -0.3784]],

        [[-0.0983,  0.4719,  0.1455,  ..., -0.5737,  0.0559, -0.3784]],

        [[-0.0983,  0.4719,  0.1455,  ..., -0.5737,  0.0559, -0.3784]],

        [[-0.0983,  0.4719,  0.1455,  ..., -0.5737,  0.0559, -0.3784]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0187,  0.0883,  0.0349,  ..., -0.1292,  0.0078, -0.1020]],

        [[-0.0187,  0.0883,  0.0349,  ..., -0.1292,  0.0078, -0.1020]],

        [[-0.0187,  0.0883,  0.0349,  ..., -0.1292,  0.0078, -0.1020]],

        [[-0.0187,  0.0883,  0.0349,  ..., -0.1292,  0.0078, -0.1020]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 3 0
layer name:  MLP
i,j,k 20 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0804,  0.6348,  0.1038,  ..., -0.3025,  0.0336, -0.5117]],

        [[-0.0804,  0.6348,  0.1038,  ..., -0.3025,  0.0336, -0.5117]],

        [[-0.0804,  0.6348,  0.1038,  ..., -0.3025,  0.0336, -0.5117]],

        [[-0.0804,  0.6348,  0.1038,  ..., -0.3025,  0.0336, -0.5117]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0163,  0.1223,  0.0299,  ..., -0.0732,  0.0058, -0.1283]],

        [[-0.0163,  0.1223,  0.0299,  ..., -0.0732,  0.0058, -0.1283]],

        [[-0.0163,  0.1223,  0.0299,  ..., -0.0732,  0.0058, -0.1283]],

        [[-0.0163,  0.1223,  0.0299,  ..., -0.0732,  0.0058, -0.1283]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 6 0
layer name:  MLP
i,j,k 20 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1658,  0.6494,  0.0284,  ..., -0.3059,  0.0204, -0.7217]],

        [[ 0.1658,  0.6494,  0.0284,  ..., -0.3059,  0.0204, -0.7217]],

        [[ 0.1658,  0.6494,  0.0284,  ..., -0.3059,  0.0204, -0.7217]],

        [[ 0.1658,  0.6494,  0.0284,  ..., -0.3059,  0.0204, -0.7217]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0388,  0.1409,  0.0119,  ..., -0.0559,  0.0066, -0.1422]],

        [[ 0.0388,  0.1409,  0.0119,  ..., -0.0559,  0.0066, -0.1422]],

        [[ 0.0388,  0.1409,  0.0119,  ..., -0.0559,  0.0066, -0.1422]],

        [[ 0.0388,  0.1409,  0.0119,  ..., -0.0559,  0.0066, -0.1422]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 9 0
layer name:  MLP
i,j,k 20 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2084,  0.3948, -0.0199,  ..., -0.0612, -0.0145, -0.5928]],

        [[ 0.2084,  0.3948, -0.0199,  ..., -0.0612, -0.0145, -0.5928]],

        [[ 0.2084,  0.3948, -0.0199,  ..., -0.0612, -0.0145, -0.5928]],

        [[ 0.2084,  0.3948, -0.0199,  ..., -0.0612, -0.0145, -0.5928]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0551,  0.1072,  0.0011,  ..., -0.0099,  0.0014, -0.1387]],

        [[ 0.0551,  0.1072,  0.0011,  ..., -0.0099,  0.0014, -0.1387]],

        [[ 0.0551,  0.1072,  0.0011,  ..., -0.0099,  0.0014, -0.1387]],

        [[ 0.0551,  0.1072,  0.0011,  ..., -0.0099,  0.0014, -0.1387]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 12 0
layer name:  MLP
i,j,k 20 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 2.0178e-01,  3.6304e-01, -3.0494e-04,  ...,  2.9492e-01,
           2.8259e-02, -5.7959e-01]],

        [[ 2.0178e-01,  3.6304e-01, -3.0494e-04,  ...,  2.9492e-01,
           2.8259e-02, -5.7959e-01]],

        [[ 2.0178e-01,  3.6304e-01, -3.0494e-04,  ...,  2.9492e-01,
           2.8259e-02, -5.7959e-01]],

        [[ 2.0178e-01,  3.6304e-01, -3.0494e-04,  ...,  2.9492e-01,
           2.8259e-02, -5.7959e-01]]], device='cuda:0', dtype=torch.float16)
i,j,k 20 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0486,  0.0873,  0.0065,  ...,  0.0695,  0.0122, -0.1273]],

        [[ 0.0486,  0.0873,  0.0065,  ...,  0.0695,  0.0122, -0.1273]],

        [[ 0.0486,  0.0873,  0.0065,  ...,  0.0695,  0.0122, -0.1273]],

        [[ 0.0486,  0.0873,  0.0065,  ...,  0.0695,  0.0122, -0.1273]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 15 0
layer name:  MLP
i,j,k 20 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0968,  0.5225,  0.0890,  ...,  0.3474,  0.0687, -0.4509]],

        [[ 0.0968,  0.5225,  0.0890,  ...,  0.3474,  0.0687, -0.4509]],

        [[ 0.0968,  0.5225,  0.0890,  ...,  0.3474,  0.0687, -0.4509]],

        [[ 0.0968,  0.5225,  0.0890,  ...,  0.3474,  0.0687, -0.4509]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0301,  0.1466,  0.0277,  ...,  0.0887,  0.0237, -0.1071]],

        [[ 0.0301,  0.1466,  0.0277,  ...,  0.0887,  0.0237, -0.1071]],

        [[ 0.0301,  0.1466,  0.0277,  ...,  0.0887,  0.0237, -0.1071]],

        [[ 0.0301,  0.1466,  0.0277,  ...,  0.0887,  0.0237, -0.1071]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 18 0
layer name:  MLP
i,j,k 20 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0078,  0.5264,  0.0901,  ...,  0.4775, -0.0215, -0.1175]],

        [[-0.0078,  0.5264,  0.0901,  ...,  0.4775, -0.0215, -0.1175]],

        [[-0.0078,  0.5264,  0.0901,  ...,  0.4775, -0.0215, -0.1175]],

        [[-0.0078,  0.5264,  0.0901,  ...,  0.4775, -0.0215, -0.1175]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0054,  0.1637,  0.0309,  ...,  0.1324,  0.0017, -0.0244]],

        [[ 0.0054,  0.1637,  0.0309,  ...,  0.1324,  0.0017, -0.0244]],

        [[ 0.0054,  0.1637,  0.0309,  ...,  0.1324,  0.0017, -0.0244]],

        [[ 0.0054,  0.1637,  0.0309,  ...,  0.1324,  0.0017, -0.0244]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 21 0
layer name:  MLP
i,j,k 20 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1250,  0.6523,  0.0309,  ...,  0.4333,  0.0502, -0.2925]],

        [[-0.1250,  0.6523,  0.0309,  ...,  0.4333,  0.0502, -0.2925]],

        [[-0.1250,  0.6523,  0.0309,  ...,  0.4333,  0.0502, -0.2925]],

        [[-0.1250,  0.6523,  0.0309,  ...,  0.4333,  0.0502, -0.2925]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0240,  0.2050,  0.0193,  ...,  0.1285,  0.0233, -0.0726]],

        [[-0.0240,  0.2050,  0.0193,  ...,  0.1285,  0.0233, -0.0726]],

        [[-0.0240,  0.2050,  0.0193,  ...,  0.1285,  0.0233, -0.0726]],

        [[-0.0240,  0.2050,  0.0193,  ...,  0.1285,  0.0233, -0.0726]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 24 0
layer name:  MLP
i,j,k 20 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0864,  0.2581,  0.0688,  ...,  0.3367, -0.4165, -0.1946]],

        [[-0.0864,  0.2581,  0.0688,  ...,  0.3367, -0.4165, -0.1946]],

        [[-0.0864,  0.2581,  0.0688,  ...,  0.3367, -0.4165, -0.1946]],

        [[-0.0864,  0.2581,  0.0688,  ...,  0.3367, -0.4165, -0.1946]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0134,  0.1128,  0.0343,  ...,  0.1115, -0.1171, -0.0515]],

        [[-0.0134,  0.1128,  0.0343,  ...,  0.1115, -0.1171, -0.0515]],

        [[-0.0134,  0.1128,  0.0343,  ...,  0.1115, -0.1171, -0.0515]],

        [[-0.0134,  0.1128,  0.0343,  ...,  0.1115, -0.1171, -0.0515]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 27 0
layer name:  MLP
i,j,k 20 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1570,  0.1588,  0.0634,  ...,  0.4509, -0.5439, -0.3621]],

        [[-0.1570,  0.1588,  0.0634,  ...,  0.4509, -0.5439, -0.3621]],

        [[-0.1570,  0.1588,  0.0634,  ...,  0.4509, -0.5439, -0.3621]],

        [[-0.1570,  0.1588,  0.0634,  ...,  0.4509, -0.5439, -0.3621]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0365,  0.0925,  0.0367,  ...,  0.1711, -0.1746, -0.1105]],

        [[-0.0365,  0.0925,  0.0367,  ...,  0.1711, -0.1746, -0.1105]],

        [[-0.0365,  0.0925,  0.0367,  ...,  0.1711, -0.1746, -0.1105]],

        [[-0.0365,  0.0925,  0.0367,  ...,  0.1711, -0.1746, -0.1105]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 30 0
layer name:  MLP
i,j,k 20 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5518,  0.9351, -0.0519,  ...,  0.5649, -0.5317, -0.3850]],

        [[-0.5518,  0.9351, -0.0519,  ...,  0.5649, -0.5317, -0.3850]],

        [[-0.5518,  0.9351, -0.0519,  ...,  0.5649, -0.5317, -0.3850]],

        [[-0.5518,  0.9351, -0.0519,  ...,  0.5649, -0.5317, -0.3850]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2136,  0.6719, -0.0022,  ...,  0.2693, -0.2192, -0.1406]],

        [[-0.2136,  0.6719, -0.0022,  ...,  0.2693, -0.2192, -0.1406]],

        [[-0.2136,  0.6719, -0.0022,  ...,  0.2693, -0.2192, -0.1406]],

        [[-0.2136,  0.6719, -0.0022,  ...,  0.2693, -0.2192, -0.1406]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 33 0
layer name:  MLP
i,j,k 20 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5767,  1.4443,  0.3174,  ...,  0.6343, -0.5991, -0.3625]],

        [[-0.5767,  1.4443,  0.3174,  ...,  0.6343, -0.5991, -0.3625]],

        [[-0.5767,  1.4443,  0.3174,  ...,  0.6343, -0.5991, -0.3625]],

        [[-0.5767,  1.4443,  0.3174,  ...,  0.6343, -0.5991, -0.3625]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 20 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2832,  0.8936,  0.2207,  ...,  0.3857, -0.3359, -0.1545]],

        [[-0.2832,  0.8936,  0.2207,  ...,  0.3857, -0.3359, -0.1545]],

        [[-0.2832,  0.8936,  0.2207,  ...,  0.3857, -0.3359, -0.1545]],

        [[-0.2832,  0.8936,  0.2207,  ...,  0.3857, -0.3359, -0.1545]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 20 36 0
layer name:  MLP
i,j,k 20 37 0
layer name:  OutputEmbed
i,j,k 21 0 0
layer name:  InputEmbed
i,j,k 21 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2864,  0.4233,  0.0751,  ..., -0.2759, -0.2081, -0.0856]],

        [[ 0.2864,  0.4233,  0.0751,  ..., -0.2759, -0.2081, -0.0856]],

        [[ 0.2864,  0.4233,  0.0751,  ..., -0.2759, -0.2081, -0.0856]],

        [[ 0.2864,  0.4233,  0.0751,  ..., -0.2759, -0.2081, -0.0856]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0510,  0.0652,  0.0104,  ..., -0.0624, -0.0524, -0.0182]],

        [[ 0.0510,  0.0652,  0.0104,  ..., -0.0624, -0.0524, -0.0182]],

        [[ 0.0510,  0.0652,  0.0104,  ..., -0.0624, -0.0524, -0.0182]],

        [[ 0.0510,  0.0652,  0.0104,  ..., -0.0624, -0.0524, -0.0182]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 3 0
layer name:  MLP
i,j,k 21 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4990,  0.4114, -0.0783,  ..., -0.1272, -0.2659, -0.1162]],

        [[ 0.4990,  0.4114, -0.0783,  ..., -0.1272, -0.2659, -0.1162]],

        [[ 0.4990,  0.4114, -0.0783,  ..., -0.1272, -0.2659, -0.1162]],

        [[ 0.4990,  0.4114, -0.0783,  ..., -0.1272, -0.2659, -0.1162]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1057,  0.0683, -0.0211,  ..., -0.0342, -0.0565, -0.0219]],

        [[ 0.1057,  0.0683, -0.0211,  ..., -0.0342, -0.0565, -0.0219]],

        [[ 0.1057,  0.0683, -0.0211,  ..., -0.0342, -0.0565, -0.0219]],

        [[ 0.1057,  0.0683, -0.0211,  ..., -0.0342, -0.0565, -0.0219]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 6 0
layer name:  MLP
i,j,k 21 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.7993,  0.4023, -0.0928,  ...,  0.0558, -0.2418, -0.4053]],

        [[ 0.7993,  0.4023, -0.0928,  ...,  0.0558, -0.2418, -0.4053]],

        [[ 0.7993,  0.4023, -0.0928,  ...,  0.0558, -0.2418, -0.4053]],

        [[ 0.7993,  0.4023, -0.0928,  ...,  0.0558, -0.2418, -0.4053]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1418,  0.0707, -0.0185,  ...,  0.0063, -0.0426, -0.0723]],

        [[ 0.1418,  0.0707, -0.0185,  ...,  0.0063, -0.0426, -0.0723]],

        [[ 0.1418,  0.0707, -0.0185,  ...,  0.0063, -0.0426, -0.0723]],

        [[ 0.1418,  0.0707, -0.0185,  ...,  0.0063, -0.0426, -0.0723]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 9 0
layer name:  MLP
i,j,k 21 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.6201, -0.1979, -0.0876,  ...,  0.2732, -0.0945, -0.6006]],

        [[ 0.6201, -0.1979, -0.0876,  ...,  0.2732, -0.0945, -0.6006]],

        [[ 0.6201, -0.1979, -0.0876,  ...,  0.2732, -0.0945, -0.6006]],

        [[ 0.6201, -0.1979, -0.0876,  ...,  0.2732, -0.0945, -0.6006]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1267, -0.0433, -0.0197,  ...,  0.0533, -0.0214, -0.1287]],

        [[ 0.1267, -0.0433, -0.0197,  ...,  0.0533, -0.0214, -0.1287]],

        [[ 0.1267, -0.0433, -0.0197,  ...,  0.0533, -0.0214, -0.1287]],

        [[ 0.1267, -0.0433, -0.0197,  ...,  0.0533, -0.0214, -0.1287]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 12 0
layer name:  MLP
i,j,k 21 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.7812, -0.2524, -0.2708,  ...,  0.6958, -0.0991, -0.6973]],

        [[ 0.7812, -0.2524, -0.2708,  ...,  0.6958, -0.0991, -0.6973]],

        [[ 0.7812, -0.2524, -0.2708,  ...,  0.6958, -0.0991, -0.6973]],

        [[ 0.7812, -0.2524, -0.2708,  ...,  0.6958, -0.0991, -0.6973]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1462, -0.0499, -0.0525,  ...,  0.1306, -0.0202, -0.1432]],

        [[ 0.1462, -0.0499, -0.0525,  ...,  0.1306, -0.0202, -0.1432]],

        [[ 0.1462, -0.0499, -0.0525,  ...,  0.1306, -0.0202, -0.1432]],

        [[ 0.1462, -0.0499, -0.0525,  ...,  0.1306, -0.0202, -0.1432]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 15 0
layer name:  MLP
i,j,k 21 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.6699, -0.1361, -0.2147,  ...,  0.7856, -0.2119, -0.5913]],

        [[ 0.6699, -0.1361, -0.2147,  ...,  0.7856, -0.2119, -0.5913]],

        [[ 0.6699, -0.1361, -0.2147,  ...,  0.7856, -0.2119, -0.5913]],

        [[ 0.6699, -0.1361, -0.2147,  ...,  0.7856, -0.2119, -0.5913]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1484, -0.0299, -0.0461,  ...,  0.1635, -0.0447, -0.1332]],

        [[ 0.1484, -0.0299, -0.0461,  ...,  0.1635, -0.0447, -0.1332]],

        [[ 0.1484, -0.0299, -0.0461,  ...,  0.1635, -0.0447, -0.1332]],

        [[ 0.1484, -0.0299, -0.0461,  ...,  0.1635, -0.0447, -0.1332]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 18 0
layer name:  MLP
i,j,k 21 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.6675,  0.0400, -0.3953,  ...,  0.9624, -0.1964, -0.3040]],

        [[ 0.6675,  0.0400, -0.3953,  ...,  0.9624, -0.1964, -0.3040]],

        [[ 0.6675,  0.0400, -0.3953,  ...,  0.9624, -0.1964, -0.3040]],

        [[ 0.6675,  0.0400, -0.3953,  ...,  0.9624, -0.1964, -0.3040]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1628,  0.0125, -0.0916,  ...,  0.2184, -0.0449, -0.0741]],

        [[ 0.1628,  0.0125, -0.0916,  ...,  0.2184, -0.0449, -0.0741]],

        [[ 0.1628,  0.0125, -0.0916,  ...,  0.2184, -0.0449, -0.0741]],

        [[ 0.1628,  0.0125, -0.0916,  ...,  0.2184, -0.0449, -0.0741]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 21 0
layer name:  MLP
i,j,k 21 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.5986, -0.2710, -0.4319,  ...,  0.9116, -0.2268, -0.2903]],

        [[ 0.5986, -0.2710, -0.4319,  ...,  0.9116, -0.2268, -0.2903]],

        [[ 0.5986, -0.2710, -0.4319,  ...,  0.9116, -0.2268, -0.2903]],

        [[ 0.5986, -0.2710, -0.4319,  ...,  0.9116, -0.2268, -0.2903]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1589, -0.0651, -0.1096,  ...,  0.2334, -0.0547, -0.0738]],

        [[ 0.1589, -0.0651, -0.1096,  ...,  0.2334, -0.0547, -0.0738]],

        [[ 0.1589, -0.0651, -0.1096,  ...,  0.2334, -0.0547, -0.0738]],

        [[ 0.1589, -0.0651, -0.1096,  ...,  0.2334, -0.0547, -0.0738]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 24 0
layer name:  MLP
i,j,k 21 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3916, -0.3899, -0.1544,  ...,  0.9282, -0.8623,  0.0220]],

        [[ 0.3916, -0.3899, -0.1544,  ...,  0.9282, -0.8623,  0.0220]],

        [[ 0.3916, -0.3899, -0.1544,  ...,  0.9282, -0.8623,  0.0220]],

        [[ 0.3916, -0.3899, -0.1544,  ...,  0.9282, -0.8623,  0.0220]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1230, -0.1161, -0.0416,  ...,  0.2625, -0.2439,  0.0089]],

        [[ 0.1230, -0.1161, -0.0416,  ...,  0.2625, -0.2439,  0.0089]],

        [[ 0.1230, -0.1161, -0.0416,  ...,  0.2625, -0.2439,  0.0089]],

        [[ 0.1230, -0.1161, -0.0416,  ...,  0.2625, -0.2439,  0.0089]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 27 0
layer name:  MLP
i,j,k 21 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3289,  0.5801, -0.0719,  ...,  0.9121, -0.9556, -0.4402]],

        [[ 0.3289,  0.5801, -0.0719,  ...,  0.9121, -0.9556, -0.4402]],

        [[ 0.3289,  0.5801, -0.0719,  ...,  0.9121, -0.9556, -0.4402]],

        [[ 0.3289,  0.5801, -0.0719,  ...,  0.9121, -0.9556, -0.4402]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1283,  0.2292, -0.0169,  ...,  0.3242, -0.3223, -0.1421]],

        [[ 0.1283,  0.2292, -0.0169,  ...,  0.3242, -0.3223, -0.1421]],

        [[ 0.1283,  0.2292, -0.0169,  ...,  0.3242, -0.3223, -0.1421]],

        [[ 0.1283,  0.2292, -0.0169,  ...,  0.3242, -0.3223, -0.1421]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 30 0
layer name:  MLP
i,j,k 21 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0686,  1.0947,  0.3831,  ...,  1.0088, -0.7241, -0.1313]],

        [[-0.0686,  1.0947,  0.3831,  ...,  1.0088, -0.7241, -0.1313]],

        [[-0.0686,  1.0947,  0.3831,  ...,  1.0088, -0.7241, -0.1313]],

        [[-0.0686,  1.0947,  0.3831,  ...,  1.0088, -0.7241, -0.1313]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0098,  0.8184,  0.2001,  ...,  0.4895, -0.3311, -0.0417]],

        [[-0.0098,  0.8184,  0.2001,  ...,  0.4895, -0.3311, -0.0417]],

        [[-0.0098,  0.8184,  0.2001,  ...,  0.4895, -0.3311, -0.0417]],

        [[-0.0098,  0.8184,  0.2001,  ...,  0.4895, -0.3311, -0.0417]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 33 0
layer name:  MLP
i,j,k 21 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0980,  2.2227,  0.5024,  ...,  0.8374, -0.6509, -0.0962]],

        [[-0.0980,  2.2227,  0.5024,  ...,  0.8374, -0.6509, -0.0962]],

        [[-0.0980,  2.2227,  0.5024,  ...,  0.8374, -0.6509, -0.0962]],

        [[-0.0980,  2.2227,  0.5024,  ...,  0.8374, -0.6509, -0.0962]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 21 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0279,  1.4443,  0.3525,  ...,  0.5376, -0.4082, -0.0231]],

        [[-0.0279,  1.4443,  0.3525,  ...,  0.5376, -0.4082, -0.0231]],

        [[-0.0279,  1.4443,  0.3525,  ...,  0.5376, -0.4082, -0.0231]],

        [[-0.0279,  1.4443,  0.3525,  ...,  0.5376, -0.4082, -0.0231]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 21 36 0
layer name:  MLP
i,j,k 21 37 0
layer name:  OutputEmbed
i,j,k 22 0 0
layer name:  InputEmbed
i,j,k 22 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2947,  0.4712,  0.3035,  ..., -0.0963, -0.1137,  0.1959]],

        [[-0.2947,  0.4712,  0.3035,  ..., -0.0963, -0.1137,  0.1959]],

        [[-0.2947,  0.4712,  0.3035,  ..., -0.0963, -0.1137,  0.1959]],

        [[-0.2947,  0.4712,  0.3035,  ..., -0.0963, -0.1137,  0.1959]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0658,  0.0696,  0.0548,  ..., -0.0283, -0.0375,  0.0560]],

        [[-0.0658,  0.0696,  0.0548,  ..., -0.0283, -0.0375,  0.0560]],

        [[-0.0658,  0.0696,  0.0548,  ..., -0.0283, -0.0375,  0.0560]],

        [[-0.0658,  0.0696,  0.0548,  ..., -0.0283, -0.0375,  0.0560]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 3 0
layer name:  MLP
i,j,k 22 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0909,  0.4336,  0.2971,  ...,  0.0336, -0.1731,  0.1356]],

        [[-0.0909,  0.4336,  0.2971,  ...,  0.0336, -0.1731,  0.1356]],

        [[-0.0909,  0.4336,  0.2971,  ...,  0.0336, -0.1731,  0.1356]],

        [[-0.0909,  0.4336,  0.2971,  ...,  0.0336, -0.1731,  0.1356]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0279,  0.0665,  0.0596,  ..., -0.0019, -0.0429,  0.0386]],

        [[-0.0279,  0.0665,  0.0596,  ..., -0.0019, -0.0429,  0.0386]],

        [[-0.0279,  0.0665,  0.0596,  ..., -0.0019, -0.0429,  0.0386]],

        [[-0.0279,  0.0665,  0.0596,  ..., -0.0019, -0.0429,  0.0386]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 6 0
layer name:  MLP
i,j,k 22 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1610,  0.4507,  0.3813,  ...,  0.3440, -0.3613,  0.0645]],

        [[ 0.1610,  0.4507,  0.3813,  ...,  0.3440, -0.3613,  0.0645]],

        [[ 0.1610,  0.4507,  0.3813,  ...,  0.3440, -0.3613,  0.0645]],

        [[ 0.1610,  0.4507,  0.3813,  ...,  0.3440, -0.3613,  0.0645]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0219,  0.0756,  0.0631,  ...,  0.0517, -0.0657,  0.0102]],

        [[ 0.0219,  0.0756,  0.0631,  ...,  0.0517, -0.0657,  0.0102]],

        [[ 0.0219,  0.0756,  0.0631,  ...,  0.0517, -0.0657,  0.0102]],

        [[ 0.0219,  0.0756,  0.0631,  ...,  0.0517, -0.0657,  0.0102]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 9 0
layer name:  MLP
i,j,k 22 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2263,  0.4341,  0.3557,  ...,  0.4678, -0.1499, -0.0779]],

        [[ 0.2263,  0.4341,  0.3557,  ...,  0.4678, -0.1499, -0.0779]],

        [[ 0.2263,  0.4341,  0.3557,  ...,  0.4678, -0.1499, -0.0779]],

        [[ 0.2263,  0.4341,  0.3557,  ...,  0.4678, -0.1499, -0.0779]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0375,  0.0844,  0.0605,  ...,  0.0839, -0.0343, -0.0195]],

        [[ 0.0375,  0.0844,  0.0605,  ...,  0.0839, -0.0343, -0.0195]],

        [[ 0.0375,  0.0844,  0.0605,  ...,  0.0839, -0.0343, -0.0195]],

        [[ 0.0375,  0.0844,  0.0605,  ...,  0.0839, -0.0343, -0.0195]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 12 0
layer name:  MLP
i,j,k 22 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4382,  0.4126,  0.2435,  ...,  0.6514, -0.1462, -0.2566]],

        [[ 0.4382,  0.4126,  0.2435,  ...,  0.6514, -0.1462, -0.2566]],

        [[ 0.4382,  0.4126,  0.2435,  ...,  0.6514, -0.1462, -0.2566]],

        [[ 0.4382,  0.4126,  0.2435,  ...,  0.6514, -0.1462, -0.2566]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0688,  0.0676,  0.0362,  ...,  0.1063, -0.0316, -0.0530]],

        [[ 0.0688,  0.0676,  0.0362,  ...,  0.1063, -0.0316, -0.0530]],

        [[ 0.0688,  0.0676,  0.0362,  ...,  0.1063, -0.0316, -0.0530]],

        [[ 0.0688,  0.0676,  0.0362,  ...,  0.1063, -0.0316, -0.0530]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 15 0
layer name:  MLP
i,j,k 22 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4155,  0.3618, -0.0432,  ...,  0.5996, -0.1827, -0.6255]],

        [[ 0.4155,  0.3618, -0.0432,  ...,  0.5996, -0.1827, -0.6255]],

        [[ 0.4155,  0.3618, -0.0432,  ...,  0.5996, -0.1827, -0.6255]],

        [[ 0.4155,  0.3618, -0.0432,  ...,  0.5996, -0.1827, -0.6255]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0771,  0.0721, -0.0140,  ...,  0.1070, -0.0396, -0.1315]],

        [[ 0.0771,  0.0721, -0.0140,  ...,  0.1070, -0.0396, -0.1315]],

        [[ 0.0771,  0.0721, -0.0140,  ...,  0.1070, -0.0396, -0.1315]],

        [[ 0.0771,  0.0721, -0.0140,  ...,  0.1070, -0.0396, -0.1315]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 18 0
layer name:  MLP
i,j,k 22 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2427,  0.2200, -0.1316,  ...,  0.7314, -0.0626, -0.3748]],

        [[ 0.2427,  0.2200, -0.1316,  ...,  0.7314, -0.0626, -0.3748]],

        [[ 0.2427,  0.2200, -0.1316,  ...,  0.7314, -0.0626, -0.3748]],

        [[ 0.2427,  0.2200, -0.1316,  ...,  0.7314, -0.0626, -0.3748]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0507,  0.0494, -0.0335,  ...,  0.1503, -0.0181, -0.0903]],

        [[ 0.0507,  0.0494, -0.0335,  ...,  0.1503, -0.0181, -0.0903]],

        [[ 0.0507,  0.0494, -0.0335,  ...,  0.1503, -0.0181, -0.0903]],

        [[ 0.0507,  0.0494, -0.0335,  ...,  0.1503, -0.0181, -0.0903]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 21 0
layer name:  MLP
i,j,k 22 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4272, -0.2087, -0.2316,  ...,  0.7974, -0.2343, -0.7759]],

        [[ 0.4272, -0.2087, -0.2316,  ...,  0.7974, -0.2343, -0.7759]],

        [[ 0.4272, -0.2087, -0.2316,  ...,  0.7974, -0.2343, -0.7759]],

        [[ 0.4272, -0.2087, -0.2316,  ...,  0.7974, -0.2343, -0.7759]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1071, -0.0514, -0.0603,  ...,  0.1945, -0.0592, -0.1982]],

        [[ 0.1071, -0.0514, -0.0603,  ...,  0.1945, -0.0592, -0.1982]],

        [[ 0.1071, -0.0514, -0.0603,  ...,  0.1945, -0.0592, -0.1982]],

        [[ 0.1071, -0.0514, -0.0603,  ...,  0.1945, -0.0592, -0.1982]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 24 0
layer name:  MLP
i,j,k 22 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.5044, -0.6519, -0.2487,  ...,  0.6128, -0.2461, -0.6094]],

        [[ 0.5044, -0.6519, -0.2487,  ...,  0.6128, -0.2461, -0.6094]],

        [[ 0.5044, -0.6519, -0.2487,  ...,  0.6128, -0.2461, -0.6094]],

        [[ 0.5044, -0.6519, -0.2487,  ...,  0.6128, -0.2461, -0.6094]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1543, -0.2091, -0.0746,  ...,  0.1711, -0.0717, -0.1841]],

        [[ 0.1543, -0.2091, -0.0746,  ...,  0.1711, -0.0717, -0.1841]],

        [[ 0.1543, -0.2091, -0.0746,  ...,  0.1711, -0.0717, -0.1841]],

        [[ 0.1543, -0.2091, -0.0746,  ...,  0.1711, -0.0717, -0.1841]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 27 0
layer name:  MLP
i,j,k 22 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.5771, -0.0162, -0.2981,  ...,  0.3416, -0.1124, -0.6323]],

        [[ 0.5771, -0.0162, -0.2981,  ...,  0.3416, -0.1124, -0.6323]],

        [[ 0.5771, -0.0162, -0.2981,  ...,  0.3416, -0.1124, -0.6323]],

        [[ 0.5771, -0.0162, -0.2981,  ...,  0.3416, -0.1124, -0.6323]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2168,  0.0219, -0.1063,  ...,  0.1250, -0.0356, -0.2213]],

        [[ 0.2168,  0.0219, -0.1063,  ...,  0.1250, -0.0356, -0.2213]],

        [[ 0.2168,  0.0219, -0.1063,  ...,  0.1250, -0.0356, -0.2213]],

        [[ 0.2168,  0.0219, -0.1063,  ...,  0.1250, -0.0356, -0.2213]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 30 0
layer name:  MLP
i,j,k 22 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.8179,  0.8428, -0.2964,  ...,  0.1489, -0.3098, -0.7759]],

        [[ 0.8179,  0.8428, -0.2964,  ...,  0.1489, -0.3098, -0.7759]],

        [[ 0.8179,  0.8428, -0.2964,  ...,  0.1489, -0.3098, -0.7759]],

        [[ 0.8179,  0.8428, -0.2964,  ...,  0.1489, -0.3098, -0.7759]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.4136,  0.6709, -0.1403,  ...,  0.0808, -0.1488, -0.3569]],

        [[ 0.4136,  0.6709, -0.1403,  ...,  0.0808, -0.1488, -0.3569]],

        [[ 0.4136,  0.6709, -0.1403,  ...,  0.0808, -0.1488, -0.3569]],

        [[ 0.4136,  0.6709, -0.1403,  ...,  0.0808, -0.1488, -0.3569]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 33 0
layer name:  MLP
i,j,k 22 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.8018,  1.5869, -0.1052,  ...,  0.0027, -0.1974, -0.7524]],

        [[ 0.8018,  1.5869, -0.1052,  ...,  0.0027, -0.1974, -0.7524]],

        [[ 0.8018,  1.5869, -0.1052,  ...,  0.0027, -0.1974, -0.7524]],

        [[ 0.8018,  1.5869, -0.1052,  ...,  0.0027, -0.1974, -0.7524]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 22 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.4756,  0.9863, -0.0424,  ...,  0.0225, -0.1138, -0.3857]],

        [[ 0.4756,  0.9863, -0.0424,  ...,  0.0225, -0.1138, -0.3857]],

        [[ 0.4756,  0.9863, -0.0424,  ...,  0.0225, -0.1138, -0.3857]],

        [[ 0.4756,  0.9863, -0.0424,  ...,  0.0225, -0.1138, -0.3857]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 22 36 0
layer name:  MLP
i,j,k 22 37 0
layer name:  OutputEmbed
i,j,k 23 0 0
layer name:  InputEmbed
i,j,k 23 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1810,  0.1135, -0.3118,  ...,  0.3074,  0.3926, -0.5483]],

        [[-0.1810,  0.1135, -0.3118,  ...,  0.3074,  0.3926, -0.5483]],

        [[-0.1810,  0.1135, -0.3118,  ...,  0.3074,  0.3926, -0.5483]],

        [[-0.1810,  0.1135, -0.3118,  ...,  0.3074,  0.3926, -0.5483]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0236,  0.0204, -0.0586,  ...,  0.0826,  0.0894, -0.1371]],

        [[-0.0236,  0.0204, -0.0586,  ...,  0.0826,  0.0894, -0.1371]],

        [[-0.0236,  0.0204, -0.0586,  ...,  0.0826,  0.0894, -0.1371]],

        [[-0.0236,  0.0204, -0.0586,  ...,  0.0826,  0.0894, -0.1371]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 3 0
layer name:  MLP
i,j,k 23 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1512,  0.1071, -0.3630,  ...,  0.4312,  0.3560, -0.5083]],

        [[-0.1512,  0.1071, -0.3630,  ...,  0.4312,  0.3560, -0.5083]],

        [[-0.1512,  0.1071, -0.3630,  ...,  0.4312,  0.3560, -0.5083]],

        [[-0.1512,  0.1071, -0.3630,  ...,  0.4312,  0.3560, -0.5083]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0209,  0.0358, -0.0715,  ...,  0.1139,  0.0775, -0.1104]],

        [[-0.0209,  0.0358, -0.0715,  ...,  0.1139,  0.0775, -0.1104]],

        [[-0.0209,  0.0358, -0.0715,  ...,  0.1139,  0.0775, -0.1104]],

        [[-0.0209,  0.0358, -0.0715,  ...,  0.1139,  0.0775, -0.1104]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 6 0
layer name:  MLP
i,j,k 23 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0257,  0.2085, -0.5098,  ...,  0.7695,  0.4382, -0.5923]],

        [[-0.0257,  0.2085, -0.5098,  ...,  0.7695,  0.4382, -0.5923]],

        [[-0.0257,  0.2085, -0.5098,  ...,  0.7695,  0.4382, -0.5923]],

        [[-0.0257,  0.2085, -0.5098,  ...,  0.7695,  0.4382, -0.5923]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0096,  0.0542, -0.0779,  ...,  0.1489,  0.0839, -0.0911]],

        [[ 0.0096,  0.0542, -0.0779,  ...,  0.1489,  0.0839, -0.0911]],

        [[ 0.0096,  0.0542, -0.0779,  ...,  0.1489,  0.0839, -0.0911]],

        [[ 0.0096,  0.0542, -0.0779,  ...,  0.1489,  0.0839, -0.0911]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 9 0
layer name:  MLP
i,j,k 23 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0737,  0.3704, -0.4707,  ...,  0.7227,  0.4268, -0.5439]],

        [[-0.0737,  0.3704, -0.4707,  ...,  0.7227,  0.4268, -0.5439]],

        [[-0.0737,  0.3704, -0.4707,  ...,  0.7227,  0.4268, -0.5439]],

        [[-0.0737,  0.3704, -0.4707,  ...,  0.7227,  0.4268, -0.5439]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 1.1253e-04,  9.3079e-02, -7.2815e-02,  ...,  1.5527e-01,
           9.2407e-02, -9.2102e-02]],

        [[ 1.1253e-04,  9.3079e-02, -7.2815e-02,  ...,  1.5527e-01,
           9.2407e-02, -9.2102e-02]],

        [[ 1.1253e-04,  9.3079e-02, -7.2815e-02,  ...,  1.5527e-01,
           9.2407e-02, -9.2102e-02]],

        [[ 1.1253e-04,  9.3079e-02, -7.2815e-02,  ...,  1.5527e-01,
           9.2407e-02, -9.2102e-02]]], device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 12 0
layer name:  MLP
i,j,k 23 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0901,  0.2771, -0.6094,  ...,  0.8496,  0.5068, -0.7236]],

        [[-0.0901,  0.2771, -0.6094,  ...,  0.8496,  0.5068, -0.7236]],

        [[-0.0901,  0.2771, -0.6094,  ...,  0.8496,  0.5068, -0.7236]],

        [[-0.0901,  0.2771, -0.6094,  ...,  0.8496,  0.5068, -0.7236]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0011,  0.0670, -0.0922,  ...,  0.1667,  0.1058, -0.1219]],

        [[-0.0011,  0.0670, -0.0922,  ...,  0.1667,  0.1058, -0.1219]],

        [[-0.0011,  0.0670, -0.0922,  ...,  0.1667,  0.1058, -0.1219]],

        [[-0.0011,  0.0670, -0.0922,  ...,  0.1667,  0.1058, -0.1219]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 15 0
layer name:  MLP
i,j,k 23 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0651,  0.3662, -0.6753,  ...,  0.6875,  0.3235, -0.8179]],

        [[-0.0651,  0.3662, -0.6753,  ...,  0.6875,  0.3235, -0.8179]],

        [[-0.0651,  0.3662, -0.6753,  ...,  0.6875,  0.3235, -0.8179]],

        [[-0.0651,  0.3662, -0.6753,  ...,  0.6875,  0.3235, -0.8179]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0020,  0.0970, -0.1144,  ...,  0.1488,  0.0793, -0.1523]],

        [[ 0.0020,  0.0970, -0.1144,  ...,  0.1488,  0.0793, -0.1523]],

        [[ 0.0020,  0.0970, -0.1144,  ...,  0.1488,  0.0793, -0.1523]],

        [[ 0.0020,  0.0970, -0.1144,  ...,  0.1488,  0.0793, -0.1523]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 18 0
layer name:  MLP
i,j,k 23 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2365,  0.3142, -0.7456,  ...,  0.7217,  0.2964, -0.5967]],

        [[-0.2365,  0.3142, -0.7456,  ...,  0.7217,  0.2964, -0.5967]],

        [[-0.2365,  0.3142, -0.7456,  ...,  0.7217,  0.2964, -0.5967]],

        [[-0.2365,  0.3142, -0.7456,  ...,  0.7217,  0.2964, -0.5967]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0374,  0.0949, -0.1465,  ...,  0.1727,  0.0814, -0.1225]],

        [[-0.0374,  0.0949, -0.1465,  ...,  0.1727,  0.0814, -0.1225]],

        [[-0.0374,  0.0949, -0.1465,  ...,  0.1727,  0.0814, -0.1225]],

        [[-0.0374,  0.0949, -0.1465,  ...,  0.1727,  0.0814, -0.1225]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 21 0
layer name:  MLP
i,j,k 23 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0027,  0.1256, -0.7500,  ...,  0.7129, -0.1978, -0.6597]],

        [[-0.0027,  0.1256, -0.7500,  ...,  0.7129, -0.1978, -0.6597]],

        [[-0.0027,  0.1256, -0.7500,  ...,  0.7129, -0.1978, -0.6597]],

        [[-0.0027,  0.1256, -0.7500,  ...,  0.7129, -0.1978, -0.6597]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0196,  0.0568, -0.1678,  ...,  0.1935, -0.0284, -0.1459]],

        [[ 0.0196,  0.0568, -0.1678,  ...,  0.1935, -0.0284, -0.1459]],

        [[ 0.0196,  0.0568, -0.1678,  ...,  0.1935, -0.0284, -0.1459]],

        [[ 0.0196,  0.0568, -0.1678,  ...,  0.1935, -0.0284, -0.1459]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 24 0
layer name:  MLP
i,j,k 23 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2179, -0.5200, -0.5698,  ...,  0.5718, -0.3540, -0.4839]],

        [[ 0.2179, -0.5200, -0.5698,  ...,  0.5718, -0.3540, -0.4839]],

        [[ 0.2179, -0.5200, -0.5698,  ...,  0.5718, -0.3540, -0.4839]],

        [[ 0.2179, -0.5200, -0.5698,  ...,  0.5718, -0.3540, -0.4839]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0898, -0.1436, -0.1487,  ...,  0.1819, -0.0813, -0.1248]],

        [[ 0.0898, -0.1436, -0.1487,  ...,  0.1819, -0.0813, -0.1248]],

        [[ 0.0898, -0.1436, -0.1487,  ...,  0.1819, -0.0813, -0.1248]],

        [[ 0.0898, -0.1436, -0.1487,  ...,  0.1819, -0.0813, -0.1248]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 27 0
layer name:  MLP
i,j,k 23 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2581, -0.3738, -0.4324,  ...,  0.4016, -0.2406, -0.4189]],

        [[ 0.2581, -0.3738, -0.4324,  ...,  0.4016, -0.2406, -0.4189]],

        [[ 0.2581, -0.3738, -0.4324,  ...,  0.4016, -0.2406, -0.4189]],

        [[ 0.2581, -0.3738, -0.4324,  ...,  0.4016, -0.2406, -0.4189]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1227, -0.0820, -0.1337,  ...,  0.1680, -0.0601, -0.1237]],

        [[ 0.1227, -0.0820, -0.1337,  ...,  0.1680, -0.0601, -0.1237]],

        [[ 0.1227, -0.0820, -0.1337,  ...,  0.1680, -0.0601, -0.1237]],

        [[ 0.1227, -0.0820, -0.1337,  ...,  0.1680, -0.0601, -0.1237]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 30 0
layer name:  MLP
i,j,k 23 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4165,  0.6851, -0.3030,  ...,  0.3787, -0.5767, -0.4119]],

        [[ 0.4165,  0.6851, -0.3030,  ...,  0.3787, -0.5767, -0.4119]],

        [[ 0.4165,  0.6851, -0.3030,  ...,  0.3787, -0.5767, -0.4119]],

        [[ 0.4165,  0.6851, -0.3030,  ...,  0.3787, -0.5767, -0.4119]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.2473,  0.5947, -0.1235,  ...,  0.2234, -0.2659, -0.1655]],

        [[ 0.2473,  0.5947, -0.1235,  ...,  0.2234, -0.2659, -0.1655]],

        [[ 0.2473,  0.5947, -0.1235,  ...,  0.2234, -0.2659, -0.1655]],

        [[ 0.2473,  0.5947, -0.1235,  ...,  0.2234, -0.2659, -0.1655]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 33 0
layer name:  MLP
i,j,k 23 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2751,  1.7861, -0.0749,  ...,  0.2971, -0.5005, -0.3423]],

        [[ 0.2751,  1.7861, -0.0749,  ...,  0.2971, -0.5005, -0.3423]],

        [[ 0.2751,  1.7861, -0.0749,  ...,  0.2971, -0.5005, -0.3423]],

        [[ 0.2751,  1.7861, -0.0749,  ...,  0.2971, -0.5005, -0.3423]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 23 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 2.1069e-01,  1.1816e+00,  1.2207e-04,  ...,  2.2485e-01,
          -2.9077e-01, -1.4697e-01]],

        [[ 2.1069e-01,  1.1816e+00,  1.2207e-04,  ...,  2.2485e-01,
          -2.9077e-01, -1.4697e-01]],

        [[ 2.1069e-01,  1.1816e+00,  1.2207e-04,  ...,  2.2485e-01,
          -2.9077e-01, -1.4697e-01]],

        [[ 2.1069e-01,  1.1816e+00,  1.2207e-04,  ...,  2.2485e-01,
          -2.9077e-01, -1.4697e-01]]], device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 23 36 0
layer name:  MLP
i,j,k 23 37 0
layer name:  OutputEmbed
i,j,k 24 0 0
layer name:  InputEmbed
i,j,k 24 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1974,  0.4419, -0.1541,  ..., -0.0760, -0.1716, -0.3101]],

        [[-0.1974,  0.4419, -0.1541,  ..., -0.0760, -0.1716, -0.3101]],

        [[-0.1974,  0.4419, -0.1541,  ..., -0.0760, -0.1716, -0.3101]],

        [[-0.1974,  0.4419, -0.1541,  ..., -0.0760, -0.1716, -0.3101]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0454,  0.0703, -0.0414,  ..., -0.0213, -0.0474, -0.0850]],

        [[-0.0454,  0.0703, -0.0414,  ..., -0.0213, -0.0474, -0.0850]],

        [[-0.0454,  0.0703, -0.0414,  ..., -0.0213, -0.0474, -0.0850]],

        [[-0.0454,  0.0703, -0.0414,  ..., -0.0213, -0.0474, -0.0850]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 3 0
layer name:  MLP
i,j,k 24 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3796,  0.4727, -0.1710,  ...,  0.1335, -0.0723, -0.2878]],

        [[-0.3796,  0.4727, -0.1710,  ...,  0.1335, -0.0723, -0.2878]],

        [[-0.3796,  0.4727, -0.1710,  ...,  0.1335, -0.0723, -0.2878]],

        [[-0.3796,  0.4727, -0.1710,  ...,  0.1335, -0.0723, -0.2878]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0876,  0.0786, -0.0422,  ...,  0.0252, -0.0208, -0.0664]],

        [[-0.0876,  0.0786, -0.0422,  ...,  0.0252, -0.0208, -0.0664]],

        [[-0.0876,  0.0786, -0.0422,  ...,  0.0252, -0.0208, -0.0664]],

        [[-0.0876,  0.0786, -0.0422,  ...,  0.0252, -0.0208, -0.0664]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 6 0
layer name:  MLP
i,j,k 24 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3591,  0.5464, -0.1190,  ...,  0.3547, -0.1179, -0.1627]],

        [[-0.3591,  0.5464, -0.1190,  ...,  0.3547, -0.1179, -0.1627]],

        [[-0.3591,  0.5464, -0.1190,  ...,  0.3547, -0.1179, -0.1627]],

        [[-0.3591,  0.5464, -0.1190,  ...,  0.3547, -0.1179, -0.1627]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0696,  0.0999, -0.0231,  ...,  0.0595, -0.0229, -0.0276]],

        [[-0.0696,  0.0999, -0.0231,  ...,  0.0595, -0.0229, -0.0276]],

        [[-0.0696,  0.0999, -0.0231,  ...,  0.0595, -0.0229, -0.0276]],

        [[-0.0696,  0.0999, -0.0231,  ...,  0.0595, -0.0229, -0.0276]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 9 0
layer name:  MLP
i,j,k 24 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4272,  0.5439, -0.0226,  ...,  0.2837, -0.1554, -0.2405]],

        [[-0.4272,  0.5439, -0.0226,  ...,  0.2837, -0.1554, -0.2405]],

        [[-0.4272,  0.5439, -0.0226,  ...,  0.2837, -0.1554, -0.2405]],

        [[-0.4272,  0.5439, -0.0226,  ...,  0.2837, -0.1554, -0.2405]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0876,  0.1126, -0.0059,  ...,  0.0533, -0.0311, -0.0483]],

        [[-0.0876,  0.1126, -0.0059,  ...,  0.0533, -0.0311, -0.0483]],

        [[-0.0876,  0.1126, -0.0059,  ...,  0.0533, -0.0311, -0.0483]],

        [[-0.0876,  0.1126, -0.0059,  ...,  0.0533, -0.0311, -0.0483]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 12 0
layer name:  MLP
i,j,k 24 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.7163,  0.5288,  0.0535,  ...,  0.6055,  0.0543, -0.1934]],

        [[-0.7163,  0.5288,  0.0535,  ...,  0.6055,  0.0543, -0.1934]],

        [[-0.7163,  0.5288,  0.0535,  ...,  0.6055,  0.0543, -0.1934]],

        [[-0.7163,  0.5288,  0.0535,  ...,  0.6055,  0.0543, -0.1934]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1323,  0.0974,  0.0085,  ...,  0.1080,  0.0082, -0.0380]],

        [[-0.1323,  0.0974,  0.0085,  ...,  0.1080,  0.0082, -0.0380]],

        [[-0.1323,  0.0974,  0.0085,  ...,  0.1080,  0.0082, -0.0380]],

        [[-0.1323,  0.0974,  0.0085,  ...,  0.1080,  0.0082, -0.0380]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 15 0
layer name:  MLP
i,j,k 24 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.9390,  0.4187, -0.3472,  ...,  0.6919,  0.0178, -0.0812]],

        [[-0.9390,  0.4187, -0.3472,  ...,  0.6919,  0.0178, -0.0812]],

        [[-0.9390,  0.4187, -0.3472,  ...,  0.6919,  0.0178, -0.0812]],

        [[-0.9390,  0.4187, -0.3472,  ...,  0.6919,  0.0178, -0.0812]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2058,  0.0954, -0.0718,  ...,  0.1389,  0.0031, -0.0182]],

        [[-0.2058,  0.0954, -0.0718,  ...,  0.1389,  0.0031, -0.0182]],

        [[-0.2058,  0.0954, -0.0718,  ...,  0.1389,  0.0031, -0.0182]],

        [[-0.2058,  0.0954, -0.0718,  ...,  0.1389,  0.0031, -0.0182]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 18 0
layer name:  MLP
i,j,k 24 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-1.0195,  0.2759, -0.3228,  ...,  0.6035, -0.0329, -0.0555]],

        [[-1.0195,  0.2759, -0.3228,  ...,  0.6035, -0.0329, -0.0555]],

        [[-1.0195,  0.2759, -0.3228,  ...,  0.6035, -0.0329, -0.0555]],

        [[-1.0195,  0.2759, -0.3228,  ...,  0.6035, -0.0329, -0.0555]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2627,  0.0755, -0.0795,  ...,  0.1437, -0.0085, -0.0133]],

        [[-0.2627,  0.0755, -0.0795,  ...,  0.1437, -0.0085, -0.0133]],

        [[-0.2627,  0.0755, -0.0795,  ...,  0.1437, -0.0085, -0.0133]],

        [[-0.2627,  0.0755, -0.0795,  ...,  0.1437, -0.0085, -0.0133]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 21 0
layer name:  MLP
i,j,k 24 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.9526,  0.1528, -0.3687,  ...,  0.6504, -0.1698, -0.0412]],

        [[-0.9526,  0.1528, -0.3687,  ...,  0.6504, -0.1698, -0.0412]],

        [[-0.9526,  0.1528, -0.3687,  ...,  0.6504, -0.1698, -0.0412]],

        [[-0.9526,  0.1528, -0.3687,  ...,  0.6504, -0.1698, -0.0412]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2507,  0.0482, -0.0958,  ...,  0.1691, -0.0421, -0.0103]],

        [[-0.2507,  0.0482, -0.0958,  ...,  0.1691, -0.0421, -0.0103]],

        [[-0.2507,  0.0482, -0.0958,  ...,  0.1691, -0.0421, -0.0103]],

        [[-0.2507,  0.0482, -0.0958,  ...,  0.1691, -0.0421, -0.0103]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 24 0
layer name:  MLP
i,j,k 24 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.8516, -0.2966, -0.3423,  ...,  0.6958, -0.4187, -0.0815]],

        [[-0.8516, -0.2966, -0.3423,  ...,  0.6958, -0.4187, -0.0815]],

        [[-0.8516, -0.2966, -0.3423,  ...,  0.6958, -0.4187, -0.0815]],

        [[-0.8516, -0.2966, -0.3423,  ...,  0.6958, -0.4187, -0.0815]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2727, -0.0950, -0.1075,  ...,  0.2128, -0.1284, -0.0254]],

        [[-0.2727, -0.0950, -0.1075,  ...,  0.2128, -0.1284, -0.0254]],

        [[-0.2727, -0.0950, -0.1075,  ...,  0.2128, -0.1284, -0.0254]],

        [[-0.2727, -0.0950, -0.1075,  ...,  0.2128, -0.1284, -0.0254]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 27 0
layer name:  MLP
i,j,k 24 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.8281, -0.0906, -0.3401,  ...,  0.5107, -0.1825, -0.2227]],

        [[-0.8281, -0.0906, -0.3401,  ...,  0.5107, -0.1825, -0.2227]],

        [[-0.8281, -0.0906, -0.3401,  ...,  0.5107, -0.1825, -0.2227]],

        [[-0.8281, -0.0906, -0.3401,  ...,  0.5107, -0.1825, -0.2227]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2910, -0.0035, -0.1207,  ...,  0.1860, -0.0601, -0.0749]],

        [[-0.2910, -0.0035, -0.1207,  ...,  0.1860, -0.0601, -0.0749]],

        [[-0.2910, -0.0035, -0.1207,  ...,  0.1860, -0.0601, -0.0749]],

        [[-0.2910, -0.0035, -0.1207,  ...,  0.1860, -0.0601, -0.0749]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 30 0
layer name:  MLP
i,j,k 24 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6914,  0.6450, -0.0076,  ...,  0.3438, -0.2299, -0.1946]],

        [[-0.6914,  0.6450, -0.0076,  ...,  0.3438, -0.2299, -0.1946]],

        [[-0.6914,  0.6450, -0.0076,  ...,  0.3438, -0.2299, -0.1946]],

        [[-0.6914,  0.6450, -0.0076,  ...,  0.3438, -0.2299, -0.1946]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3091,  0.5181,  0.0092,  ...,  0.1768, -0.1019, -0.0756]],

        [[-0.3091,  0.5181,  0.0092,  ...,  0.1768, -0.1019, -0.0756]],

        [[-0.3091,  0.5181,  0.0092,  ...,  0.1768, -0.1019, -0.0756]],

        [[-0.3091,  0.5181,  0.0092,  ...,  0.1768, -0.1019, -0.0756]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 33 0
layer name:  MLP
i,j,k 24 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3896,  1.6035,  0.1109,  ...,  0.0220, -0.3992, -0.2637]],

        [[-0.3896,  1.6035,  0.1109,  ...,  0.0220, -0.3992, -0.2637]],

        [[-0.3896,  1.6035,  0.1109,  ...,  0.0220, -0.3992, -0.2637]],

        [[-0.3896,  1.6035,  0.1109,  ...,  0.0220, -0.3992, -0.2637]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 24 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1951,  0.9917,  0.0912,  ...,  0.0379, -0.2317, -0.1160]],

        [[-0.1951,  0.9917,  0.0912,  ...,  0.0379, -0.2317, -0.1160]],

        [[-0.1951,  0.9917,  0.0912,  ...,  0.0379, -0.2317, -0.1160]],

        [[-0.1951,  0.9917,  0.0912,  ...,  0.0379, -0.2317, -0.1160]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 24 36 0
layer name:  MLP
i,j,k 24 37 0
layer name:  OutputEmbed
i,j,k 25 0 0
layer name:  InputEmbed
i,j,k 25 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4626,  0.2107,  0.3240,  ...,  0.5039,  0.3027, -0.3643]],

        [[ 0.4626,  0.2107,  0.3240,  ...,  0.5039,  0.3027, -0.3643]],

        [[ 0.4626,  0.2107,  0.3240,  ...,  0.5039,  0.3027, -0.3643]],

        [[ 0.4626,  0.2107,  0.3240,  ...,  0.5039,  0.3027, -0.3643]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1085,  0.0496,  0.0891,  ...,  0.1257,  0.0760, -0.0609]],

        [[ 0.1085,  0.0496,  0.0891,  ...,  0.1257,  0.0760, -0.0609]],

        [[ 0.1085,  0.0496,  0.0891,  ...,  0.1257,  0.0760, -0.0609]],

        [[ 0.1085,  0.0496,  0.0891,  ...,  0.1257,  0.0760, -0.0609]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 3 0
layer name:  MLP
i,j,k 25 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3149,  0.2700,  0.2954,  ...,  0.3799,  0.3333, -0.3765]],

        [[ 0.3149,  0.2700,  0.2954,  ...,  0.3799,  0.3333, -0.3765]],

        [[ 0.3149,  0.2700,  0.2954,  ...,  0.3799,  0.3333, -0.3765]],

        [[ 0.3149,  0.2700,  0.2954,  ...,  0.3799,  0.3333, -0.3765]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1133,  0.0871,  0.1125,  ...,  0.1335,  0.0983, -0.0836]],

        [[ 0.1133,  0.0871,  0.1125,  ...,  0.1335,  0.0983, -0.0836]],

        [[ 0.1133,  0.0871,  0.1125,  ...,  0.1335,  0.0983, -0.0836]],

        [[ 0.1133,  0.0871,  0.1125,  ...,  0.1335,  0.0983, -0.0836]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 6 0
layer name:  MLP
i,j,k 25 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.4839,  0.3826,  0.4238,  ...,  0.4429,  0.3391, -0.3684]],

        [[ 0.4839,  0.3826,  0.4238,  ...,  0.4429,  0.3391, -0.3684]],

        [[ 0.4839,  0.3826,  0.4238,  ...,  0.4429,  0.3391, -0.3684]],

        [[ 0.4839,  0.3826,  0.4238,  ...,  0.4429,  0.3391, -0.3684]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1407,  0.1182,  0.1279,  ...,  0.1251,  0.0946, -0.0544]],

        [[ 0.1407,  0.1182,  0.1279,  ...,  0.1251,  0.0946, -0.0544]],

        [[ 0.1407,  0.1182,  0.1279,  ...,  0.1251,  0.0946, -0.0544]],

        [[ 0.1407,  0.1182,  0.1279,  ...,  0.1251,  0.0946, -0.0544]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 9 0
layer name:  MLP
i,j,k 25 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3354,  0.2603,  0.5093,  ...,  0.3708,  0.1213, -0.2554]],

        [[ 0.3354,  0.2603,  0.5093,  ...,  0.3708,  0.1213, -0.2554]],

        [[ 0.3354,  0.2603,  0.5093,  ...,  0.3708,  0.1213, -0.2554]],

        [[ 0.3354,  0.2603,  0.5093,  ...,  0.3708,  0.1213, -0.2554]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1178,  0.1038,  0.1593,  ...,  0.1261,  0.0576, -0.0390]],

        [[ 0.1178,  0.1038,  0.1593,  ...,  0.1261,  0.0576, -0.0390]],

        [[ 0.1178,  0.1038,  0.1593,  ...,  0.1261,  0.0576, -0.0390]],

        [[ 0.1178,  0.1038,  0.1593,  ...,  0.1261,  0.0576, -0.0390]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 12 0
layer name:  MLP
i,j,k 25 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.3916,  0.4580,  0.3682,  ...,  0.5518,  0.0478, -0.2426]],

        [[ 0.3916,  0.4580,  0.3682,  ...,  0.5518,  0.0478, -0.2426]],

        [[ 0.3916,  0.4580,  0.3682,  ...,  0.5518,  0.0478, -0.2426]],

        [[ 0.3916,  0.4580,  0.3682,  ...,  0.5518,  0.0478, -0.2426]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1193,  0.1396,  0.1154,  ...,  0.1577,  0.0398, -0.0310]],

        [[ 0.1193,  0.1396,  0.1154,  ...,  0.1577,  0.0398, -0.0310]],

        [[ 0.1193,  0.1396,  0.1154,  ...,  0.1577,  0.0398, -0.0310]],

        [[ 0.1193,  0.1396,  0.1154,  ...,  0.1577,  0.0398, -0.0310]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 15 0
layer name:  MLP
i,j,k 25 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1912,  0.6025,  0.0886,  ...,  0.3806, -0.0726, -0.2854]],

        [[ 0.1912,  0.6025,  0.0886,  ...,  0.3806, -0.0726, -0.2854]],

        [[ 0.1912,  0.6025,  0.0886,  ...,  0.3806, -0.0726, -0.2854]],

        [[ 0.1912,  0.6025,  0.0886,  ...,  0.3806, -0.0726, -0.2854]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0837,  0.2120,  0.0538,  ...,  0.1317,  0.0116, -0.0510]],

        [[ 0.0837,  0.2120,  0.0538,  ...,  0.1317,  0.0116, -0.0510]],

        [[ 0.0837,  0.2120,  0.0538,  ...,  0.1317,  0.0116, -0.0510]],

        [[ 0.0837,  0.2120,  0.0538,  ...,  0.1317,  0.0116, -0.0510]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 18 0
layer name:  MLP
i,j,k 25 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2910,  0.2615, -0.2036,  ...,  0.4321, -0.1959, -0.0947]],

        [[ 0.2910,  0.2615, -0.2036,  ...,  0.4321, -0.1959, -0.0947]],

        [[ 0.2910,  0.2615, -0.2036,  ...,  0.4321, -0.1959, -0.0947]],

        [[ 0.2910,  0.2615, -0.2036,  ...,  0.4321, -0.1959, -0.0947]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1299,  0.1262, -0.0330,  ...,  0.1665, -0.0293,  0.0022]],

        [[ 0.1299,  0.1262, -0.0330,  ...,  0.1665, -0.0293,  0.0022]],

        [[ 0.1299,  0.1262, -0.0330,  ...,  0.1665, -0.0293,  0.0022]],

        [[ 0.1299,  0.1262, -0.0330,  ...,  0.1665, -0.0293,  0.0022]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 21 0
layer name:  MLP
i,j,k 25 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1796,  0.1526, -0.2502,  ...,  0.5273, -0.2812, -0.0297]],

        [[ 0.1796,  0.1526, -0.2502,  ...,  0.5273, -0.2812, -0.0297]],

        [[ 0.1796,  0.1526, -0.2502,  ...,  0.5273, -0.2812, -0.0297]],

        [[ 0.1796,  0.1526, -0.2502,  ...,  0.5273, -0.2812, -0.0297]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0980,  0.0957, -0.0504,  ...,  0.2108, -0.0580,  0.0238]],

        [[ 0.0980,  0.0957, -0.0504,  ...,  0.2108, -0.0580,  0.0238]],

        [[ 0.0980,  0.0957, -0.0504,  ...,  0.2108, -0.0580,  0.0238]],

        [[ 0.0980,  0.0957, -0.0504,  ...,  0.2108, -0.0580,  0.0238]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 24 0
layer name:  MLP
i,j,k 25 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1782, -0.1635, -0.3931,  ...,  0.9868, -0.8379,  0.0106]],

        [[ 0.1782, -0.1635, -0.3931,  ...,  0.9868, -0.8379,  0.0106]],

        [[ 0.1782, -0.1635, -0.3931,  ...,  0.9868, -0.8379,  0.0106]],

        [[ 0.1782, -0.1635, -0.3931,  ...,  0.9868, -0.8379,  0.0106]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1094, -0.0208, -0.1111,  ...,  0.3923, -0.2720,  0.0402]],

        [[ 0.1094, -0.0208, -0.1111,  ...,  0.3923, -0.2720,  0.0402]],

        [[ 0.1094, -0.0208, -0.1111,  ...,  0.3923, -0.2720,  0.0402]],

        [[ 0.1094, -0.0208, -0.1111,  ...,  0.3923, -0.2720,  0.0402]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 27 0
layer name:  MLP
i,j,k 25 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1699, -0.7725, -0.3521,  ...,  0.4329, -0.6567, -0.3367]],

        [[ 0.1699, -0.7725, -0.3521,  ...,  0.4329, -0.6567, -0.3367]],

        [[ 0.1699, -0.7725, -0.3521,  ...,  0.4329, -0.6567, -0.3367]],

        [[ 0.1699, -0.7725, -0.3521,  ...,  0.4329, -0.6567, -0.3367]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.1133, -0.2354, -0.1042,  ...,  0.2119, -0.2222, -0.0934]],

        [[ 0.1133, -0.2354, -0.1042,  ...,  0.2119, -0.2222, -0.0934]],

        [[ 0.1133, -0.2354, -0.1042,  ...,  0.2119, -0.2222, -0.0934]],

        [[ 0.1133, -0.2354, -0.1042,  ...,  0.2119, -0.2222, -0.0934]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 30 0
layer name:  MLP
i,j,k 25 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0223,  0.0979, -0.4863,  ...,  0.5127, -0.6821, -0.0911]],

        [[-0.0223,  0.0979, -0.4863,  ...,  0.5127, -0.6821, -0.0911]],

        [[-0.0223,  0.0979, -0.4863,  ...,  0.5127, -0.6821, -0.0911]],

        [[-0.0223,  0.0979, -0.4863,  ...,  0.5127, -0.6821, -0.0911]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0393,  0.1533, -0.1748,  ...,  0.2681, -0.2632,  0.0063]],

        [[ 0.0393,  0.1533, -0.1748,  ...,  0.2681, -0.2632,  0.0063]],

        [[ 0.0393,  0.1533, -0.1748,  ...,  0.2681, -0.2632,  0.0063]],

        [[ 0.0393,  0.1533, -0.1748,  ...,  0.2681, -0.2632,  0.0063]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 33 0
layer name:  MLP
i,j,k 25 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0517,  0.0397, -0.1476,  ...,  0.2612, -0.5405,  0.2537]],

        [[-0.0517,  0.0397, -0.1476,  ...,  0.2612, -0.5405,  0.2537]],

        [[-0.0517,  0.0397, -0.1476,  ...,  0.2612, -0.5405,  0.2537]],

        [[-0.0517,  0.0397, -0.1476,  ...,  0.2612, -0.5405,  0.2537]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 25 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0249,  0.1172, -0.0247,  ...,  0.1768, -0.2434,  0.1667]],

        [[ 0.0249,  0.1172, -0.0247,  ...,  0.1768, -0.2434,  0.1667]],

        [[ 0.0249,  0.1172, -0.0247,  ...,  0.1768, -0.2434,  0.1667]],

        [[ 0.0249,  0.1172, -0.0247,  ...,  0.1768, -0.2434,  0.1667]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 25 36 0
layer name:  MLP
i,j,k 25 37 0
layer name:  OutputEmbed
i,j,k 26 0 0
layer name:  InputEmbed
i,j,k 26 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1182,  0.2080,  0.1306,  ...,  0.1365, -0.1776,  0.3999]],

        [[-0.1182,  0.2080,  0.1306,  ...,  0.1365, -0.1776,  0.3999]],

        [[-0.1182,  0.2080,  0.1306,  ...,  0.1365, -0.1776,  0.3999]],

        [[-0.1182,  0.2080,  0.1306,  ...,  0.1365, -0.1776,  0.3999]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-6.1401e-02, -4.6082e-03, -3.4161e-03,  ...,  7.6294e-05,
          -8.2886e-02,  1.0785e-01]],

        [[-6.1401e-02, -4.6082e-03, -3.4161e-03,  ...,  7.6294e-05,
          -8.2886e-02,  1.0785e-01]],

        [[-6.1401e-02, -4.6082e-03, -3.4161e-03,  ...,  7.6294e-05,
          -8.2886e-02,  1.0785e-01]],

        [[-6.1401e-02, -4.6082e-03, -3.4161e-03,  ...,  7.6294e-05,
          -8.2886e-02,  1.0785e-01]]], device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 3 0
layer name:  MLP
i,j,k 26 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1510,  0.2062,  0.0992,  ...,  0.0770, -0.1771,  0.4717]],

        [[-0.1510,  0.2062,  0.0992,  ...,  0.0770, -0.1771,  0.4717]],

        [[-0.1510,  0.2062,  0.0992,  ...,  0.0770, -0.1771,  0.4717]],

        [[-0.1510,  0.2062,  0.0992,  ...,  0.0770, -0.1771,  0.4717]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0662,  0.0029, -0.0110,  ..., -0.0182, -0.0689,  0.0968]],

        [[-0.0662,  0.0029, -0.0110,  ..., -0.0182, -0.0689,  0.0968]],

        [[-0.0662,  0.0029, -0.0110,  ..., -0.0182, -0.0689,  0.0968]],

        [[-0.0662,  0.0029, -0.0110,  ..., -0.0182, -0.0689,  0.0968]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 6 0
layer name:  MLP
i,j,k 26 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0836,  0.2529,  0.0986,  ...,  0.0594, -0.3108,  0.6938]],

        [[-0.0836,  0.2529,  0.0986,  ...,  0.0594, -0.3108,  0.6938]],

        [[-0.0836,  0.2529,  0.0986,  ...,  0.0594, -0.3108,  0.6938]],

        [[-0.0836,  0.2529,  0.0986,  ...,  0.0594, -0.3108,  0.6938]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0473,  0.0108, -0.0150,  ..., -0.0235, -0.0799,  0.0914]],

        [[-0.0473,  0.0108, -0.0150,  ..., -0.0235, -0.0799,  0.0914]],

        [[-0.0473,  0.0108, -0.0150,  ..., -0.0235, -0.0799,  0.0914]],

        [[-0.0473,  0.0108, -0.0150,  ..., -0.0235, -0.0799,  0.0914]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 9 0
layer name:  MLP
i,j,k 26 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0357,  0.1411,  0.0540,  ...,  0.1151, -0.2388,  0.4832]],

        [[-0.0357,  0.1411,  0.0540,  ...,  0.1151, -0.2388,  0.4832]],

        [[-0.0357,  0.1411,  0.0540,  ...,  0.1151, -0.2388,  0.4832]],

        [[-0.0357,  0.1411,  0.0540,  ...,  0.1151, -0.2388,  0.4832]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0393, -0.0041, -0.0229,  ..., -0.0124, -0.0726,  0.0594]],

        [[-0.0393, -0.0041, -0.0229,  ..., -0.0124, -0.0726,  0.0594]],

        [[-0.0393, -0.0041, -0.0229,  ..., -0.0124, -0.0726,  0.0594]],

        [[-0.0393, -0.0041, -0.0229,  ..., -0.0124, -0.0726,  0.0594]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 12 0
layer name:  MLP
i,j,k 26 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0523,  0.2131, -0.1092,  ...,  0.2834, -0.3008,  0.4517]],

        [[ 0.0523,  0.2131, -0.1092,  ...,  0.2834, -0.3008,  0.4517]],

        [[ 0.0523,  0.2131, -0.1092,  ...,  0.2834, -0.3008,  0.4517]],

        [[ 0.0523,  0.2131, -0.1092,  ...,  0.2834, -0.3008,  0.4517]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0246,  0.0026, -0.0488,  ...,  0.0120, -0.0789,  0.0454]],

        [[-0.0246,  0.0026, -0.0488,  ...,  0.0120, -0.0789,  0.0454]],

        [[-0.0246,  0.0026, -0.0488,  ...,  0.0120, -0.0789,  0.0454]],

        [[-0.0246,  0.0026, -0.0488,  ...,  0.0120, -0.0789,  0.0454]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 15 0
layer name:  MLP
i,j,k 26 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1041,  0.2427, -0.2522,  ..., -0.0303, -0.3806,  0.1948]],

        [[-0.1041,  0.2427, -0.2522,  ..., -0.0303, -0.3806,  0.1948]],

        [[-0.1041,  0.2427, -0.2522,  ..., -0.0303, -0.3806,  0.1948]],

        [[-0.1041,  0.2427, -0.2522,  ..., -0.0303, -0.3806,  0.1948]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0518,  0.0158, -0.0751,  ..., -0.0374, -0.0962,  0.0040]],

        [[-0.0518,  0.0158, -0.0751,  ..., -0.0374, -0.0962,  0.0040]],

        [[-0.0518,  0.0158, -0.0751,  ..., -0.0374, -0.0962,  0.0040]],

        [[-0.0518,  0.0158, -0.0751,  ..., -0.0374, -0.0962,  0.0040]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 18 0
layer name:  MLP
i,j,k 26 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0906,  0.0873, -0.2644,  ..., -0.0166, -0.3113,  0.2859]],

        [[-0.0906,  0.0873, -0.2644,  ..., -0.0166, -0.3113,  0.2859]],

        [[-0.0906,  0.0873, -0.2644,  ..., -0.0166, -0.3113,  0.2859]],

        [[-0.0906,  0.0873, -0.2644,  ..., -0.0166, -0.3113,  0.2859]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0506, -0.0094, -0.0856,  ..., -0.0350, -0.0941,  0.0336]],

        [[-0.0506, -0.0094, -0.0856,  ..., -0.0350, -0.0941,  0.0336]],

        [[-0.0506, -0.0094, -0.0856,  ..., -0.0350, -0.0941,  0.0336]],

        [[-0.0506, -0.0094, -0.0856,  ..., -0.0350, -0.0941,  0.0336]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 21 0
layer name:  MLP
i,j,k 26 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3313, -0.1440, -0.5337,  ..., -0.1362, -0.2081,  0.3088]],

        [[-0.3313, -0.1440, -0.5337,  ..., -0.1362, -0.2081,  0.3088]],

        [[-0.3313, -0.1440, -0.5337,  ..., -0.1362, -0.2081,  0.3088]],

        [[-0.3313, -0.1440, -0.5337,  ..., -0.1362, -0.2081,  0.3088]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1084, -0.0594, -0.1580,  ..., -0.0626, -0.0771,  0.0462]],

        [[-0.1084, -0.0594, -0.1580,  ..., -0.0626, -0.0771,  0.0462]],

        [[-0.1084, -0.0594, -0.1580,  ..., -0.0626, -0.0771,  0.0462]],

        [[-0.1084, -0.0594, -0.1580,  ..., -0.0626, -0.0771,  0.0462]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 24 0
layer name:  MLP
i,j,k 26 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3484, -0.6226, -0.3760,  ..., -0.2810,  0.0024,  0.1558]],

        [[-0.3484, -0.6226, -0.3760,  ..., -0.2810,  0.0024,  0.1558]],

        [[-0.3484, -0.6226, -0.3760,  ..., -0.2810,  0.0024,  0.1558]],

        [[-0.3484, -0.6226, -0.3760,  ..., -0.2810,  0.0024,  0.1558]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1230, -0.2123, -0.1313,  ..., -0.1049, -0.0256,  0.0153]],

        [[-0.1230, -0.2123, -0.1313,  ..., -0.1049, -0.0256,  0.0153]],

        [[-0.1230, -0.2123, -0.1313,  ..., -0.1049, -0.0256,  0.0153]],

        [[-0.1230, -0.2123, -0.1313,  ..., -0.1049, -0.0256,  0.0153]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 27 0
layer name:  MLP
i,j,k 26 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3052, -0.7544, -0.5190,  ..., -0.5400,  0.0888, -0.0262]],

        [[-0.3052, -0.7544, -0.5190,  ..., -0.5400,  0.0888, -0.0262]],

        [[-0.3052, -0.7544, -0.5190,  ..., -0.5400,  0.0888, -0.0262]],

        [[-0.3052, -0.7544, -0.5190,  ..., -0.5400,  0.0888, -0.0262]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1234, -0.2493, -0.2024,  ..., -0.2063,  0.0076, -0.0325]],

        [[-0.1234, -0.2493, -0.2024,  ..., -0.2063,  0.0076, -0.0325]],

        [[-0.1234, -0.2493, -0.2024,  ..., -0.2063,  0.0076, -0.0325]],

        [[-0.1234, -0.2493, -0.2024,  ..., -0.2063,  0.0076, -0.0325]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 30 0
layer name:  MLP
i,j,k 26 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4373, -1.1113, -0.0094,  ..., -0.6328,  0.1984,  0.1083]],

        [[-0.4373, -1.1113, -0.0094,  ..., -0.6328,  0.1984,  0.1083]],

        [[-0.4373, -1.1113, -0.0094,  ..., -0.6328,  0.1984,  0.1083]],

        [[-0.4373, -1.1113, -0.0094,  ..., -0.6328,  0.1984,  0.1083]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2139, -0.7275, -0.0229,  ..., -0.3169,  0.0729,  0.0310]],

        [[-0.2139, -0.7275, -0.0229,  ..., -0.3169,  0.0729,  0.0310]],

        [[-0.2139, -0.7275, -0.0229,  ..., -0.3169,  0.0729,  0.0310]],

        [[-0.2139, -0.7275, -0.0229,  ..., -0.3169,  0.0729,  0.0310]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 33 0
layer name:  MLP
i,j,k 26 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4297, -2.0684,  0.3633,  ..., -0.8682,  0.0543, -0.0363]],

        [[-0.4297, -2.0684,  0.3633,  ..., -0.8682,  0.0543, -0.0363]],

        [[-0.4297, -2.0684,  0.3633,  ..., -0.8682,  0.0543, -0.0363]],

        [[-0.4297, -2.0684,  0.3633,  ..., -0.8682,  0.0543, -0.0363]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 26 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2837, -1.2988,  0.2478,  ..., -0.5684,  0.0216, -0.0253]],

        [[-0.2837, -1.2988,  0.2478,  ..., -0.5684,  0.0216, -0.0253]],

        [[-0.2837, -1.2988,  0.2478,  ..., -0.5684,  0.0216, -0.0253]],

        [[-0.2837, -1.2988,  0.2478,  ..., -0.5684,  0.0216, -0.0253]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 26 36 0
layer name:  MLP
i,j,k 26 37 0
layer name:  OutputEmbed
i,j,k 27 0 0
layer name:  InputEmbed
i,j,k 27 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1627,  0.2250,  0.3103,  ..., -0.3071, -0.0026,  0.5220]],

        [[-0.1627,  0.2250,  0.3103,  ..., -0.3071, -0.0026,  0.5220]],

        [[-0.1627,  0.2250,  0.3103,  ..., -0.3071, -0.0026,  0.5220]],

        [[-0.1627,  0.2250,  0.3103,  ..., -0.3071, -0.0026,  0.5220]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0760, -0.0117,  0.0267,  ..., -0.1104, -0.0495,  0.1230]],

        [[-0.0760, -0.0117,  0.0267,  ..., -0.1104, -0.0495,  0.1230]],

        [[-0.0760, -0.0117,  0.0267,  ..., -0.1104, -0.0495,  0.1230]],

        [[-0.0760, -0.0117,  0.0267,  ..., -0.1104, -0.0495,  0.1230]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 3 0
layer name:  MLP
i,j,k 27 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1864,  0.2054,  0.2656,  ..., -0.2581, -0.0399,  0.6001]],

        [[-0.1864,  0.2054,  0.2656,  ..., -0.2581, -0.0399,  0.6001]],

        [[-0.1864,  0.2054,  0.2656,  ..., -0.2581, -0.0399,  0.6001]],

        [[-0.1864,  0.2054,  0.2656,  ..., -0.2581, -0.0399,  0.6001]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0793, -0.0080,  0.0135,  ..., -0.0963, -0.0521,  0.1097]],

        [[-0.0793, -0.0080,  0.0135,  ..., -0.0963, -0.0521,  0.1097]],

        [[-0.0793, -0.0080,  0.0135,  ..., -0.0963, -0.0521,  0.1097]],

        [[-0.0793, -0.0080,  0.0135,  ..., -0.0963, -0.0521,  0.1097]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 6 0
layer name:  MLP
i,j,k 27 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0092,  0.2659,  0.2507,  ..., -0.2578, -0.1771,  0.7700]],

        [[ 0.0092,  0.2659,  0.2507,  ..., -0.2578, -0.1771,  0.7700]],

        [[ 0.0092,  0.2659,  0.2507,  ..., -0.2578, -0.1771,  0.7700]],

        [[ 0.0092,  0.2659,  0.2507,  ..., -0.2578, -0.1771,  0.7700]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0399,  0.0020, -0.0002,  ..., -0.0799, -0.0668,  0.0881]],

        [[-0.0399,  0.0020, -0.0002,  ..., -0.0799, -0.0668,  0.0881]],

        [[-0.0399,  0.0020, -0.0002,  ..., -0.0799, -0.0668,  0.0881]],

        [[-0.0399,  0.0020, -0.0002,  ..., -0.0799, -0.0668,  0.0881]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 9 0
layer name:  MLP
i,j,k 27 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0278,  0.1198,  0.2524,  ..., -0.0686, -0.0631,  0.5063]],

        [[-0.0278,  0.1198,  0.2524,  ..., -0.0686, -0.0631,  0.5063]],

        [[-0.0278,  0.1198,  0.2524,  ..., -0.0686, -0.0631,  0.5063]],

        [[-0.0278,  0.1198,  0.2524,  ..., -0.0686, -0.0631,  0.5063]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0458, -0.0181, -0.0002,  ..., -0.0524, -0.0509,  0.0486]],

        [[-0.0458, -0.0181, -0.0002,  ..., -0.0524, -0.0509,  0.0486]],

        [[-0.0458, -0.0181, -0.0002,  ..., -0.0524, -0.0509,  0.0486]],

        [[-0.0458, -0.0181, -0.0002,  ..., -0.0524, -0.0509,  0.0486]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 12 0
layer name:  MLP
i,j,k 27 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0291,  0.0710,  0.0814,  ...,  0.0702, -0.2352,  0.3796]],

        [[ 0.0291,  0.0710,  0.0814,  ...,  0.0702, -0.2352,  0.3796]],

        [[ 0.0291,  0.0710,  0.0814,  ...,  0.0702, -0.2352,  0.3796]],

        [[ 0.0291,  0.0710,  0.0814,  ...,  0.0702, -0.2352,  0.3796]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0372, -0.0296, -0.0287,  ..., -0.0307, -0.0748,  0.0200]],

        [[-0.0372, -0.0296, -0.0287,  ..., -0.0307, -0.0748,  0.0200]],

        [[-0.0372, -0.0296, -0.0287,  ..., -0.0307, -0.0748,  0.0200]],

        [[-0.0372, -0.0296, -0.0287,  ..., -0.0307, -0.0748,  0.0200]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 15 0
layer name:  MLP
i,j,k 27 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-4.1626e-02,  1.4148e-01,  6.7627e-02,  ...,  7.7438e-04,
          -8.8623e-01,  3.8757e-02]],

        [[-4.1626e-02,  1.4148e-01,  6.7627e-02,  ...,  7.7438e-04,
          -8.8623e-01,  3.8757e-02]],

        [[-4.1626e-02,  1.4148e-01,  6.7627e-02,  ...,  7.7438e-04,
          -8.8623e-01,  3.8757e-02]],

        [[-4.1626e-02,  1.4148e-01,  6.7627e-02,  ...,  7.7438e-04,
          -8.8623e-01,  3.8757e-02]]], device='cuda:0', dtype=torch.float16)
i,j,k 27 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0486, -0.0122, -0.0292,  ..., -0.0405, -0.1880, -0.0330]],

        [[-0.0486, -0.0122, -0.0292,  ..., -0.0405, -0.1880, -0.0330]],

        [[-0.0486, -0.0122, -0.0292,  ..., -0.0405, -0.1880, -0.0330]],

        [[-0.0486, -0.0122, -0.0292,  ..., -0.0405, -0.1880, -0.0330]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 18 0
layer name:  MLP
i,j,k 27 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0687,  0.2251, -0.2854,  ...,  0.2242, -0.6235, -0.2078]],

        [[-0.0687,  0.2251, -0.2854,  ...,  0.2242, -0.6235, -0.2078]],

        [[-0.0687,  0.2251, -0.2854,  ...,  0.2242, -0.6235, -0.2078]],

        [[-0.0687,  0.2251, -0.2854,  ...,  0.2242, -0.6235, -0.2078]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0539,  0.0112, -0.0966,  ...,  0.0036, -0.1619, -0.0826]],

        [[-0.0539,  0.0112, -0.0966,  ...,  0.0036, -0.1619, -0.0826]],

        [[-0.0539,  0.0112, -0.0966,  ...,  0.0036, -0.1619, -0.0826]],

        [[-0.0539,  0.0112, -0.0966,  ...,  0.0036, -0.1619, -0.0826]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 21 0
layer name:  MLP
i,j,k 27 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1144, -0.0183, -0.4070,  ...,  0.3379, -0.5908,  0.1620]],

        [[ 0.1144, -0.0183, -0.4070,  ...,  0.3379, -0.5908,  0.1620]],

        [[ 0.1144, -0.0183, -0.4070,  ...,  0.3379, -0.5908,  0.1620]],

        [[ 0.1144, -0.0183, -0.4070,  ...,  0.3379, -0.5908,  0.1620]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0102, -0.0380, -0.1344,  ...,  0.0390, -0.1727, -0.0007]],

        [[-0.0102, -0.0380, -0.1344,  ...,  0.0390, -0.1727, -0.0007]],

        [[-0.0102, -0.0380, -0.1344,  ...,  0.0390, -0.1727, -0.0007]],

        [[-0.0102, -0.0380, -0.1344,  ...,  0.0390, -0.1727, -0.0007]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 24 0
layer name:  MLP
i,j,k 27 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5327, -0.3630, -0.4380,  ...,  0.3765, -0.6479,  0.1213]],

        [[-0.5327, -0.3630, -0.4380,  ...,  0.3765, -0.6479,  0.1213]],

        [[-0.5327, -0.3630, -0.4380,  ...,  0.3765, -0.6479,  0.1213]],

        [[-0.5327, -0.3630, -0.4380,  ...,  0.3765, -0.6479,  0.1213]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1919, -0.1471, -0.1646,  ...,  0.0621, -0.2203, -0.0048]],

        [[-0.1919, -0.1471, -0.1646,  ...,  0.0621, -0.2203, -0.0048]],

        [[-0.1919, -0.1471, -0.1646,  ...,  0.0621, -0.2203, -0.0048]],

        [[-0.1919, -0.1471, -0.1646,  ...,  0.0621, -0.2203, -0.0048]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 27 0
layer name:  MLP
i,j,k 27 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2764, -0.3013, -0.2417,  ...,  0.4680, -0.1051, -0.2202]],

        [[-0.2764, -0.3013, -0.2417,  ...,  0.4680, -0.1051, -0.2202]],

        [[-0.2764, -0.3013, -0.2417,  ...,  0.4680, -0.1051, -0.2202]],

        [[-0.2764, -0.3013, -0.2417,  ...,  0.4680, -0.1051, -0.2202]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1318, -0.1178, -0.1251,  ...,  0.1277, -0.0731, -0.1145]],

        [[-0.1318, -0.1178, -0.1251,  ...,  0.1277, -0.0731, -0.1145]],

        [[-0.1318, -0.1178, -0.1251,  ...,  0.1277, -0.0731, -0.1145]],

        [[-0.1318, -0.1178, -0.1251,  ...,  0.1277, -0.0731, -0.1145]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 30 0
layer name:  MLP
i,j,k 27 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4355, -0.4016, -0.2976,  ...,  0.5732, -0.1831, -0.3184]],

        [[-0.4355, -0.4016, -0.2976,  ...,  0.5732, -0.1831, -0.3184]],

        [[-0.4355, -0.4016, -0.2976,  ...,  0.5732, -0.1831, -0.3184]],

        [[-0.4355, -0.4016, -0.2976,  ...,  0.5732, -0.1831, -0.3184]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2417, -0.2739, -0.1851,  ...,  0.2512, -0.1289, -0.1846]],

        [[-0.2417, -0.2739, -0.1851,  ...,  0.2512, -0.1289, -0.1846]],

        [[-0.2417, -0.2739, -0.1851,  ...,  0.2512, -0.1289, -0.1846]],

        [[-0.2417, -0.2739, -0.1851,  ...,  0.2512, -0.1289, -0.1846]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 33 0
layer name:  MLP
i,j,k 27 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4507, -1.6055, -0.0928,  ...,  0.4316, -0.1726, -0.3540]],

        [[-0.4507, -1.6055, -0.0928,  ...,  0.4316, -0.1726, -0.3540]],

        [[-0.4507, -1.6055, -0.0928,  ...,  0.4316, -0.1726, -0.3540]],

        [[-0.4507, -1.6055, -0.0928,  ...,  0.4316, -0.1726, -0.3540]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 27 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2971, -0.9390, -0.0846,  ...,  0.2336, -0.1516, -0.2279]],

        [[-0.2971, -0.9390, -0.0846,  ...,  0.2336, -0.1516, -0.2279]],

        [[-0.2971, -0.9390, -0.0846,  ...,  0.2336, -0.1516, -0.2279]],

        [[-0.2971, -0.9390, -0.0846,  ...,  0.2336, -0.1516, -0.2279]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 27 36 0
layer name:  MLP
i,j,k 27 37 0
layer name:  OutputEmbed
i,j,k 28 0 0
layer name:  InputEmbed
i,j,k 28 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3418, -0.2991,  0.0659,  ...,  0.1107,  0.1531,  0.0112]],

        [[-0.3418, -0.2991,  0.0659,  ...,  0.1107,  0.1531,  0.0112]],

        [[-0.3418, -0.2991,  0.0659,  ...,  0.1107,  0.1531,  0.0112]],

        [[-0.3418, -0.2991,  0.0659,  ...,  0.1107,  0.1531,  0.0112]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0784, -0.0886, -0.0002,  ...,  0.0111,  0.0113,  0.0004]],

        [[-0.0784, -0.0886, -0.0002,  ...,  0.0111,  0.0113,  0.0004]],

        [[-0.0784, -0.0886, -0.0002,  ...,  0.0111,  0.0113,  0.0004]],

        [[-0.0784, -0.0886, -0.0002,  ...,  0.0111,  0.0113,  0.0004]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 3 0
layer name:  MLP
i,j,k 28 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3042, -0.3223,  0.2074,  ...,  0.0064,  0.0331, -0.0511]],

        [[-0.3042, -0.3223,  0.2074,  ...,  0.0064,  0.0331, -0.0511]],

        [[-0.3042, -0.3223,  0.2074,  ...,  0.0064,  0.0331, -0.0511]],

        [[-0.3042, -0.3223,  0.2074,  ...,  0.0064,  0.0331, -0.0511]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0786, -0.0622,  0.0354,  ..., -0.0122, -0.0100, -0.0134]],

        [[-0.0786, -0.0622,  0.0354,  ..., -0.0122, -0.0100, -0.0134]],

        [[-0.0786, -0.0622,  0.0354,  ..., -0.0122, -0.0100, -0.0134]],

        [[-0.0786, -0.0622,  0.0354,  ..., -0.0122, -0.0100, -0.0134]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 6 0
layer name:  MLP
i,j,k 28 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2087, -0.1831,  0.3220,  ..., -0.2815, -0.1479, -0.0953]],

        [[-0.2087, -0.1831,  0.3220,  ..., -0.2815, -0.1479, -0.0953]],

        [[-0.2087, -0.1831,  0.3220,  ..., -0.2815, -0.1479, -0.0953]],

        [[-0.2087, -0.1831,  0.3220,  ..., -0.2815, -0.1479, -0.0953]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0502, -0.0441,  0.0491,  ..., -0.0604, -0.0364, -0.0236]],

        [[-0.0502, -0.0441,  0.0491,  ..., -0.0604, -0.0364, -0.0236]],

        [[-0.0502, -0.0441,  0.0491,  ..., -0.0604, -0.0364, -0.0236]],

        [[-0.0502, -0.0441,  0.0491,  ..., -0.0604, -0.0364, -0.0236]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 9 0
layer name:  MLP
i,j,k 28 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1964, -0.2311,  0.3992,  ..., -0.2754, -0.1371, -0.2715]],

        [[-0.1964, -0.2311,  0.3992,  ..., -0.2754, -0.1371, -0.2715]],

        [[-0.1964, -0.2311,  0.3992,  ..., -0.2754, -0.1371, -0.2715]],

        [[-0.1964, -0.2311,  0.3992,  ..., -0.2754, -0.1371, -0.2715]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0512, -0.0573,  0.0673,  ..., -0.0665, -0.0370, -0.0646]],

        [[-0.0512, -0.0573,  0.0673,  ..., -0.0665, -0.0370, -0.0646]],

        [[-0.0512, -0.0573,  0.0673,  ..., -0.0665, -0.0370, -0.0646]],

        [[-0.0512, -0.0573,  0.0673,  ..., -0.0665, -0.0370, -0.0646]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 12 0
layer name:  MLP
i,j,k 28 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2700, -0.2260,  0.4336,  ..., -0.3394, -0.2037, -0.3330]],

        [[-0.2700, -0.2260,  0.4336,  ..., -0.3394, -0.2037, -0.3330]],

        [[-0.2700, -0.2260,  0.4336,  ..., -0.3394, -0.2037, -0.3330]],

        [[-0.2700, -0.2260,  0.4336,  ..., -0.3394, -0.2037, -0.3330]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0590, -0.0506,  0.0679,  ..., -0.0707, -0.0463, -0.0728]],

        [[-0.0590, -0.0506,  0.0679,  ..., -0.0707, -0.0463, -0.0728]],

        [[-0.0590, -0.0506,  0.0679,  ..., -0.0707, -0.0463, -0.0728]],

        [[-0.0590, -0.0506,  0.0679,  ..., -0.0707, -0.0463, -0.0728]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 15 0
layer name:  MLP
i,j,k 28 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3604, -0.1272,  0.5283,  ..., -0.4536, -0.3127, -0.4409]],

        [[-0.3604, -0.1272,  0.5283,  ..., -0.4536, -0.3127, -0.4409]],

        [[-0.3604, -0.1272,  0.5283,  ..., -0.4536, -0.3127, -0.4409]],

        [[-0.3604, -0.1272,  0.5283,  ..., -0.4536, -0.3127, -0.4409]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0859, -0.0345,  0.0947,  ..., -0.0986, -0.0701, -0.1016]],

        [[-0.0859, -0.0345,  0.0947,  ..., -0.0986, -0.0701, -0.1016]],

        [[-0.0859, -0.0345,  0.0947,  ..., -0.0986, -0.0701, -0.1016]],

        [[-0.0859, -0.0345,  0.0947,  ..., -0.0986, -0.0701, -0.1016]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 18 0
layer name:  MLP
i,j,k 28 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4060, -0.0813,  0.3743,  ..., -0.5776, -0.1985, -0.5693]],

        [[-0.4060, -0.0813,  0.3743,  ..., -0.5776, -0.1985, -0.5693]],

        [[-0.4060, -0.0813,  0.3743,  ..., -0.5776, -0.1985, -0.5693]],

        [[-0.4060, -0.0813,  0.3743,  ..., -0.5776, -0.1985, -0.5693]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1025, -0.0251,  0.0740,  ..., -0.1343, -0.0513, -0.1421]],

        [[-0.1025, -0.0251,  0.0740,  ..., -0.1343, -0.0513, -0.1421]],

        [[-0.1025, -0.0251,  0.0740,  ..., -0.1343, -0.0513, -0.1421]],

        [[-0.1025, -0.0251,  0.0740,  ..., -0.1343, -0.0513, -0.1421]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 21 0
layer name:  MLP
i,j,k 28 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1866,  0.2649,  0.3386,  ..., -0.5947, -0.1115, -0.5820]],

        [[-0.1866,  0.2649,  0.3386,  ..., -0.5947, -0.1115, -0.5820]],

        [[-0.1866,  0.2649,  0.3386,  ..., -0.5947, -0.1115, -0.5820]],

        [[-0.1866,  0.2649,  0.3386,  ..., -0.5947, -0.1115, -0.5820]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0532,  0.0672,  0.0791,  ..., -0.1565, -0.0341, -0.1541]],

        [[-0.0532,  0.0672,  0.0791,  ..., -0.1565, -0.0341, -0.1541]],

        [[-0.0532,  0.0672,  0.0791,  ..., -0.1565, -0.0341, -0.1541]],

        [[-0.0532,  0.0672,  0.0791,  ..., -0.1565, -0.0341, -0.1541]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 24 0
layer name:  MLP
i,j,k 28 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5312,  0.0114,  0.1805,  ..., -0.8032, -0.3528, -0.5703]],

        [[-0.5312,  0.0114,  0.1805,  ..., -0.8032, -0.3528, -0.5703]],

        [[-0.5312,  0.0114,  0.1805,  ..., -0.8032, -0.3528, -0.5703]],

        [[-0.5312,  0.0114,  0.1805,  ..., -0.8032, -0.3528, -0.5703]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1763,  0.0073,  0.0525,  ..., -0.2566, -0.1157, -0.1907]],

        [[-0.1763,  0.0073,  0.0525,  ..., -0.2566, -0.1157, -0.1907]],

        [[-0.1763,  0.0073,  0.0525,  ..., -0.2566, -0.1157, -0.1907]],

        [[-0.1763,  0.0073,  0.0525,  ..., -0.2566, -0.1157, -0.1907]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 27 0
layer name:  MLP
i,j,k 28 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4043,  0.6353,  0.1632,  ..., -0.7803,  0.0187, -0.7642]],

        [[-0.4043,  0.6353,  0.1632,  ..., -0.7803,  0.0187, -0.7642]],

        [[-0.4043,  0.6353,  0.1632,  ..., -0.7803,  0.0187, -0.7642]],

        [[-0.4043,  0.6353,  0.1632,  ..., -0.7803,  0.0187, -0.7642]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1564,  0.2612,  0.0597,  ..., -0.3052,  0.0050, -0.2952]],

        [[-0.1564,  0.2612,  0.0597,  ..., -0.3052,  0.0050, -0.2952]],

        [[-0.1564,  0.2612,  0.0597,  ..., -0.3052,  0.0050, -0.2952]],

        [[-0.1564,  0.2612,  0.0597,  ..., -0.3052,  0.0050, -0.2952]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 30 0
layer name:  MLP
i,j,k 28 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3110,  0.8223,  0.3320,  ..., -0.4873, -0.0031, -0.8384]],

        [[-0.3110,  0.8223,  0.3320,  ..., -0.4873, -0.0031, -0.8384]],

        [[-0.3110,  0.8223,  0.3320,  ..., -0.4873, -0.0031, -0.8384]],

        [[-0.3110,  0.8223,  0.3320,  ..., -0.4873, -0.0031, -0.8384]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1536,  0.6899,  0.1797,  ..., -0.2583, -0.0032, -0.4197]],

        [[-0.1536,  0.6899,  0.1797,  ..., -0.2583, -0.0032, -0.4197]],

        [[-0.1536,  0.6899,  0.1797,  ..., -0.2583, -0.0032, -0.4197]],

        [[-0.1536,  0.6899,  0.1797,  ..., -0.2583, -0.0032, -0.4197]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 33 0
layer name:  MLP
i,j,k 28 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2156,  1.3789,  0.3855,  ..., -0.7163, -0.1469, -0.8696]],

        [[-0.2156,  1.3789,  0.3855,  ..., -0.7163, -0.1469, -0.8696]],

        [[-0.2156,  1.3789,  0.3855,  ..., -0.7163, -0.1469, -0.8696]],

        [[-0.2156,  1.3789,  0.3855,  ..., -0.7163, -0.1469, -0.8696]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 28 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1168,  0.8960,  0.2539,  ..., -0.4128, -0.0958, -0.4788]],

        [[-0.1168,  0.8960,  0.2539,  ..., -0.4128, -0.0958, -0.4788]],

        [[-0.1168,  0.8960,  0.2539,  ..., -0.4128, -0.0958, -0.4788]],

        [[-0.1168,  0.8960,  0.2539,  ..., -0.4128, -0.0958, -0.4788]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 28 36 0
layer name:  MLP
i,j,k 28 37 0
layer name:  OutputEmbed
i,j,k 29 0 0
layer name:  InputEmbed
i,j,k 29 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0340,  0.0088, -0.0238,  ...,  0.2903, -0.0130, -0.3218]],

        [[-0.0340,  0.0088, -0.0238,  ...,  0.2903, -0.0130, -0.3218]],

        [[-0.0340,  0.0088, -0.0238,  ...,  0.2903, -0.0130, -0.3218]],

        [[-0.0340,  0.0088, -0.0238,  ...,  0.2903, -0.0130, -0.3218]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0035, -0.0042,  0.0038,  ...,  0.0728,  0.0017, -0.0696]],

        [[ 0.0035, -0.0042,  0.0038,  ...,  0.0728,  0.0017, -0.0696]],

        [[ 0.0035, -0.0042,  0.0038,  ...,  0.0728,  0.0017, -0.0696]],

        [[ 0.0035, -0.0042,  0.0038,  ...,  0.0728,  0.0017, -0.0696]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 3 0
layer name:  MLP
i,j,k 29 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0244, -0.0923, -0.1186,  ...,  0.2617, -0.0116, -0.2297]],

        [[-0.0244, -0.0923, -0.1186,  ...,  0.2617, -0.0116, -0.2297]],

        [[-0.0244, -0.0923, -0.1186,  ...,  0.2617, -0.0116, -0.2297]],

        [[-0.0244, -0.0923, -0.1186,  ...,  0.2617, -0.0116, -0.2297]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0052, -0.0016, -0.0164,  ...,  0.0714,  0.0049, -0.0383]],

        [[ 0.0052, -0.0016, -0.0164,  ...,  0.0714,  0.0049, -0.0383]],

        [[ 0.0052, -0.0016, -0.0164,  ...,  0.0714,  0.0049, -0.0383]],

        [[ 0.0052, -0.0016, -0.0164,  ...,  0.0714,  0.0049, -0.0383]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 6 0
layer name:  MLP
i,j,k 29 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.0678,  0.1234, -0.2252,  ...,  0.1587,  0.0901, -0.2871]],

        [[ 0.0678,  0.1234, -0.2252,  ...,  0.1587,  0.0901, -0.2871]],

        [[ 0.0678,  0.1234, -0.2252,  ...,  0.1587,  0.0901, -0.2871]],

        [[ 0.0678,  0.1234, -0.2252,  ...,  0.1587,  0.0901, -0.2871]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0248,  0.0369, -0.0304,  ...,  0.0409,  0.0257, -0.0391]],

        [[ 0.0248,  0.0369, -0.0304,  ...,  0.0409,  0.0257, -0.0391]],

        [[ 0.0248,  0.0369, -0.0304,  ...,  0.0409,  0.0257, -0.0391]],

        [[ 0.0248,  0.0369, -0.0304,  ...,  0.0409,  0.0257, -0.0391]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 9 0
layer name:  MLP
i,j,k 29 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0242,  0.1893, -0.1455,  ...,  0.0906,  0.2185, -0.3130]],

        [[-0.0242,  0.1893, -0.1455,  ...,  0.0906,  0.2185, -0.3130]],

        [[-0.0242,  0.1893, -0.1455,  ...,  0.0906,  0.2185, -0.3130]],

        [[-0.0242,  0.1893, -0.1455,  ...,  0.0906,  0.2185, -0.3130]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0071,  0.0570, -0.0163,  ...,  0.0314,  0.0553, -0.0526]],

        [[ 0.0071,  0.0570, -0.0163,  ...,  0.0314,  0.0553, -0.0526]],

        [[ 0.0071,  0.0570, -0.0163,  ...,  0.0314,  0.0553, -0.0526]],

        [[ 0.0071,  0.0570, -0.0163,  ...,  0.0314,  0.0553, -0.0526]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 12 0
layer name:  MLP
i,j,k 29 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0504,  0.0067, -0.1859,  ...,  0.0584,  0.2678, -0.2627]],

        [[-0.0504,  0.0067, -0.1859,  ...,  0.0584,  0.2678, -0.2627]],

        [[-0.0504,  0.0067, -0.1859,  ...,  0.0584,  0.2678, -0.2627]],

        [[-0.0504,  0.0067, -0.1859,  ...,  0.0584,  0.2678, -0.2627]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0032,  0.0160, -0.0216,  ...,  0.0249,  0.0655, -0.0393]],

        [[ 0.0032,  0.0160, -0.0216,  ...,  0.0249,  0.0655, -0.0393]],

        [[ 0.0032,  0.0160, -0.0216,  ...,  0.0249,  0.0655, -0.0393]],

        [[ 0.0032,  0.0160, -0.0216,  ...,  0.0249,  0.0655, -0.0393]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 15 0
layer name:  MLP
i,j,k 29 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1377, -0.0322, -0.1517,  ...,  0.0019,  0.1685, -0.2452]],

        [[-0.1377, -0.0322, -0.1517,  ...,  0.0019,  0.1685, -0.2452]],

        [[-0.1377, -0.0322, -0.1517,  ...,  0.0019,  0.1685, -0.2452]],

        [[-0.1377, -0.0322, -0.1517,  ...,  0.0019,  0.1685, -0.2452]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0173,  0.0095, -0.0174,  ...,  0.0145,  0.0501, -0.0399]],

        [[-0.0173,  0.0095, -0.0174,  ...,  0.0145,  0.0501, -0.0399]],

        [[-0.0173,  0.0095, -0.0174,  ...,  0.0145,  0.0501, -0.0399]],

        [[-0.0173,  0.0095, -0.0174,  ...,  0.0145,  0.0501, -0.0399]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 18 0
layer name:  MLP
i,j,k 29 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2164,  0.1185, -0.1969,  ..., -0.2224,  0.2668, -0.2474]],

        [[-0.2164,  0.1185, -0.1969,  ..., -0.2224,  0.2668, -0.2474]],

        [[-0.2164,  0.1185, -0.1969,  ..., -0.2224,  0.2668, -0.2474]],

        [[-0.2164,  0.1185, -0.1969,  ..., -0.2224,  0.2668, -0.2474]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0369,  0.0486, -0.0301,  ..., -0.0355,  0.0771, -0.0443]],

        [[-0.0369,  0.0486, -0.0301,  ..., -0.0355,  0.0771, -0.0443]],

        [[-0.0369,  0.0486, -0.0301,  ..., -0.0355,  0.0771, -0.0443]],

        [[-0.0369,  0.0486, -0.0301,  ..., -0.0355,  0.0771, -0.0443]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 21 0
layer name:  MLP
i,j,k 29 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0626,  0.3748, -0.1593,  ..., -0.1411,  0.2783, -0.1562]],

        [[-0.0626,  0.3748, -0.1593,  ..., -0.1411,  0.2783, -0.1562]],

        [[-0.0626,  0.3748, -0.1593,  ..., -0.1411,  0.2783, -0.1562]],

        [[-0.0626,  0.3748, -0.1593,  ..., -0.1411,  0.2783, -0.1562]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0020,  0.1262, -0.0244,  ..., -0.0212,  0.0885, -0.0246]],

        [[ 0.0020,  0.1262, -0.0244,  ..., -0.0212,  0.0885, -0.0246]],

        [[ 0.0020,  0.1262, -0.0244,  ..., -0.0212,  0.0885, -0.0246]],

        [[ 0.0020,  0.1262, -0.0244,  ..., -0.0212,  0.0885, -0.0246]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 24 0
layer name:  MLP
i,j,k 29 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2900, -0.0482, -0.3140,  ..., -0.2632, -0.0929, -0.1052]],

        [[-0.2900, -0.0482, -0.3140,  ..., -0.2632, -0.0929, -0.1052]],

        [[-0.2900, -0.0482, -0.3140,  ..., -0.2632, -0.0929, -0.1052]],

        [[-0.2900, -0.0482, -0.3140,  ..., -0.2632, -0.0929, -0.1052]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0739,  0.0114, -0.0828,  ..., -0.0663, -0.0091, -0.0161]],

        [[-0.0739,  0.0114, -0.0828,  ..., -0.0663, -0.0091, -0.0161]],

        [[-0.0739,  0.0114, -0.0828,  ..., -0.0663, -0.0091, -0.0161]],

        [[-0.0739,  0.0114, -0.0828,  ..., -0.0663, -0.0091, -0.0161]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 27 0
layer name:  MLP
i,j,k 29 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1716, -0.0385, -0.1659,  ..., -0.3757,  0.1070, -0.3611]],

        [[-0.1716, -0.0385, -0.1659,  ..., -0.3757,  0.1070, -0.3611]],

        [[-0.1716, -0.0385, -0.1659,  ..., -0.3757,  0.1070, -0.3611]],

        [[-0.1716, -0.0385, -0.1659,  ..., -0.3757,  0.1070, -0.3611]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0421,  0.0346, -0.0459,  ..., -0.1294,  0.0676, -0.1210]],

        [[-0.0421,  0.0346, -0.0459,  ..., -0.1294,  0.0676, -0.1210]],

        [[-0.0421,  0.0346, -0.0459,  ..., -0.1294,  0.0676, -0.1210]],

        [[-0.0421,  0.0346, -0.0459,  ..., -0.1294,  0.0676, -0.1210]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 30 0
layer name:  MLP
i,j,k 29 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4424,  0.6138,  0.0974,  ..., -0.1105, -0.2720, -0.5078]],

        [[-0.4424,  0.6138,  0.0974,  ..., -0.1105, -0.2720, -0.5078]],

        [[-0.4424,  0.6138,  0.0974,  ..., -0.1105, -0.2720, -0.5078]],

        [[-0.4424,  0.6138,  0.0974,  ..., -0.1105, -0.2720, -0.5078]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2061,  0.5908,  0.0861,  ..., -0.0339, -0.1265, -0.2396]],

        [[-0.2061,  0.5908,  0.0861,  ..., -0.0339, -0.1265, -0.2396]],

        [[-0.2061,  0.5908,  0.0861,  ..., -0.0339, -0.1265, -0.2396]],

        [[-0.2061,  0.5908,  0.0861,  ..., -0.0339, -0.1265, -0.2396]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 33 0
layer name:  MLP
i,j,k 29 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4302,  1.3135,  0.2266,  ..., -0.4077, -0.4333, -0.5337]],

        [[-0.4302,  1.3135,  0.2266,  ..., -0.4077, -0.4333, -0.5337]],

        [[-0.4302,  1.3135,  0.2266,  ..., -0.4077, -0.4333, -0.5337]],

        [[-0.4302,  1.3135,  0.2266,  ..., -0.4077, -0.4333, -0.5337]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 29 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2203,  0.9165,  0.1909,  ..., -0.2053, -0.2581, -0.2668]],

        [[-0.2203,  0.9165,  0.1909,  ..., -0.2053, -0.2581, -0.2668]],

        [[-0.2203,  0.9165,  0.1909,  ..., -0.2053, -0.2581, -0.2668]],

        [[-0.2203,  0.9165,  0.1909,  ..., -0.2053, -0.2581, -0.2668]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 29 36 0
layer name:  MLP
i,j,k 29 37 0
layer name:  OutputEmbed
i,j,k 30 0 0
layer name:  InputEmbed
i,j,k 30 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0640,  0.7202,  0.0444,  ..., -0.1130,  0.2457, -0.3594]],

        [[-0.0640,  0.7202,  0.0444,  ..., -0.1130,  0.2457, -0.3594]],

        [[-0.0640,  0.7202,  0.0444,  ..., -0.1130,  0.2457, -0.3594]],

        [[-0.0640,  0.7202,  0.0444,  ..., -0.1130,  0.2457, -0.3594]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0294,  0.1088, -0.0091,  ..., -0.0396,  0.0249, -0.1052]],

        [[-0.0294,  0.1088, -0.0091,  ..., -0.0396,  0.0249, -0.1052]],

        [[-0.0294,  0.1088, -0.0091,  ..., -0.0396,  0.0249, -0.1052]],

        [[-0.0294,  0.1088, -0.0091,  ..., -0.0396,  0.0249, -0.1052]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 3 0
layer name:  MLP
i,j,k 30 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1877,  0.7285, -0.0252,  ...,  0.0816,  0.3623, -0.3018]],

        [[-0.1877,  0.7285, -0.0252,  ...,  0.0816,  0.3623, -0.3018]],

        [[-0.1877,  0.7285, -0.0252,  ...,  0.0816,  0.3623, -0.3018]],

        [[-0.1877,  0.7285, -0.0252,  ...,  0.0816,  0.3623, -0.3018]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0576,  0.1091, -0.0210,  ...,  0.0011,  0.0466, -0.0822]],

        [[-0.0576,  0.1091, -0.0210,  ...,  0.0011,  0.0466, -0.0822]],

        [[-0.0576,  0.1091, -0.0210,  ...,  0.0011,  0.0466, -0.0822]],

        [[-0.0576,  0.1091, -0.0210,  ...,  0.0011,  0.0466, -0.0822]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 6 0
layer name:  MLP
i,j,k 30 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1978,  0.6958, -0.0442,  ...,  0.1160,  0.4016, -0.3638]],

        [[-0.1978,  0.6958, -0.0442,  ...,  0.1160,  0.4016, -0.3638]],

        [[-0.1978,  0.6958, -0.0442,  ...,  0.1160,  0.4016, -0.3638]],

        [[-0.1978,  0.6958, -0.0442,  ...,  0.1160,  0.4016, -0.3638]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0527,  0.1188, -0.0217,  ...,  0.0060,  0.0505, -0.0793]],

        [[-0.0527,  0.1188, -0.0217,  ...,  0.0060,  0.0505, -0.0793]],

        [[-0.0527,  0.1188, -0.0217,  ...,  0.0060,  0.0505, -0.0793]],

        [[-0.0527,  0.1188, -0.0217,  ...,  0.0060,  0.0505, -0.0793]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 9 0
layer name:  MLP
i,j,k 30 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.3149,  0.5547,  0.0744,  ...,  0.1329,  0.4043, -0.4485]],

        [[-0.3149,  0.5547,  0.0744,  ...,  0.1329,  0.4043, -0.4485]],

        [[-0.3149,  0.5547,  0.0744,  ...,  0.1329,  0.4043, -0.4485]],

        [[-0.3149,  0.5547,  0.0744,  ...,  0.1329,  0.4043, -0.4485]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0811,  0.1101,  0.0014,  ...,  0.0133,  0.0646, -0.1082]],

        [[-0.0811,  0.1101,  0.0014,  ...,  0.0133,  0.0646, -0.1082]],

        [[-0.0811,  0.1101,  0.0014,  ...,  0.0133,  0.0646, -0.1082]],

        [[-0.0811,  0.1101,  0.0014,  ...,  0.0133,  0.0646, -0.1082]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 12 0
layer name:  MLP
i,j,k 30 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.4575,  0.7046,  0.0685,  ...,  0.2394,  0.4395, -0.4099]],

        [[-0.4575,  0.7046,  0.0685,  ...,  0.2394,  0.4395, -0.4099]],

        [[-0.4575,  0.7046,  0.0685,  ...,  0.2394,  0.4395, -0.4099]],

        [[-0.4575,  0.7046,  0.0685,  ...,  0.2394,  0.4395, -0.4099]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-9.7168e-02,  1.1926e-01, -9.9182e-05,  ...,  3.0441e-02,
           6.7017e-02, -9.2163e-02]],

        [[-9.7168e-02,  1.1926e-01, -9.9182e-05,  ...,  3.0441e-02,
           6.7017e-02, -9.2163e-02]],

        [[-9.7168e-02,  1.1926e-01, -9.9182e-05,  ...,  3.0441e-02,
           6.7017e-02, -9.2163e-02]],

        [[-9.7168e-02,  1.1926e-01, -9.9182e-05,  ...,  3.0441e-02,
           6.7017e-02, -9.2163e-02]]], device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 15 0
layer name:  MLP
i,j,k 30 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5601,  0.4768,  0.0522,  ...,  0.0850,  0.2925, -0.4829]],

        [[-0.5601,  0.4768,  0.0522,  ...,  0.0850,  0.2925, -0.4829]],

        [[-0.5601,  0.4768,  0.0522,  ...,  0.0850,  0.2925, -0.4829]],

        [[-0.5601,  0.4768,  0.0522,  ...,  0.0850,  0.2925, -0.4829]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1345,  0.0977, -0.0015,  ...,  0.0050,  0.0480, -0.1165]],

        [[-0.1345,  0.0977, -0.0015,  ...,  0.0050,  0.0480, -0.1165]],

        [[-0.1345,  0.0977, -0.0015,  ...,  0.0050,  0.0480, -0.1165]],

        [[-0.1345,  0.0977, -0.0015,  ...,  0.0050,  0.0480, -0.1165]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 18 0
layer name:  MLP
i,j,k 30 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6157,  0.5605,  0.1432,  ..., -0.0581,  0.3069, -0.5459]],

        [[-0.6157,  0.5605,  0.1432,  ..., -0.0581,  0.3069, -0.5459]],

        [[-0.6157,  0.5605,  0.1432,  ..., -0.0581,  0.3069, -0.5459]],

        [[-0.6157,  0.5605,  0.1432,  ..., -0.0581,  0.3069, -0.5459]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1602,  0.1326,  0.0216,  ..., -0.0245,  0.0590, -0.1447]],

        [[-0.1602,  0.1326,  0.0216,  ..., -0.0245,  0.0590, -0.1447]],

        [[-0.1602,  0.1326,  0.0216,  ..., -0.0245,  0.0590, -0.1447]],

        [[-0.1602,  0.1326,  0.0216,  ..., -0.0245,  0.0590, -0.1447]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 21 0
layer name:  MLP
i,j,k 30 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5073,  0.5762,  0.1046,  ..., -0.1176,  0.0884, -0.3562]],

        [[-0.5073,  0.5762,  0.1046,  ..., -0.1176,  0.0884, -0.3562]],

        [[-0.5073,  0.5762,  0.1046,  ..., -0.1176,  0.0884, -0.3562]],

        [[-0.5073,  0.5762,  0.1046,  ..., -0.1176,  0.0884, -0.3562]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1381,  0.1488,  0.0188,  ..., -0.0399,  0.0129, -0.1002]],

        [[-0.1381,  0.1488,  0.0188,  ..., -0.0399,  0.0129, -0.1002]],

        [[-0.1381,  0.1488,  0.0188,  ..., -0.0399,  0.0129, -0.1002]],

        [[-0.1381,  0.1488,  0.0188,  ..., -0.0399,  0.0129, -0.1002]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 24 0
layer name:  MLP
i,j,k 30 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5728,  0.0371,  0.0194,  ..., -0.2179, -0.3108, -0.3333]],

        [[-0.5728,  0.0371,  0.0194,  ..., -0.2179, -0.3108, -0.3333]],

        [[-0.5728,  0.0371,  0.0194,  ..., -0.2179, -0.3108, -0.3333]],

        [[-0.5728,  0.0371,  0.0194,  ..., -0.2179, -0.3108, -0.3333]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.1809,  0.0157,  0.0006,  ..., -0.0728, -0.0978, -0.1091]],

        [[-0.1809,  0.0157,  0.0006,  ..., -0.0728, -0.0978, -0.1091]],

        [[-0.1809,  0.0157,  0.0006,  ..., -0.0728, -0.0978, -0.1091]],

        [[-0.1809,  0.0157,  0.0006,  ..., -0.0728, -0.0978, -0.1091]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 27 0
layer name:  MLP
i,j,k 30 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6509, -0.0855,  0.2177,  ..., -0.5488, -0.2183, -0.4551]],

        [[-0.6509, -0.0855,  0.2177,  ..., -0.5488, -0.2183, -0.4551]],

        [[-0.6509, -0.0855,  0.2177,  ..., -0.5488, -0.2183, -0.4551]],

        [[-0.6509, -0.0855,  0.2177,  ..., -0.5488, -0.2183, -0.4551]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2438, -0.0090,  0.0793,  ..., -0.2087, -0.0832, -0.1711]],

        [[-0.2438, -0.0090,  0.0793,  ..., -0.2087, -0.0832, -0.1711]],

        [[-0.2438, -0.0090,  0.0793,  ..., -0.2087, -0.0832, -0.1711]],

        [[-0.2438, -0.0090,  0.0793,  ..., -0.2087, -0.0832, -0.1711]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 30 0
layer name:  MLP
i,j,k 30 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.8052,  0.3926,  0.3000,  ..., -0.4019, -0.5142, -0.6323]],

        [[-0.8052,  0.3926,  0.3000,  ..., -0.4019, -0.5142, -0.6323]],

        [[-0.8052,  0.3926,  0.3000,  ..., -0.4019, -0.5142, -0.6323]],

        [[-0.8052,  0.3926,  0.3000,  ..., -0.4019, -0.5142, -0.6323]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3896,  0.3445,  0.1567,  ..., -0.2019, -0.2617, -0.3000]],

        [[-0.3896,  0.3445,  0.1567,  ..., -0.2019, -0.2617, -0.3000]],

        [[-0.3896,  0.3445,  0.1567,  ..., -0.2019, -0.2617, -0.3000]],

        [[-0.3896,  0.3445,  0.1567,  ..., -0.2019, -0.2617, -0.3000]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 33 0
layer name:  MLP
i,j,k 30 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.9331,  0.7354,  0.3296,  ..., -0.5420, -0.5356, -0.7246]],

        [[-0.9331,  0.7354,  0.3296,  ..., -0.5420, -0.5356, -0.7246]],

        [[-0.9331,  0.7354,  0.3296,  ..., -0.5420, -0.5356, -0.7246]],

        [[-0.9331,  0.7354,  0.3296,  ..., -0.5420, -0.5356, -0.7246]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 30 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.5337,  0.5107,  0.2214,  ..., -0.3022, -0.3396, -0.3892]],

        [[-0.5337,  0.5107,  0.2214,  ..., -0.3022, -0.3396, -0.3892]],

        [[-0.5337,  0.5107,  0.2214,  ..., -0.3022, -0.3396, -0.3892]],

        [[-0.5337,  0.5107,  0.2214,  ..., -0.3022, -0.3396, -0.3892]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 30 36 0
layer name:  MLP
i,j,k 30 37 0
layer name:  OutputEmbed
i,j,k 31 0 0
layer name:  InputEmbed
i,j,k 31 1 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0594,  0.4622,  0.1642,  ..., -0.5542, -0.0241, -0.3733]],

        [[-0.0594,  0.4622,  0.1642,  ..., -0.5542, -0.0241, -0.3733]],

        [[-0.0594,  0.4622,  0.1642,  ..., -0.5542, -0.0241, -0.3733]],

        [[-0.0594,  0.4622,  0.1642,  ..., -0.5542, -0.0241, -0.3733]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 2 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0097,  0.0860,  0.0396,  ..., -0.1226, -0.0088, -0.0986]],

        [[-0.0097,  0.0860,  0.0396,  ..., -0.1226, -0.0088, -0.0986]],

        [[-0.0097,  0.0860,  0.0396,  ..., -0.1226, -0.0088, -0.0986]],

        [[-0.0097,  0.0860,  0.0396,  ..., -0.1226, -0.0088, -0.0986]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 3 0
layer name:  MLP
i,j,k 31 4 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0405,  0.5913,  0.1410,  ..., -0.2808, -0.0497, -0.4778]],

        [[-0.0405,  0.5913,  0.1410,  ..., -0.2808, -0.0497, -0.4778]],

        [[-0.0405,  0.5913,  0.1410,  ..., -0.2808, -0.0497, -0.4778]],

        [[-0.0405,  0.5913,  0.1410,  ..., -0.2808, -0.0497, -0.4778]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 5 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0061,  0.1172,  0.0406,  ..., -0.0682, -0.0102, -0.1199]],

        [[-0.0061,  0.1172,  0.0406,  ..., -0.0682, -0.0102, -0.1199]],

        [[-0.0061,  0.1172,  0.0406,  ..., -0.0682, -0.0102, -0.1199]],

        [[-0.0061,  0.1172,  0.0406,  ..., -0.0682, -0.0102, -0.1199]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 6 0
layer name:  MLP
i,j,k 31 7 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2002,  0.6392,  0.0873,  ..., -0.3325,  0.0555, -0.6885]],

        [[ 0.2002,  0.6392,  0.0873,  ..., -0.3325,  0.0555, -0.6885]],

        [[ 0.2002,  0.6392,  0.0873,  ..., -0.3325,  0.0555, -0.6885]],

        [[ 0.2002,  0.6392,  0.0873,  ..., -0.3325,  0.0555, -0.6885]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 8 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0477,  0.1422,  0.0254,  ..., -0.0614,  0.0141, -0.1367]],

        [[ 0.0477,  0.1422,  0.0254,  ..., -0.0614,  0.0141, -0.1367]],

        [[ 0.0477,  0.1422,  0.0254,  ..., -0.0614,  0.0141, -0.1367]],

        [[ 0.0477,  0.1422,  0.0254,  ..., -0.0614,  0.0141, -0.1367]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 9 0
layer name:  MLP
i,j,k 31 10 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2366,  0.3730,  0.0740,  ..., -0.1801,  0.0519, -0.5752]],

        [[ 0.2366,  0.3730,  0.0740,  ..., -0.1801,  0.0519, -0.5752]],

        [[ 0.2366,  0.3730,  0.0740,  ..., -0.1801,  0.0519, -0.5752]],

        [[ 0.2366,  0.3730,  0.0740,  ..., -0.1801,  0.0519, -0.5752]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 11 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0638,  0.1041,  0.0241,  ..., -0.0378,  0.0175, -0.1351]],

        [[ 0.0638,  0.1041,  0.0241,  ..., -0.0378,  0.0175, -0.1351]],

        [[ 0.0638,  0.1041,  0.0241,  ..., -0.0378,  0.0175, -0.1351]],

        [[ 0.0638,  0.1041,  0.0241,  ..., -0.0378,  0.0175, -0.1351]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 12 0
layer name:  MLP
i,j,k 31 13 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.2695,  0.2910,  0.1436,  ...,  0.1648,  0.1069, -0.5347]],

        [[ 0.2695,  0.2910,  0.1436,  ...,  0.1648,  0.1069, -0.5347]],

        [[ 0.2695,  0.2910,  0.1436,  ...,  0.1648,  0.1069, -0.5347]],

        [[ 0.2695,  0.2910,  0.1436,  ...,  0.1648,  0.1069, -0.5347]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 14 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0649,  0.0730,  0.0389,  ...,  0.0429,  0.0305, -0.1164]],

        [[ 0.0649,  0.0730,  0.0389,  ...,  0.0429,  0.0305, -0.1164]],

        [[ 0.0649,  0.0730,  0.0389,  ...,  0.0429,  0.0305, -0.1164]],

        [[ 0.0649,  0.0730,  0.0389,  ...,  0.0429,  0.0305, -0.1164]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 15 0
layer name:  MLP
i,j,k 31 16 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[ 0.1184,  0.4236,  0.3064,  ...,  0.2214,  0.0458, -0.4497]],

        [[ 0.1184,  0.4236,  0.3064,  ...,  0.2214,  0.0458, -0.4497]],

        [[ 0.1184,  0.4236,  0.3064,  ...,  0.2214,  0.0458, -0.4497]],

        [[ 0.1184,  0.4236,  0.3064,  ...,  0.2214,  0.0458, -0.4497]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 17 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0370,  0.1219,  0.0806,  ...,  0.0602,  0.0197, -0.1050]],

        [[ 0.0370,  0.1219,  0.0806,  ...,  0.0602,  0.0197, -0.1050]],

        [[ 0.0370,  0.1219,  0.0806,  ...,  0.0602,  0.0197, -0.1050]],

        [[ 0.0370,  0.1219,  0.0806,  ...,  0.0602,  0.0197, -0.1050]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 18 0
layer name:  MLP
i,j,k 31 19 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0322,  0.4695,  0.3508,  ...,  0.3237,  0.1444, -0.1821]],

        [[-0.0322,  0.4695,  0.3508,  ...,  0.3237,  0.1444, -0.1821]],

        [[-0.0322,  0.4695,  0.3508,  ...,  0.3237,  0.1444, -0.1821]],

        [[-0.0322,  0.4695,  0.3508,  ...,  0.3237,  0.1444, -0.1821]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 20 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[ 0.0004,  0.1498,  0.1027,  ...,  0.0943,  0.0475, -0.0414]],

        [[ 0.0004,  0.1498,  0.1027,  ...,  0.0943,  0.0475, -0.0414]],

        [[ 0.0004,  0.1498,  0.1027,  ...,  0.0943,  0.0475, -0.0414]],

        [[ 0.0004,  0.1498,  0.1027,  ...,  0.0943,  0.0475, -0.0414]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 21 0
layer name:  MLP
i,j,k 31 22 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.1725,  0.5376,  0.2957,  ...,  0.4050, -0.0181, -0.1648]],

        [[-0.1725,  0.5376,  0.2957,  ...,  0.4050, -0.0181, -0.1648]],

        [[-0.1725,  0.5376,  0.2957,  ...,  0.4050, -0.0181, -0.1648]],

        [[-0.1725,  0.5376,  0.2957,  ...,  0.4050, -0.0181, -0.1648]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 23 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0374,  0.1805,  0.0995,  ...,  0.1274,  0.0068, -0.0367]],

        [[-0.0374,  0.1805,  0.0995,  ...,  0.1274,  0.0068, -0.0367]],

        [[-0.0374,  0.1805,  0.0995,  ...,  0.1274,  0.0068, -0.0367]],

        [[-0.0374,  0.1805,  0.0995,  ...,  0.1274,  0.0068, -0.0367]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 24 0
layer name:  MLP
i,j,k 31 25 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.0967,  0.2781,  0.3123,  ...,  0.3220, -0.3450, -0.0076]],

        [[-0.0967,  0.2781,  0.3123,  ...,  0.3220, -0.3450, -0.0076]],

        [[-0.0967,  0.2781,  0.3123,  ...,  0.3220, -0.3450, -0.0076]],

        [[-0.0967,  0.2781,  0.3123,  ...,  0.3220, -0.3450, -0.0076]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 26 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0154,  0.1292,  0.1208,  ...,  0.1155, -0.0980,  0.0111]],

        [[-0.0154,  0.1292,  0.1208,  ...,  0.1155, -0.0980,  0.0111]],

        [[-0.0154,  0.1292,  0.1208,  ...,  0.1155, -0.0980,  0.0111]],

        [[-0.0154,  0.1292,  0.1208,  ...,  0.1155, -0.0980,  0.0111]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 27 0
layer name:  MLP
i,j,k 31 28 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.2395, -0.0807,  0.1071,  ...,  0.2583, -0.6450, -0.2476]],

        [[-0.2395, -0.0807,  0.1071,  ...,  0.2583, -0.6450, -0.2476]],

        [[-0.2395, -0.0807,  0.1071,  ...,  0.2583, -0.6450, -0.2476]],

        [[-0.2395, -0.0807,  0.1071,  ...,  0.2583, -0.6450, -0.2476]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 29 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.0704,  0.0139,  0.0599,  ...,  0.1169, -0.2301, -0.0763]],

        [[-0.0704,  0.0139,  0.0599,  ...,  0.1169, -0.2301, -0.0763]],

        [[-0.0704,  0.0139,  0.0599,  ...,  0.1169, -0.2301, -0.0763]],

        [[-0.0704,  0.0139,  0.0599,  ...,  0.1169, -0.2301, -0.0763]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 30 0
layer name:  MLP
i,j,k 31 31 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.5298,  0.7280, -0.0597,  ...,  0.2295, -0.6587, -0.2888]],

        [[-0.5298,  0.7280, -0.0597,  ...,  0.2295, -0.6587, -0.2888]],

        [[-0.5298,  0.7280, -0.0597,  ...,  0.2295, -0.6587, -0.2888]],

        [[-0.5298,  0.7280, -0.0597,  ...,  0.2295, -0.6587, -0.2888]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 32 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.2172,  0.5771, -0.0049,  ...,  0.1305, -0.2942, -0.1057]],

        [[-0.2172,  0.5771, -0.0049,  ...,  0.1305, -0.2942, -0.1057]],

        [[-0.2172,  0.5771, -0.0049,  ...,  0.1305, -0.2942, -0.1057]],

        [[-0.2172,  0.5771, -0.0049,  ...,  0.1305, -0.2942, -0.1057]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 33 0
layer name:  MLP
i,j,k 31 34 0
layer name:  layer_norm
self attention decode layernorm =======
layernorm----------------------------------------------------------
out tensor([[[-0.6729,  1.5029, -0.0667,  ...,  0.1360, -0.8164, -0.4465]],

        [[-0.6729,  1.5029, -0.0667,  ...,  0.1360, -0.8164, -0.4465]],

        [[-0.6729,  1.5029, -0.0667,  ...,  0.1360, -0.8164, -0.4465]],

        [[-0.6729,  1.5029, -0.0667,  ...,  0.1360, -0.8164, -0.4465]]],
       device='cuda:0', dtype=torch.float16)
i,j,k 31 35 0
layer name:  SelfAttention
------------------************   number of head 12
prev_hidden <class 'flexgen_utils.ValueHolder'>
prev_hidden.val TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
prev_hidden.val.data tensor([[[-0.3252,  0.9009, -0.0018,  ...,  0.1090, -0.4514, -0.1915]],

        [[-0.3252,  0.9009, -0.0018,  ...,  0.1090, -0.4514, -0.1915]],

        [[-0.3252,  0.9009, -0.0018,  ...,  0.1090, -0.4514, -0.1915]],

        [[-0.3252,  0.9009, -0.0018,  ...,  0.1090, -0.4514, -0.1915]]],
       device='cuda:0', dtype=torch.float16)
self attention decode =======
self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
mha_gen----------------------------------------------------------
b,tgt_s,h 4 1 768
head_dim 64
scaling 0.125
hidden torch.Size([4, 1, 768])
i,j,k 31 36 0
layer name:  MLP
i,j,k 31 37 0
layer name:  OutputEmbed
Outputs:
----------------------------------------------------------------------
0: Paris is the capital city of France.
I'm not sure if you're being sarcastic or not, but I'm not sure if you're being sarcastic.
I'm not sure if
----------------------------------------------------------------------
3: Paris is the capital city of France.
I'm not sure if you're being sarcastic or not, but I'm not sure if you're being sarcastic.
I'm not sure if
----------------------------------------------------------------------

TorchDevice: cuda:0
  cur_mem: 0.3205 GB,  peak_mem: 0.5925 GB
TorchDevice: cpu
  cur_mem: 0.0000 GB,  peak_mem: 0.0000 GB
model size: 0.230 GB	cache size: 0.075 GB	hidden size (p): 0.003 GB
peak gpu mem: 0.592 GB	projected: False
prefill latency: 0.584 s	prefill throughput: 3509.183 token/s
decode latency: 2.949 s	decode throughput: 42.055 token/s
total latency: 3.532 s	total throughput: 36.239 token/s
