args.head_ip  127.0.0.1
Distributed package is available!
args.head_ip  127.0.0.1
Distributed package is available!
> initializing torch.distributed with local rank: 0, rank: 0, world size: 2
> initializing torch.distributed with local rank: 1, rank: 1, world size: 2
dist init successfully
rank #1: Finished initializing -* tensor parallel *- distributed environment
rank #1: group,  <function get_tensor_model_parallel_group at 0x7f0d03f169d0>
rank #1: Backend: nccl
rank #1: World Size: 2
rank #1: Rank: 1
rank #1: <run_flexgen>: args.model: facebook/opt-125m
dist init successfully
rank #0: Finished initializing -* tensor parallel *- distributed environment
rank #0: group,  <function get_tensor_model_parallel_group at 0x7f2186ac29d0>
rank #0: Backend: nccl
rank #0: World Size: 2
rank #0: Rank: 0
rank #0: <run_flexgen>: args.model: facebook/opt-125m
rank #0: prompt_len, gen_len, cut_gen_len  256 32 5
rank #1: prompt_len, gen_len, cut_gen_len  256 32 5
rank #0: model size: 0.230 GB, cache size: 0.079 GB, hidden size (prefill): 0.003 GB
rank #0: init weight...
rank #0: start create model 
rank #0: args.rank 0
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #1: model size: 0.230 GB, cache size: 0.079 GB, hidden size (prefill): 0.003 GB
rank #1: init weight...
rank #1: start create model 
rank #1: args.rank 1
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #0: init all weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #1: init all weights 
rank #1: ******* OPTLM model init weight
rank #0: the time init all weights  0.17490744590759277
rank #0: the model construction time  4.389786005020142
rank #0:    model structure 
rank #0: InputEmbed
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: OutputEmbed
rank #0:
rank #0: the useful data start from here -------------------------------------
rank #0: benchmark - generate
rank #0: args.gen_len  32
rank #0: input  torch.Size([8, 256])
rank #0: task.prompt_len,  256
rank #0: task.gen_len  32
rank #0: self.execute_gen_len,  5
rank #0: gen_len........  32
rank #0: num_gpu_batches  2
rank #0: self.hidden shape  32
rank #0: ============ generate loop normal ============
rank #0: generation_loop_normal start.........
rank #0: i: self.execute_gen_len  5
rank #0: j: self.num_layers  38
rank #0: k: self.num_gpu_batches  2
rank #0: generate start -----
rank #1: ******* OPTLM model init weight
rank #0: i, j, k = 0, 0, 0
rank #0: load_cache 
rank #1: ******* OPTLM model init weight
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: ******* OPTLM model init weight
rank #1: the time init all weights  0.132063627243042
rank #1: the model construction time  4.5121660232543945
rank #1:    model structure 
rank #1: InputEmbed
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: OutputEmbed
rank #1:
rank #1: the useful data start from here -------------------------------------
rank #1: benchmark - generate
rank #1: args.gen_len  32
rank #1: input  torch.Size([8, 256])
rank #1: task.prompt_len,  256
rank #1: task.gen_len  32
rank #1: self.execute_gen_len,  5
rank #1: gen_len........  32
rank #1: num_gpu_batches  2
rank #1: self.hidden shape  32
rank #1: ============ generate loop normal ============
rank #1: generation_loop_normal start.........
rank #1: i: self.execute_gen_len  5
rank #1: j: self.num_layers  38
rank #1: k: self.num_gpu_batches  2
rank #1: generate start -----
rank #1: i, j, k = 0, 0, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 0, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 1, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 1, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 2, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #1: ------------------------layer name  InputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 0, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   0
rank #1: ------------------------layer name  InputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 1, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   1
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 1, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   1
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 2, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   2
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: self.compute  TorchDevice(name=cuda:0)
