args.head_ip  127.0.0.1
Distributed package is available!
> initializing torch.distributed with local rank: 0, rank: 0, world size: 1
dist init successfully
rank #0: Finished initializing -* tensor parallel *- distributed environment
rank #0: group,  <function get_tensor_model_parallel_group at 0x7f216e6ce1f0>
rank #0: Backend: nccl
rank #0: World Size: 1
rank #0: Rank: 0
rank #0: <run_flexgen>: args.model: facebook/opt-125m
rank #0: model size: 0.230 GB, cache size: 0.079 GB, hidden size (prefill): 0.003 GB
rank #0: init weight...
rank #0: start create model 
rank #0: args.rank 0
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: init all weights 
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.4804097702687206
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((50272, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_tokens.weight')
rank #0: *********-------=-=-=--mid_percent  0.9804097702687206
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((2050, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_positions.weight')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn_layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn_layer_norm.bias')
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.q_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.q_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.k_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.k_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.v_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.v_proj.bias')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.out_proj.weight')
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.out_proj.bias')
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  9.945498667303179e-06
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layer_norm.weight')
rank #0: *********-------=-=-=--mid_percent  2.9836496001909535e-05
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layer_norm.bias')
rank #0: *********-------=-=-=--mid_percent  0.5000198909973346
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((50272, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_tokens.weight')
rank #0: the time init all weights  4.791256427764893
rank #0: the model construction time  4.917229175567627
rank #0:    model structure 
rank #0: InputEmbed
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: OutputEmbed
rank #0:
rank #0: the useful data start from here -------------------------------------
rank #0: benchmark - generate
rank #0: args.gen_len  32
rank #0: input  torch.Size([8, 256])
rank #0: task.prompt_len,  256
rank #0: task.gen_len  32
rank #0: self.execute_gen_len,  5
rank #0: gen_len........  32
rank #0: num_gpu_batches  2
rank #0: self.hidden shape  32
rank #0: ============ generate loop normal ============
rank #0: generation_loop_normal start.........
rank #0: i: self.execute_gen_len  5
rank #0: j: self.num_layers  38
rank #0: k: self.num_gpu_batches  2
rank #0: generate start -----
rank #0: i, j, k = 0, 0, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 0, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 1, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 1, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 2, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 2, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 3, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 3, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 4, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 4, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 5, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 5, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 6, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 6, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 7, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 7, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 8, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 8, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 9, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 9, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 10, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 10, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 11, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 11, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 12, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 12, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 13, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 13, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 14, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 14, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 15, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 15, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 16, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 16, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 17, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 17, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 18, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 18, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 19, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 19, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 20, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 20, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 21, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 21, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 22, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 22, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 23, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 23, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 24, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 24, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 25, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 25, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 26, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 26, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 27, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 27, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 28, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 28, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 29, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 29, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 30, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 30, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 31, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 31, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 32, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 32, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 33, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 33, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 34, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 34, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 35, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 35, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 36, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 36, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 37, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: i, j, k = 0, 37, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: generate stop *******
rank #0: i, j, k = 1, 0, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 0, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 1, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 1, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 2, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 2, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 3, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 3, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 4, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 4, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 5, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 5, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 6, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 6, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 7, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 7, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 8, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 8, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 9, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 9, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 10, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 10, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 11, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 11, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 12, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 12, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 13, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 13, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 14, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 14, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 15, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 15, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 16, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 16, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 17, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 17, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 18, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 18, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 19, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 19, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 20, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 20, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 21, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 21, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 22, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 22, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 23, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 23, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 24, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 24, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 25, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 25, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 26, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 26, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 27, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 27, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 28, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 28, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 29, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 29, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 30, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 30, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 31, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 31, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 32, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 32, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 33, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 33, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 34, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 34, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 35, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 35, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: k shape  torch.Size([48, 64, 257])
rank #0: v shape  torch.Size([257, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 36, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 36, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 37, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: i, j, k = 1, 37, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: generate stop *******
rank #0: i, j, k = 2, 0, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 0, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 1, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 1, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 2, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 2, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 3, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 3, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 4, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 4, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 5, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 5, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 6, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 6, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 7, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 7, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 8, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 8, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 9, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 9, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 10, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 10, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 11, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 11, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 12, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 12, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 13, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 13, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 14, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 14, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 15, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 15, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 16, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 16, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 17, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 17, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 18, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 18, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 19, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 19, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 20, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 20, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 21, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 21, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 22, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 22, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 23, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 23, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 24, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 24, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 25, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 25, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 26, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 26, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 27, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 27, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 28, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 28, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 29, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 29, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 30, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 30, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 31, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 31, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 32, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 32, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 33, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 33, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 34, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 34, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 35, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 35, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  258
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  258
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([258, 48, 64])
rank #0: k shape  torch.Size([48, 64, 258])
rank #0: v shape  torch.Size([258, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  258
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 258])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 258])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 258])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 258])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 36, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 36, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 2, 37, 0
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: i, j, k = 2, 37, 1
rank #0: load_cache 
rank #0: load hidden i  2
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: generate stop *******
rank #0: i, j, k = 3, 0, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 0, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 1, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 1, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 2, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 2, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 3, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 3, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 4, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 4, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 5, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 5, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 6, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 6, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 7, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 7, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 8, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 8, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 9, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 9, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 10, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 10, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 11, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 11, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 12, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 12, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 13, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 13, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 14, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 14, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 15, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 15, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 16, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 16, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 17, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 17, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 18, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 18, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 19, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 19, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 20, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 20, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 21, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 21, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 22, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 22, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 23, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 23, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 24, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 24, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 25, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 25, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 26, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 26, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 27, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 27, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 28, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 28, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 29, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 29, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 30, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 30, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 31, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 31, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 32, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 32, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 33, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 33, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 34, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 34, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 35, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 35, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  259
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  259
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([259, 48, 64])
rank #0: k shape  torch.Size([48, 64, 259])
rank #0: v shape  torch.Size([259, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  259
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 259])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 259])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 259])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 259])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 36, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 36, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 3, 37, 0
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: i, j, k = 3, 37, 1
rank #0: load_cache 
rank #0: load hidden i  3
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: generate stop *******
rank #0: i, j, k = 4, 0, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 0, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 1, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 1, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 2, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 2, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 3, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 3, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 4, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 4, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 5, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 5, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 6, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 6, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 7, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 7, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 8, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 8, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 9, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 9, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 10, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 10, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 11, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 11, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 12, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 12, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 13, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 13, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 14, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 14, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 15, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 15, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 16, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 16, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 17, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 17, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 18, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 18, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 19, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 19, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 20, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 20, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 21, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 21, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 22, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 22, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 23, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 23, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 24, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 24, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 25, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 25, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 26, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 26, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 27, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 27, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 28, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 28, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 29, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 29, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 30, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 30, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 31, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 31, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 32, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 32, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 33, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 33, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 34, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 34, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 35, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 35, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: decode input h.data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  260
rank #0: head dimension : 64
rank #0: number of heads per partition:  12
rank #0: world_size_mha_gen_TP 1
rank #0: tensor_parallel_size  1
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 1
rank #0: len(q_weight_partitions[0][0] ) 768
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([768, 768])
rank #0: shape of b_q_rank  torch.Size([768])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<AsStridedBackward0>)
rank #0: w_q_rank shape  torch.Size([768, 768])
rank #0: b_q_rank shape  torch.Size([768])
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 768])
rank #0: k shape after F.linear  torch.Size([4, 1, 768])
rank #0: v shape after F.linear  torch.Size([4, 1, 768])
rank #0: q  torch.Size([1, 4, 12, 64])
rank #0: k  torch.Size([1, 4, 12, 64])
rank #0: v  torch.Size([1, 4, 12, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 12, 1, 1)
rank #0: query_layer.size  torch.Size([1, 48, 64])
rank #0: key_layer.size  torch.Size([1, 48, 64])
rank #0: value_layer.size  torch.Size([1, 48, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 48, 64])
rank #0: k_new  torch.Size([1, 48, 64])
rank #0: v_new  torch.Size([1, 48, 64])
rank #0: src_s  260
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([260, 48, 64])
rank #0: k shape  torch.Size([48, 64, 260])
rank #0: v shape  torch.Size([260, 4, 12, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  260
rank #0: n_head  12
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([48, 1, 260])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 12, 1, 260])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 12, 1, 260])
rank #0: before ouput size q.shape  torch.Size([1, 48, 64])
rank #0: attention probs shape :  torch.Size([48, 1, 260])
rank #0: shape of value  torch.Size([1, 4, 768])
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 768])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 768])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().shape  torch.Size([768, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]],

        [[nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 36, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 36, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 4, 37, 0
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: i, j, k = 4, 37, 1
rank #0: load_cache 
rank #0: load hidden i  4
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: generate stop *******
rank #0: the model generate time  1.9829432964324951
rank #0: TorchDevice: cuda:0
rank #0:   cur_mem: 0.3156 GB,  peak_mem: 0.5688 GB
rank #0: TorchDevice: cpu
rank #0:   cur_mem: 0.0110 GB,  peak_mem: 0.0000 GB
rank #0: model size: 0.230 GB	cache size: 0.079 GB	hidden size (p): 0.003 GB
peak gpu mem: 0.569 GB	projected: True
prefill latency: 0.385 s	prefill throughput: 5325.741 token/s
decode latency: 11.892 s	decode throughput: 20.855 token/s
total latency: 12.276 s	total throughput: 20.854 token/s
