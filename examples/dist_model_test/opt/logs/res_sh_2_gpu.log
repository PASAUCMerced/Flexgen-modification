args.head_ip  127.0.0.1
Distributed package is available!
args.head_ip  127.0.0.1
Distributed package is available!
> initializing torch.distributed with local rank: 0, rank: 0, world size: 2
> initializing torch.distributed with local rank: 1, rank: 1, world size: 2
dist init successfully
rank #1: Finished initializing -* tensor parallel *- distributed environment
rank #1: group,  <function get_tensor_model_parallel_group at 0x7f7338227ee0>
rank #1: Backend: nccl
rank #1: World Size: 2
rank #1: Rank: 1
rank #1: <run_flexgen>: args.model: facebook/opt-125m
dist init successfully
rank #0: Finished initializing -* tensor parallel *- distributed environment
rank #0: group,  <function get_tensor_model_parallel_group at 0x7efc52629ee0>
rank #0: Backend: nccl
rank #0: World Size: 2
rank #0: Rank: 0
rank #0: <run_flexgen>: args.model: facebook/opt-125m
rank #1: model size: 0.230 GB, cache size: 0.079 GB, hidden size (prefill): 0.003 GB
rank #1: init weight...
rank #1: start create model 
rank #1: args.rank 1
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #1: split sizes  [768]
rank #0: model size: 0.230 GB, cache size: 0.079 GB, hidden size (prefill): 0.003 GB
rank #0: init weight...
rank #0: start create model 
rank #0: args.rank 0
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: split sizes  [768]
rank #0: init all weights 
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.4804097702687206
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((50272, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_tokens.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.9804097702687206
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((2050, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_positions.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.25
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn_layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.75
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn_layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  0.12483745123537061
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.q_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.2498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.q_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.k_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.4998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.k_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.6248374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.v_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.7498374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.v_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.8748374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.out_proj.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: *********-------=-=-=--mid_percent  0.9998374512353706
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.out_proj.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: DUMMY weights 
rank #0: ******* OPTLM model init weight
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ******* OPTLM model init weight
rank #0: *********-------=-=-=--mid_percent  9.945498667303179e-06
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layer_norm.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  2.9836496001909535e-05
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layer_norm.bias')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: *********-------=-=-=--mid_percent  0.5000198909973346
rank #0: home device is  TorchDevice(name=cuda:0)
rank #0: weight_specs[i]  ((50272, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_tokens.weight')
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #1: init all weights 
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.4804097702687206
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((50272, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_tokens.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #0: the time init all weights  0.18520545959472656
rank #0: the model construction time  4.768162250518799
rank #0:    model structure 
rank #0: InputEmbed
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: layer_norm
rank #0: SelfAttention
rank #0: prefill  None
rank #0: MLP
rank #0: OutputEmbed
rank #0:
rank #0: the useful data start from here -------------------------------------
rank #0: benchmark - generate
rank #0: args.gen_len  32
rank #0: input  torch.Size([8, 256])
rank #0: task.prompt_len,  256
rank #0: task.gen_len  32
rank #0: self.execute_gen_len,  5
rank #0: gen_len........  32
rank #0: num_gpu_batches  2
rank #0: self.hidden shape  32
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cpu
rank #0: device.dev  cpu
rank #0: data.device  cpu
rank #0: device.dev  cpu
rank #0: ============ generate loop normal ============
rank #0: generation_loop_normal start.........
rank #0: i: self.execute_gen_len  5
rank #0: j: self.num_layers  38
rank #0: k: self.num_gpu_batches  2
rank #0: generate start -----
rank #0: data.device  cpu
rank #0: device.dev  cpu
rank #1: *********-------=-=-=--mid_percent  0.9804097702687206
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((2050, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_positions.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #0: data.device  cpu
rank #1: home device is  TorchDevice(name=cuda:0)
rank #0: device.dev  cpu
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #0: i, j, k = 0, 0, 0
rank #0: load_cache 
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #1: DUMMY weights 
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.0.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.1.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.2.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.3.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.4.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.5.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.6.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.7.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.8.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.9.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.10.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.25
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn_layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.75
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn_layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  0.12483745123537061
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.q_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.2498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.q_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.3748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.k_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.4998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.k_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.6248374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.v_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.7498374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.v_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.8748374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.out_proj.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: *********-------=-=-=--mid_percent  0.9998374512353706
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layers.11.self_attn.out_proj.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: DUMMY weights 
rank #1: ******* OPTLM model init weight
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ******* OPTLM model init weight
rank #1: *********-------=-=-=--mid_percent  9.945498667303179e-06
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layer_norm.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  2.9836496001909535e-05
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((768,), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.layer_norm.bias')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: *********-------=-=-=--mid_percent  0.5000198909973346
rank #1: home device is  TorchDevice(name=cuda:0)
rank #1: weight_specs[i]  ((50272, 768), <class 'numpy.float16'>, '/home/cc/my_flexgen/examples/dist_test/_DUMMY_/opt-125m-np/decoder.embed_tokens.weight')
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: the time init all weights  0.16644048690795898
rank #1: the model construction time  4.935858488082886
rank #1:    model structure 
rank #1: InputEmbed
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: layer_norm
rank #1: SelfAttention
rank #1: prefill  None
rank #1: MLP
rank #1: OutputEmbed
rank #1:
rank #1: the useful data start from here -------------------------------------
rank #1: benchmark - generate
rank #1: args.gen_len  32
rank #1: input  torch.Size([8, 256])
rank #1: task.prompt_len,  256
rank #1: task.gen_len  32
rank #1: self.execute_gen_len,  5
rank #1: gen_len........  32
rank #1: num_gpu_batches  2
rank #1: self.hidden shape  32
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cpu
rank #1: device.dev  cpu
rank #1: data.device  cpu
rank #1: device.dev  cpu
rank #1: ============ generate loop normal ============
rank #1: generation_loop_normal start.........
rank #1: i: self.execute_gen_len  5
rank #1: j: self.num_layers  38
rank #1: k: self.num_gpu_batches  2
rank #1: generate start -----
rank #1: data.device  cpu
rank #1: device.dev  cpu
rank #1: data.device  cpu
rank #1: device.dev  cpu
rank #1: i, j, k = 0, 0, 0
rank #1: load_cache 
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 0, 1
rank #0: load_cache 
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 1, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 1, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 2, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 2, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 3, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 3, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   3
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 4, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 4, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   4
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 5, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 5, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   5
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 6, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 6, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   6
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 7, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 7, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   7
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 8, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 8, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   8
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 9, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 9, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   9
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 10, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 10, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   10
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 11, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 11, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   11
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 12, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 12, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   12
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 13, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 13, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   13
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 14, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 14, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   14
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 15, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 15, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   15
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 16, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 16, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   16
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 17, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 17, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   17
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 18, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 18, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   18
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 19, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 19, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   19
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 20, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 20, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   20
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 21, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 21, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   21
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 22, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 22, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   22
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 23, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 23, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   23
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 24, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 24, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   24
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 25, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 25, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   25
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 26, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 26, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   26
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 27, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 27, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   27
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 28, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 28, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   28
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 29, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 29, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   29
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 30, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 30, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   30
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 31, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 31, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   31
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 32, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 32, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   32
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 33, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 33, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   33
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 34, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 34, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   34
rank #0: self attention prefill--------
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 35, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 35, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   35
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: Clear the weight_read_buf if it is the last gpu batch... 
rank #0: self attention prefill--------
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: self.compute  TorchDevice(name=cuda:0)
rank #0: mha prefill----------------
rank #0:  inputs.shape  torch.Size([4, 256, 768])
rank #0: head_dim = h // n_head  64
rank #0: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  SelfAttention
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 36, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 36, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   36
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  MLP
rank #0: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 0, 37, 0
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  InputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 0, 1
rank #1: load_cache 
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  InputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 1, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   1
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 1, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   1
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 2, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   2
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: i, j, k = 0, 37, 1
rank #0: load_cache 
rank #0: load hidden i  0
rank #0: ++++++++++++------+++++ compute_layer  layer   37
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  OutputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #0: generate stop *******
rank #0: data.device  cpu
rank #0: device.dev  cpu
rank #0: data.device  cpu
rank #0: device.dev  cpu
rank #0: i, j, k = 1, 0, 0
rank #0: load_cache 
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 0, 1
rank #0: load_cache 
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: data.device  cuda:0
rank #0: device.dev  cuda:0
rank #0: ------------------------layer name  InputEmbed
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 1, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 1, 1
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   1
rank #0: self attention decode =======
rank #0: ------------------------layer name  layer_norm
rank #0: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0: i, j, k = 1, 2, 0
rank #0: load_cache 
rank #0: load hidden i  1
rank #0: ++++++++++++------+++++ compute_layer  layer   2
rank #0: ------------------************   number of head 12
rank #0: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #0:  weight_read_buf if it is not not last gpu batch... 
rank #0: -----------------self attention decode =======
rank #0: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 2, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   2
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 3, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   3
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 3, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   3
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 4, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   4
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 4, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   4
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 5, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   5
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 5, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   5
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 6, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   6
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 6, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   6
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 7, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   7
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 7, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   7
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 8, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   8
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 8, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   8
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 9, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   9
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 9, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   9
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 10, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   10
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 10, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   10
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 11, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   11
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 11, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   11
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 12, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   12
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 12, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   12
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 13, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   13
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 13, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   13
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 14, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   14
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 14, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   14
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 15, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   15
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 15, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   15
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 16, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   16
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 16, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   16
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 17, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   17
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 17, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   17
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 18, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   18
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 18, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   18
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 19, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   19
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 19, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   19
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 20, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   20
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 20, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   20
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 21, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   21
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 21, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   21
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 22, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   22
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 22, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   22
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 23, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   23
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 23, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   23
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 24, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   24
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 24, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   24
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 25, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   25
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 25, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   25
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 26, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   26
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 26, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   26
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 27, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   27
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 27, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   27
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 28, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   28
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 28, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   28
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 29, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   29
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 29, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   29
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 30, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   30
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 30, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   30
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 31, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   31
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 31, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   31
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 32, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   32
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 32, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   32
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 33, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   33
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 33, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   33
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 34, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   34
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 34, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   34
rank #1: self attention prefill--------
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 35, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   35
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 35, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   35
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: Clear the weight_read_buf if it is the last gpu batch... 
rank #1: self attention prefill--------
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: self.compute  TorchDevice(name=cuda:0)
rank #1: mha prefill----------------
rank #1:  inputs.shape  torch.Size([4, 256, 768])
rank #1: head_dim = h // n_head  64
rank #1: hidden type <class 'torch.Tensor'>
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  SelfAttention
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 36, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   36
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 36, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   36
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  MLP
rank #1: hidden  TorchTensor(shape=torch.Size([4, 256, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 0, 37, 0
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   37
rank #0: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: mha_gen decode----------------
rank #0:
rank #0: batch, tgt_sequence, hidden: 4 1 768
rank #0: number of head  12
rank #0: src_s  257
rank #0: head dimension : 64
rank #0: number of heads per partition:  6
rank #0: world_size_mha_gen_TP 2
rank #0: tensor_parallel_size  2
rank #0: w_q.data.shape  torch.Size([768, 768])
rank #0: len(q_weight_partitions[0] ) 2
rank #0: len(q_weight_partitions[0][0] ) 384
rank #0: len(q_weight_partitions[0][0][0] ) 768
rank #0: the shape of w_q_rank  torch.Size([384, 768])
rank #0: shape of b_q_rank  torch.Size([384])
rank #0: hidden shape  torch.Size([4, 1, 768])
rank #0: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #0: w_q_rank shape  torch.Size([384, 768])
rank #0: b_q_rank shape  torch.Size([384])
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  OutputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #1: i, j, k = 0, 37, 1
rank #1: load_cache 
rank #1: load hidden i  0
rank #1: ++++++++++++------+++++ compute_layer  layer   37
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  OutputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
rank #1: generate stop *******
rank #1: data.device  cpu
rank #1: device.dev  cpu
rank #1: data.device  cpu
rank #1: device.dev  cpu
rank #1: i, j, k = 1, 0, 0
rank #1: load_cache 
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: load hidden i  1
rank #1: ++++++++++++------+++++ compute_layer  layer   0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  InputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 1, 0, 1
rank #1: load_cache 
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: load hidden i  1
rank #1: ++++++++++++------+++++ compute_layer  layer   0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: data.device  cuda:0
rank #1: device.dev  cuda:0
rank #1: ------------------------layer name  InputEmbed
rank #1: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 1, 1, 0
rank #1: load_cache 
rank #1: load hidden i  1
rank #1: ++++++++++++------+++++ compute_layer  layer   1
rank #1: self attention decode =======
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 1, 1, 1
rank #1: load_cache 
rank #1: load hidden i  1
rank #1: ++++++++++++------+++++ compute_layer  layer   1
rank #1: self attention decode =======
rank #1: ------------------------layer name  layer_norm
rank #1: hidden  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #1: i, j, k = 1, 2, 0
rank #1: load_cache 
rank #1: load hidden i  1
rank #1: ++++++++++++------+++++ compute_layer  layer   2
rank #1: ------------------************   number of head 12
rank #1: self attention layer hidden value:  TorchTensor(shape=torch.Size([4, 1, 768]), dtype=torch.float16, device=cuda:0)
rank #1:  weight_read_buf if it is not not last gpu batch... 
rank #1: -----------------self attention decode =======
rank #1: self.policy.comp_cache_config  CompressionConfig(num_bits=4, group_size=64, group_dim=2, symmetric=False, enabled=True)
rank #0: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #0: q shape after F.linear  torch.Size([4, 1, 384])
rank #0: k shape after F.linear  torch.Size([4, 1, 384])
rank #0: v shape after F.linear  torch.Size([4, 1, 384])
rank #0: q  torch.Size([1, 4, 6, 64])
rank #0: k  torch.Size([1, 4, 6, 64])
rank #0: v  torch.Size([1, 4, 6, 64])
rank #0: device of q  cuda:0
rank #0: device of k  cuda:0
rank #0: device of v  cuda:0
rank #0: output_size  (4, 6, 1, 1)
rank #0: query_layer.size  torch.Size([1, 24, 64])
rank #0: key_layer.size  torch.Size([1, 24, 64])
rank #0: value_layer.size  torch.Size([1, 24, 64])
rank #0: device of q  new  cuda:0
rank #0: device of k  new cuda:0
rank #0: device of v  new cuda:0
rank #0: device of k_cache  TorchDevice(name=cpu)
rank #0: enter DENSE attention------
rank #0: we do not compress cache
rank #0: q  torch.Size([1, 24, 64])
rank #0: k_new shape torch.Size([1, 24, 64])
rank #0: k_new device  cuda:0
rank #0: v_new  torch.Size([1, 24, 64])
rank #0: src_s  257
rank #0: k_cache.data size  287
rank #0: k_cache.data size  torch.Size([287, 48, 64])
rank #0: k_cache.data[:src_s]  torch.Size([257, 24, 64])
rank #0: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #0: current rank partial v_cache.data[:src_s]  torch.Size([257, 24, 64])
rank #0: k shape  torch.Size([24, 64, 257])
rank #0: v shape  torch.Size([257, 4, 6, 64])
rank #0: device of k cuda ?  cpu
rank #0: device of v cuda ?  cpu
rank #0: k is on cpu 
rank #0: q, k, mask
rank #0: cpu, cpu, cpu
rank #0: src_s  257
rank #0: n_head  6
rank #0: head_dim  64
rank #0: attn_weights shape  torch.Size([24, 1, 257])
rank #0: attn_weights device cpu
rank #0: attn_weights.shape  torch.Size([4, 6, 1, 257])
rank #0: device of attn_weights  cpu
rank #0: attn_weights shape ,  torch.Size([4, 6, 1, 257])
rank #0: before ouput size q.shape  torch.Size([1, 24, 64])
rank #0: attention probs shape :  torch.Size([24, 1, 257])
rank #0: shape of value  torch.Size([1, 4, 384])
rank #0: value device  cuda:0
rank #0: tgt_s  1
rank #0: w_out_rank.shape  torch.Size([768, 384])
rank #0: value.shape  before rowlinear torch.Size([4, 1, 384])
rank #0: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #0: w_out_rank.t().device  cuda:0
rank #0: w_out_rank.t().shape  torch.Size([384, 768])
rank #0: type of output_parallel  torch.cuda.HalfTensor
rank #0: current rank  0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:0',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #1: decode input h.data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #1: mha_gen decode----------------
rank #1:
rank #1: batch, tgt_sequence, hidden: 4 1 768
rank #1: number of head  12
rank #1: src_s  257
rank #1: head dimension : 64
rank #1: number of heads per partition:  6
rank #1: world_size_mha_gen_TP 2
rank #1: tensor_parallel_size  2
rank #1: w_q.data.shape  torch.Size([768, 768])
rank #1: len(q_weight_partitions[0] ) 2
rank #1: len(q_weight_partitions[0][0] ) 384
rank #1: len(q_weight_partitions[0][0][0] ) 768
rank #1: the shape of w_q_rank  torch.Size([384, 768])
rank #1: shape of b_q_rank  torch.Size([384])
rank #1: hidden shape  torch.Size([4, 1, 768])
rank #1: input data  tensor([[[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]],

        [[2., 2., 2.,  ..., 2., 2., 2.]]], device='cuda:0',
       dtype=torch.float16)
rank #1: w_q_rank shape  torch.Size([384, 768])
rank #1: b_q_rank shape  torch.Size([384])
rank #1: w_q_rank  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
rank #1: q shape after F.linear  torch.Size([4, 1, 384])
rank #1: k shape after F.linear  torch.Size([4, 1, 384])
rank #1: v shape after F.linear  torch.Size([4, 1, 384])
rank #1: q  torch.Size([1, 4, 6, 64])
rank #1: k  torch.Size([1, 4, 6, 64])
rank #1: v  torch.Size([1, 4, 6, 64])
rank #1: device of q  cuda:1
rank #1: device of k  cuda:1
rank #1: device of v  cuda:1
rank #1: output_size  (4, 6, 1, 1)
rank #1: query_layer.size  torch.Size([1, 24, 64])
rank #1: key_layer.size  torch.Size([1, 24, 64])
rank #1: value_layer.size  torch.Size([1, 24, 64])
rank #1: device of q  new  cuda:1
rank #1: device of k  new cuda:1
rank #1: device of v  new cuda:1
rank #1: device of k_cache  TorchDevice(name=cpu)
rank #1: enter DENSE attention------
rank #1: we do not compress cache
rank #1: q  torch.Size([1, 24, 64])
rank #1: k_new shape torch.Size([1, 24, 64])
rank #1: k_new device  cuda:1
rank #1: v_new  torch.Size([1, 24, 64])
rank #1: src_s  257
rank #1: k_cache.data size  287
rank #1: k_cache.data size  torch.Size([287, 48, 64])
rank #1: k_cache.data[:src_s]  torch.Size([257, 24, 64])
rank #1: original v_cache.data[:src_s]  torch.Size([257, 48, 64])
rank #1: current rank partial v_cache.data[:src_s]  torch.Size([257, 24, 64])
rank #1: k shape  torch.Size([24, 64, 257])
rank #1: v shape  torch.Size([257, 4, 6, 64])
rank #1: device of k cuda ?  cpu
rank #1: device of v cuda ?  cpu
rank #1: k is on cpu 
rank #1: q, k, mask
rank #1: cpu, cpu, cpu
rank #1: src_s  257
rank #1: n_head  6
rank #1: head_dim  64
rank #1: attn_weights shape  torch.Size([24, 1, 257])
rank #1: attn_weights device cpu
rank #1: attn_weights.shape  torch.Size([4, 6, 1, 257])
rank #1: device of attn_weights  cpu
rank #1: attn_weights shape ,  torch.Size([4, 6, 1, 257])
rank #1: before ouput size q.shape  torch.Size([1, 24, 64])
rank #1: attention probs shape :  torch.Size([24, 1, 257])
rank #1: shape of value  torch.Size([1, 4, 384])
rank #1: value device  cuda:1
rank #1: tgt_s  1
rank #1: w_out_rank.shape  torch.Size([768, 384])
rank #1: value.shape  before rowlinear torch.Size([4, 1, 384])
rank #1: w_out_rank.t()  tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:1', dtype=torch.float16,
       grad_fn=<TBackward0>)
rank #1: w_out_rank.t().device  cuda:1
rank #1: w_out_rank.t().shape  torch.Size([384, 768])
rank #1: type of output_parallel  torch.cuda.HalfTensor
rank #1: current rank  1
rank #1: shape of output_parallel  torch.Size([4, 1, 768])
rank #1: output_parallel [0][0][:8]  tensor([inf, inf, inf, inf, inf, inf, inf, inf], device='cuda:1',
       dtype=torch.float16, grad_fn=<SliceBackward0>)
rank #0: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #0: device of output_parallel  cuda:0
rank #0: shape of output_parallel  torch.Size([4, 1, 768])
rank #1: output_parallel  tensor([[[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]],

        [[inf, inf, inf,  ..., inf, inf, inf]]], device='cuda:1',
       dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)
rank #1: device of output_parallel  cuda:1
rank #1: shape of output_parallel  torch.Size([4, 1, 768])
rank #0: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: inputs.data shape  torch.Size([4, 1, 768])
rank #0: inputs.data.device  cuda:0
rank #0: value shape  torch.Size([4, 1, 768])
rank #0: value.device  cuda:0
rank #1: shape of value after all reduce) torch.Size([4, 1, 768])
rank #0: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #1: inputs.data shape  torch.Size([4, 1, 768])
rank #0: TorchTensor.create_from_torch(value, self)
rank #1: inputs.data.device  cuda:0
rank #1: value shape  torch.Size([4, 1, 768])
rank #0: k_new device  cuda:0
rank #0: self.device  cuda:0
rank #1: value.device  cuda:1
rank #1: shape of value after add_(inputs.data) torch.Size([4, 1, 768])
rank #0: new tensor shape  torch.Size([1, 4, 12, 64])
rank #1: TorchTensor.create_from_torch(value, self)
rank #1: k_new device  cuda:1
rank #1: self.device  cuda:0
rank #1: new tensor shape  torch.Size([1, 4, 12, 64])
